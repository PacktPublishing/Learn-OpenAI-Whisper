# -*- coding: utf-8 -*-
"""LOAIW_ch06_1_Transcripting_translating_YouTube_with_Whisper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NhpG_iZSRxbENy8yXm5exiXyHcBIG34A

# Learn OpenAI Whisper - Chapter 6
## Using Whisper and YouTube for transcription and translation
This notebook provides a simple template for using OpenAI's Whisper and YouTube for audio transcription in Google Colab.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1NhpG_iZSRxbENy8yXm5exiXyHcBIG34A)

## Install Whisper
Run the cell below to install Whisper.

The Python libraries `openai`, `cohere`, and `tiktoken` are also installed because of dependencies for the `llmx` library. That is because `llmx` relies on them to function correctly. Each of these libraries provides specific functionalities that `llmx` uses.

1. `openai`: This is the official Python library for the OpenAI API. It provides convenient access to the OpenAI REST API from any Python 3.7+ application. The library includes type definitions for all request parameters and response fields, and offers both synchronous and asynchronous clients powered by `httpx`.

2. `cohere`: The Cohere platform builds natural language processing and generation into your product with a few lines of code. It can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation.

3. `tiktoken`: This is a fast Byte Pair Encoding (BPE) tokenizer for use with OpenAI's models. It's used to tokenize text into subwords, a necessary step before feeding text into many modern language models.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -q cohere openai tiktoken
# !pip install -q "git+https://github.com/openai/whisper.git"
# !pip install -q "git+https://github.com/garywu007/pytube.git"

import re
from pytube import YouTube

video_url = "https://youtu.be/5bs9XoTac88" #@param {type:"string"}
# episode_date = "20231220-" #@param {type:"string"}
drive_folder = "" #@param {type:"string"}

yt = YouTube(video_url)
episode_date = yt.publish_date.strftime('%Y%m%d-')
source_audio = drive_folder + episode_date + (re.sub('[^A-Za-z0-9 ]+', '', yt.title).replace(' ', '_')) + ".mp4"

audio_file = YouTube(video_url).streams.filter(only_audio=True).first().download(filename=source_audio)
print(f"Downloaded '{source_audio}")

import ipywidgets as widgets
widgets.Audio.from_file(audio_file, autoplay=False, loop=False)

import whisper
import torch

model = whisper.load_model("small")

audio = whisper.load_audio(audio_file)
audio = whisper.pad_or_trim(audio)

# make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
audio_lang = max(probs, key=probs.get)
print(f"Detected language: {audio_lang}")

# NLTK helps to split the transcription sentence by sentence
# and shows it in a neat manner one below another. You will see it in the output below.

import nltk
nltk.download('punkt')
from nltk import sent_tokenize

# decode the audio
options = whisper.DecodingOptions(fp16=torch.cuda.is_available(), language=audio_lang, task='transcribe')
result = whisper.decode(model, mel, options)

# print the recognized text
print("----\nTranscription from audio:")
for sent in sent_tokenize(result.text):
  print(sent)

# decode the audio
options = whisper.DecodingOptions(fp16=torch.cuda.is_available(), language=audio_lang, task='translate')
result = whisper.decode(model, mel, options)

# print the recognized text
print("----\nTranslation from audio:")
for sent in sent_tokenize(result.text):
  print(sent)