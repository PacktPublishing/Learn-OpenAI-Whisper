{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using Whisper for podcast transcription using RSS feeds\n",
        "This notebook provides a example of using FeedParser with OpenAI's Whisper.\n",
        "\n",
        "[FeedParser](https://feedparser.readthedocs.io/en/latest/) is a Python library for downloading and parsing syndicated feeds including RSS, Atom & RDF Feeds.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RjVE2zPYY6FFAL3k0S4AgZJYmoV0uH6w)\n",
        "## Install required Python packages\n",
        "Run the cell below to install Whisper, FFMPEG (If not installed), and FeedParser.\n",
        "\n",
        "The Python libraries `openai`, `cohere`, and `tiktoken` are also installed because of dependencies for the `llmx` library. That is because `llmx` relies on them to function correctly."
      ],
      "metadata": {
        "id": "w2wt2RJygcRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk9Jtws0xqLw"
      },
      "outputs": [],
      "source": [
        "!pip install -q cohere openai tiktoken\n",
        "!apt-get install ffmpeg\n",
        "!pip install -q \"git+https://github.com/openai/whisper.git\"\n",
        "!pip install -q feedparser requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "from urllib.parse import urlparse\n",
        "import subprocess\n",
        "import re"
      ],
      "metadata": {
        "id": "tI2em7TiLFFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UlYl8ikXAsi"
      },
      "outputs": [],
      "source": [
        "def list_episodes(feed_url):\n",
        "    \"\"\"\n",
        "    Lists all episodes in the given RSS feed.\n",
        "\n",
        "    Parameters:\n",
        "    - feed_url: The RSS feed URL.\n",
        "\n",
        "    Returns:\n",
        "    - A list of tuples containing episode titles, their URLs and published dates in 'YYYYMMDD' format.\n",
        "    \"\"\"\n",
        "    d = feedparser.parse(feed_url)\n",
        "\n",
        "    episodes = []\n",
        "    for entry in d.entries:\n",
        "        title = entry.title\n",
        "        published = time.strftime('%Y%m%d', time.gmtime(time.mktime(entry.published_parsed)))\n",
        "        url = None\n",
        "        for link in entry.links:\n",
        "            if link.type == \"audio/mpeg\":\n",
        "                url = link.href\n",
        "                break\n",
        "        if url:\n",
        "            episodes.append((title, url, published))\n",
        "\n",
        "    return episodes\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def download_episode(url, filename=None):\n",
        "    \"\"\"\n",
        "    Downloads the podcast episode from the given URL.\n",
        "\n",
        "    Parameters:\n",
        "    - url: The URL of the podcast episode.\n",
        "    - filename: The desired filename to save the podcast. If not provided, it'll use the last part of the URL.\n",
        "\n",
        "    Returns:\n",
        "    - The path to the downloaded file.\n",
        "    \"\"\"\n",
        "    # If a custom filename is provided, append the appropriate extension from the URL\n",
        "    if filename:\n",
        "        parsed_url = urlparse(url)\n",
        "        # Extract only the base path without any query parameters\n",
        "        base_path = os.path.basename(parsed_url.path)\n",
        "        ext = os.path.splitext(base_path)[1]\n",
        "        filename += ext\n",
        "    else:\n",
        "        filename = os.path.basename(parsed_url.path)\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    return filename\n",
        "\n",
        "def download_episode_start_at(url, filename=None, start_at=0):\n",
        "    \"\"\"\n",
        "    Downloads the podcast episode from the given URL and trims it starting from 'start_at' seconds.\n",
        "\n",
        "    Parameters:\n",
        "    - url: The URL of the podcast episode.\n",
        "    - filename: The desired filename to save the podcast. If not provided, it'll use the last part of the URL.\n",
        "    - start_at: The start time in seconds from where the audio should be trimmed.\n",
        "\n",
        "    Returns:\n",
        "    - The path to the downloaded and trimmed file.\n",
        "    \"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    if filename:\n",
        "        # Ensure the filename has the correct extension\n",
        "        ext = os.path.splitext(parsed_url.path)[1]\n",
        "        filename += ext\n",
        "    else:\n",
        "        filename = os.path.basename(parsed_url.path)\n",
        "\n",
        "    # Download the file\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    temp_filename = \"temp_\" + filename\n",
        "    with open(temp_filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    # Use ffmpeg to trim the audio file\n",
        "    trimmed_filename = \"trimmed_\" + filename\n",
        "    command = ['ffmpeg', '-y', '-i', temp_filename, '-ss', str(start_at), '-c', 'copy', trimmed_filename]\n",
        "    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # Remove the original downloaded file\n",
        "    os.remove(temp_filename)\n",
        "\n",
        "    return trimmed_filename"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RSS feed podcast"
      ],
      "metadata": {
        "id": "fZk8grnzwVVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Science, Quickly\n",
        "podcast = 'https://feeds.megaphone.fm/SAM8799470961'\n",
        "# WSJ Minute Briefing\n",
        "# podcast = 'https://feeds.megaphone.fm/WSJ7928321669'\n",
        "\n",
        "d = feedparser.parse(podcast)\n",
        "print(f\"Podcast name:\", d.feed.title)\n",
        "print(f\"Number of episodes:\", len(d.entries))\n",
        "\n",
        "# List episodes\n",
        "episodes = list_episodes(podcast)\n",
        "# Print the first 10 episodes\n",
        "print(\"Episodes:\")\n",
        "for idx, (title, url, published) in enumerate(episodes, 1):\n",
        "    print(f\"{idx}. {published}-{title}\")\n",
        "    if idx == 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "MGTC7X0CyyXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0HtmCwoi8ho"
      },
      "outputs": [],
      "source": [
        "episode_num = 5 #@param {type:\"integer\"}\n",
        "drive_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "title, url, published = episodes[episode_num - 1]\n",
        "custom_filename = published + '-' + (re.sub('[^A-Za-z0-9 ]+', '', title[:75]).replace(' ', '_'))\n",
        "\n",
        "# Download the selected episode\n",
        "audio_file = download_episode_start_at(url, drive_folder + custom_filename, 30)\n",
        "print(f\"Downloaded '{title}' as {audio_file}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "widgets.Audio.from_file(audio_file, autoplay=False, loop=False)"
      ],
      "metadata": {
        "id": "SA1LrZaHbwKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import torch\n",
        "# NLTK helps to split the transcription sentence by sentence\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "model = whisper.load_model(\"small\")\n",
        "\n",
        "audio = whisper.load_audio(audio_file)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "# decode the audio\n",
        "options = whisper.DecodingOptions(fp16=torch.cuda.is_available(), task='transcribe')\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(\"----\\nTranscription from audio:\")\n",
        "for sent in sent_tokenize(result.text):\n",
        "  print(sent)"
      ],
      "metadata": {
        "id": "LGaiMqnl6as7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}