{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learn OpenAI Whisper - Chapter 9 - Cloning a voice step 1: Converting audio files to LJSpeech format\n",
        "\n",
        "\n",
        "This notebook represents the initial step in the 3-step voice cloning process outlined in the chapter. This step takes an audio sample of the target voice as input and processes it into the LJSpeech dataset format. The notebook demonstrates how to use the OZEN Toolkit and OpenAI's Whisper to extract speech, transcribe it, and organize the data according to the LJSpeech structure. The resulting LJSpeech-formatted dataset, consisting of segmented audio files and corresponding transcriptions, serves as the input for the second step, \"Cloning a voice step 2: Fine-tuning a discrete variational autoencoder using the DLAS toolkit,\" where a voice cloning model is fine-tuned using this dataset.\n",
        "\n",
        "## Notebook 2: Process audio files to a LJ format with Whisper and OZEN\n",
        "\n",
        "This notebook complements the book [Learn OpenAI Whisper](https://a.co/d/1p5k4Tg).\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wnomL0dxmU9CgPKIgazR8AocolEYjAe5)\n",
        "\n",
        "This notebook is based on the [OZEN Toolkit](https://github.com/devilismyfriend/ozen-toolkit) project. Given a folder of files or a single audio file, it will extract the speech, transcribe using Whisper and save in the LJ format (segmented audio files in WAV format in `wavs` folder, transcriptions in folders `train` and `valid`).\n",
        "\n",
        "**NOTE**: The notebook stores the files using the following format.\n",
        "\n",
        "`dataset/`\n",
        "* ---├── `valid.txt`\n",
        "* ---├── `train.txt`\n",
        "* ---├── `wavs/`\n",
        "\n",
        "`wavs/` directory must contain `.wav` files.\n",
        "\n",
        "Example for `train.txt` and `valid.txt`:\n",
        "\n",
        "* `wavs/A.wav|Write the transcribed audio here.`\n",
        "\n"
      ],
      "metadata": {
        "id": "7-_XzuAwYuok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.\tCloning the OZEN Toolkit repository:\n",
        "\n",
        "The following command clones the OZEN Toolkit repository from GitHub, which contains the necessary scripts and utilities for processing audio files:"
      ],
      "metadata": {
        "id": "MZ7hGRHw9GLy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7n6I0nrcQj2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a6bbca-7a8f-4d29-e2ed-7e90bb88908a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ozen-toolkit'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 35 (delta 15), reused 20 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (35/35), 11.37 KiB | 11.37 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/devilismyfriend/ozen-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.\tInstalling required libraries\n",
        "\n",
        "These following commands install the necessary libraries for audio processing, speech recognition, and text formatting:"
      ],
      "metadata": {
        "id": "4rvuQoC_9NQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers\n",
        "!pip -q install huggingface\n",
        "!pip -q install pydub\n",
        "!pip -q install yt-dlp\n",
        "!pip -q install pyannote.audio"
      ],
      "metadata": {
        "id": "gsEWSIVHR5Er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55405e9b-a74e-43b1-9093-d7e752185013"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/208.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.1/760.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESTART SESSION\n",
        "\n",
        "In Google Colab, from the top menu, select `Runtime`, then `Restart session`.\n",
        "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter09/Restart_the_runtime_600x102.png\" width=600>"
      ],
      "metadata": {
        "id": "K7msTh8jL6jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install colorama\n",
        "!pip -q install termcolor\n",
        "!pip -q install pyfiglet"
      ],
      "metadata": {
        "id": "bTqMldutL8-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70566bb2-a210-4eef-867d-8fdd030ba1a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m0.8/1.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/content/ozen-toolkit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.\tChanging the working directory\n",
        "\n",
        "The next command changes the working directory to the cloned ozen-toolkit directory:"
      ],
      "metadata": {
        "id": "zouAX0o2-fB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ozen-toolkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KjQB5N6-eDT",
        "outputId": "c09cd7fd-b6ef-4e2c-ff00-2ac709dac259"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'ozen-toolkit'\n",
            "/content/ozen-toolkit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.\tDownloading a sample audio file\n",
        "\n",
        "If you do not have an audio file for cloning, this command downloads a sample audio file from the specified URL for demonstration purposes:"
      ],
      "metadata": {
        "id": "uNHAT18b-psU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sample file\n",
        "!wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.mp3"
      ],
      "metadata": {
        "id": "qilVdaLsQHEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d6643e-d7e4-4c56-b44a-c7bc9c773801"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 21:08:54 URL:https://raw.githubusercontent.com/PacktPublishing/Learn-OpenAI-Whisper/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.mp3 [363247/363247] -> \"Learn_OAI_Whisper_Sample_Audio01.mp3\" [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.\tUploading custom audio files\n",
        "\n",
        "If you have your audio file, this code block allows users to upload their audio files to the Colab environment. It creates a directory in `/content/ozen-toolkit` to store the uploaded files and saves them in that directory:"
      ],
      "metadata": {
        "id": "IyKrUHwN_RdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VQgw3KeV8Yqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7182e1e3-99ee-4680-be7f-50c592f9f83e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b7860d81-7565-4d32-af5e-3e48d2c1f138\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b7860d81-7565-4d32-af5e-3e48d2c1f138\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3 to Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3\n",
            "-rw-r--r-- 1 root root 1980150 Apr 11 21:10  /content/ozen-toolkit/20150415-Fracking_the_debate.mp4\n",
            "-rw-r--r-- 1 root root    2392 Apr 11 20:59  /content/ozen-toolkit/Drag_Here.cmd\n",
            "-rw-r--r-- 1 root root     276 Apr 11 20:59  /content/ozen-toolkit/environment.yaml\n",
            "-rw-r--r-- 1 root root      66 Apr 11 20:59  /content/ozen-toolkit/.gitattributes\n",
            "-rw-r--r-- 1 root root     144 Apr 11 20:59  /content/ozen-toolkit/.gitignore\n",
            "-rw-r--r-- 1 root root  363247 Apr 11 21:08  /content/ozen-toolkit/Learn_OAI_Whisper_Sample_Audio01.mp3\n",
            "-rw-r--r-- 1 root root   24361 Apr 11 21:17  /content/ozen-toolkit/Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3\n",
            "-rw-r--r-- 1 root root   14248 Apr 11 20:59  /content/ozen-toolkit/ozen.py\n",
            "-rw-r--r-- 1 root root    1066 Apr 11 20:59  /content/ozen-toolkit/README.md\n",
            "-rw-r--r-- 1 root root      80 Apr 11 20:59  /content/ozen-toolkit/requirements.txt\n",
            "-rw-r--r-- 1 root root    2491 Apr 11 20:59 '/content/ozen-toolkit/Set Up Ozen.bat'\n",
            "\n",
            "/content/ozen-toolkit/.:\n",
            "total 2360\n",
            "-rw-r--r-- 1 root root 1980150 Apr 11 21:10  20150415-Fracking_the_debate.mp4\n",
            "-rw-r--r-- 1 root root    2392 Apr 11 20:59  Drag_Here.cmd\n",
            "-rw-r--r-- 1 root root     276 Apr 11 20:59  environment.yaml\n",
            "-rw-r--r-- 1 root root  363247 Apr 11 21:08  Learn_OAI_Whisper_Sample_Audio01.mp3\n",
            "-rw-r--r-- 1 root root   24361 Apr 11 21:17  Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3\n",
            "drwxr-xr-x 2 root root    4096 Apr 11 20:59  \u001b[0m\u001b[01;34mmodules\u001b[0m/\n",
            "drwxr-xr-x 2 root root    4096 Apr 11 21:17  \u001b[01;34mmyaudiofile\u001b[0m/\n",
            "-rw-r--r-- 1 root root   14248 Apr 11 20:59  ozen.py\n",
            "-rw-r--r-- 1 root root    1066 Apr 11 20:59  README.md\n",
            "-rw-r--r-- 1 root root      80 Apr 11 20:59  requirements.txt\n",
            "-rw-r--r-- 1 root root    2491 Apr 11 20:59 'Set Up Ozen.bat'\n",
            "\n",
            "/content/ozen-toolkit/..:\n",
            "total 8\n",
            "drwxr-xr-x 5 root root 4096 Apr 11 21:17 \u001b[01;34mozen-toolkit\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Apr 11 03:25 \u001b[01;34msample_data\u001b[0m/\n",
            "\n",
            "/content/ozen-toolkit/.git:\n",
            "total 44\n",
            "drwxr-xr-x 2 root root 4096 Apr 11 20:59 \u001b[01;34mbranches\u001b[0m/\n",
            "-rw-r--r-- 1 root root  268 Apr 11 20:59 config\n",
            "-rw-r--r-- 1 root root   73 Apr 11 20:59 description\n",
            "-rw-r--r-- 1 root root   21 Apr 11 20:59 HEAD\n",
            "drwxr-xr-x 2 root root 4096 Apr 11 20:59 \u001b[01;34mhooks\u001b[0m/\n",
            "-rw-r--r-- 1 root root  890 Apr 11 20:59 index\n",
            "drwxr-xr-x 2 root root 4096 Apr 11 20:59 \u001b[01;34minfo\u001b[0m/\n",
            "drwxr-xr-x 3 root root 4096 Apr 11 20:59 \u001b[01;34mlogs\u001b[0m/\n",
            "drwxr-xr-x 4 root root 4096 Apr 11 20:59 \u001b[01;34mobjects\u001b[0m/\n",
            "-rw-r--r-- 1 root root  112 Apr 11 20:59 packed-refs\n",
            "drwxr-xr-x 5 root root 4096 Apr 11 20:59 \u001b[01;34mrefs\u001b[0m/\n",
            "\n",
            "/content/ozen-toolkit/modules:\n",
            "total 8\n",
            "-rw-r--r-- 1 root root    0 Apr 11 20:59 __init__.py\n",
            "-rw-r--r-- 1 root root 6538 Apr 11 20:59 utils.py\n",
            "\n",
            "/content/ozen-toolkit/myaudiofile:\n",
            "total 1960\n",
            "-rw-r--r-- 1 root root 1980150 Apr 11 21:10 20150415-Fracking_the_debate.mp4\n",
            "-rw-r--r-- 1 root root   24361 Apr 11 21:17 Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3\n"
          ]
        }
      ],
      "source": [
        "# CUSTOM_VOICE_NAME = \"custom\"\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "custom_voice_folder = \"./myaudiofile\"\n",
        "\n",
        "os.makedirs(custom_voice_folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "for filename, file_data in files.upload().items():\n",
        "    with open(os.path.join(custom_voice_folder, filename), 'wb') as f:\n",
        "        f.write(file_data)\n",
        "\n",
        "%ls -l \"$PWD\"/{*,.*}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.\tCreating a configuration file\n",
        "The following code section creates a configuration file named `config.ini` using the `configparser` library. It defines various settings such as the Hugging Face API key, Whisper model, device, diarization and segmentation models, validation ratio, and segmentation parameters:"
      ],
      "metadata": {
        "id": "CWIANgl6B6_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "\n",
        "# Create a new ConfigParser object\n",
        "config = configparser.ConfigParser()\n",
        "\n",
        "# Add the 'DEFAULT' section and set the options\n",
        "config['DEFAULT'] = {\n",
        "    'hf_token': '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',\n",
        "    'whisper_model': 'openai/whisper-medium',\n",
        "    'device': 'cuda',\n",
        "    'diaization_model': 'pyannote/speaker-diarization',\n",
        "    'segmentation_model': 'pyannote/segmentation',\n",
        "    'valid_ratio': '0.2',\n",
        "    'seg_onset': '0.7',\n",
        "    'seg_offset': '0.55',\n",
        "    'seg_min_duration': '2.0',\n",
        "    'seg_min_duration_off': '0.0'\n",
        "}\n",
        "\n",
        "# Write the configuration to a file\n",
        "with open('config.ini', 'w') as configfile:\n",
        "    config.write(configfile)\n",
        "\n",
        "# Print the contents of the file\n",
        "with open('config.ini', 'r') as configfile:\n",
        "    print(configfile.read())"
      ],
      "metadata": {
        "id": "MLHFTm15WatB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db20fe1-63ef-4df2-a2a6-c72f88b44fc1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEFAULT]\n",
            "hf_token = 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\n",
            "whisper_model = openai/whisper-medium\n",
            "device = cuda\n",
            "diaization_model = pyannote/speaker-diarization\n",
            "segmentation_model = pyannote/segmentation\n",
            "valid_ratio = 0.2\n",
            "seg_onset = 0.7\n",
            "seg_offset = 0.55\n",
            "seg_min_duration = 2.0\n",
            "seg_min_duration_off = 0.0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.\tRunning the OZEN script\n",
        "\n",
        "This command runs the ozen.py script with the sample audio file as an argument (or the file you uploaded).\n",
        "\n",
        "# IMPORTANT:\n",
        "`ozen.py` requires Hugging Face's `pyannote/segmentation` model. This is a gated model; you MUST request access before attempting to run the next cell. Thankfully, getting access is relatively straightforward and fast.\n",
        "\n",
        "*   You must already have a Hugging Face account; if you do not have one, see the instructions in the notebook for chapter 3:  [LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb](https://colab.research.google.com/drive/1bIiGyv_YiTdq97a7KrowCceOrZlG2hXL#scrollTo=VCEKs-Y4wAYQ)\n",
        "*   Visit https://hf.co/pyannote/segmentation to accept the user conditions.\n",
        "\n",
        "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter09/HF_pyanonote_sementation_gated_model.JPG\" width=600>\n",
        "\n",
        "The script processes the audio file, extracts speech, transcribes it using Whisper, and saves the output in the LJSpeech format. The script saves the DJ format files in a folder called `ozen-toolkit/output/<audio file name + timestamp>/`. Here is an example of the expected file structure:\n",
        "```\n",
        "ozen-toolkit/output/\n",
        "---├── Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-16_36/\n",
        "------------------├── valid.txt\n",
        "------------------├── train.txt\n",
        "------------------├── wavs/\n",
        "--------------------------├── 0.wav\n",
        "--------------------------├── 1.wav\n",
        "--------------------------├── 2.wav\n",
        "```"
      ],
      "metadata": {
        "id": "-XGpcx7QKr7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ozen.py Learn_OAI_Whisper_Sample_Audio01.mp3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEhKULyV9rT3",
        "outputId": "6e728ba0-cd3f-4363-9c2f-cf49925ca220"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 21:39:36.166216: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 21:39:36.166264: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 21:39:36.167639: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 21:39:37.262985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[1m\u001b[40m\u001b[33m  ______    ________   _______ .__   __. \n",
            " /  __  \\  |       /  |   ____||  \\ |  | \n",
            "|  |  |  | `---/  /   |  |__   |   \\|  | \n",
            "|  |  |  |    /  /    |   __|  |  . `  | \n",
            "|  `--'  |   /  /----.|  |____ |  |\\   | \n",
            " \\______/   /________||_______||__| \\__| \n",
            "                                         \n",
            "\u001b[0m\n",
            "\u001b[32mConverting to WAV...\u001b[39m\n",
            "\u001b[32mLoading Segment Model...\u001b[39m\n",
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "pytorch_model.bin: 100% 17.7M/17.7M [00:00<00:00, 149MB/s]\n",
            "config.yaml: 100% 318/318 [00:00<00:00, 2.18MB/s]\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/2ffce0501d0aecad81b43a06d538186e292d0070/pytorch_model.bin`\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.2.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "\u001b[32mSegmenting...\u001b[39m\n",
            "\u001b[32mLoading Transcribing Model...\u001b[39m\n",
            "config.json: 100% 1.99k/1.99k [00:00<00:00, 8.60MB/s]\n",
            "model.safetensors: 100% 3.06G/3.06G [00:16<00:00, 183MB/s]\n",
            "generation_config.json: 100% 3.75k/3.75k [00:00<00:00, 18.8MB/s]\n",
            "tokenizer_config.json: 100% 283k/283k [00:00<00:00, 1.77MB/s]\n",
            "vocab.json: 100% 836k/836k [00:00<00:00, 2.88MB/s]\n",
            "tokenizer.json: 100% 2.48M/2.48M [00:00<00:00, 11.8MB/s]\n",
            "merges.txt: 100% 494k/494k [00:00<00:00, 41.5MB/s]\n",
            "normalizer.json: 100% 52.7k/52.7k [00:00<00:00, 98.6MB/s]\n",
            "added_tokens.json: 100% 34.6k/34.6k [00:00<00:00, 122MB/s]\n",
            "special_tokens_map.json: 100% 2.19k/2.19k [00:00<00:00, 13.7MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "preprocessor_config.json: 100% 185k/185k [00:00<00:00, 2.99MB/s]\n",
            "\u001b[32mTranscribing...\u001b[39m\n",
            "  0% 0/5 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "100% 5/5 [00:06<00:00,  1.00s/it]\u001b[32m Done!\u001b[39m\n",
            "\n",
            "100% 5/5 [00:07<00:00,  1.58s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive (To save trained checkpoints and to load the dataset from)"
      ],
      "metadata": {
        "id": "Agpune8JjsQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.\tMounting Google Drive\n",
        "\n",
        "These lines mount the user's Google Drive to the Colab environment, allowing access to the drive for saving checkpoints and loading datasets:"
      ],
      "metadata": {
        "id": "pec0vCcWPc9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "xFt4umCqjlS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961d56eb-d010-473d-f436-f414b72648ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.\tCopying the output to Google Drive\n",
        "\n",
        "The following command copies the processed output files from the `ozen-toolkit/output` directory to your Google Drive.\n",
        "\n",
        "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter09/images/ch09_2_Google_Colab_directory.JPG\" width=600>\n"
      ],
      "metadata": {
        "id": "6IhYDEIzQzhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cp -r /content/ozen-toolkit/output/ /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "id": "CDHLbbE7GgjQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the cell, go to your Google Drive using a web browser, and you will see a directory called `output` with the DJ format dataset files in it.\n",
        "\n",
        "<img src=\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter09/images/ch09_2_Google_Drive_directory.JPG\" width=500>"
      ],
      "metadata": {
        "id": "zT66okSEUzV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "With our audio data now converted to the LJSpeech format, we are well-prepared to embark on the following critical stage of the voice cloning journey: fine-tuning a voice cloning model using the powerful DLAS toolkit. The notebook [LOAIW_ch09_3_Fine_tuning_voice_cloning_with_DLAS.ipynb](/Chapter09/LOAIW_ch09_3_Fine_tuning_voice_cloning_with_DLAS.ipynb) will cover that process in detail. By leveraging the DLAS toolkit's comprehensive features and the structured LJSpeech dataset, we can create a personalized voice model that captures the unique characteristics of our target speaker with remarkable accuracy and naturalness."
      ],
      "metadata": {
        "id": "lyoCGBH47_Jp"
      }
    }
  ]
}