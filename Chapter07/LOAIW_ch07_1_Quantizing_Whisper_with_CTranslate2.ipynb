{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learn OpenAI Whisper - Chapter 7\n",
        "## Notebook 1: Quantizing Whisper with Ctranslate2 and running inference with Faster-Whisper\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lFKZCc-mDIf8xH_v7_M1m1hfA-Ke772d)\n",
        "\n",
        "This notebook outlines a comprehensive process for quantizing the Whisper model using [CTranslate2](https://opennmt.net/CTranslate2/guides/transformers.html#whisper), a library designed for efficient inference with transformer models. This process is crucial for deploying Automated Speech Recognition (ASR) models like Whisper in environments where computational resources are limited.\n",
        "\n",
        "![ch07_1-quantizing-whisper-with-ctranslate2.png](https://raw.githubusercontent.com/PacktPublishing/Learn-OpenAI-Whisper/main/Chapter07/ch07_1-quantizing-whisper-with-ctranslate2.png)"
      ],
      "metadata": {
        "id": "r4qzcLk5RW9X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22bf06fc-5988-4e3d-9d81-7fe23ff18131"
      },
      "source": [
        "## Prerequisites\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "OHNuVY_RTujq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify authentication\n",
        "from huggingface_hub import whoami\n",
        "whoami()\n",
        "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
      ],
      "metadata": {
        "id": "MMHkYq7oF6y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.\tInstalling libraries:\n",
        "\n",
        "The code begins with installing ctranslate2, transformers, and faster-whisper.\n",
        "\n",
        "These libraries are essential for quantization and leveraging the Whisper model's capabilities.\n"
      ],
      "metadata": {
        "id": "Y5gCMCsI8-ex"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xXUbdEyp53b"
      },
      "outputs": [],
      "source": [
        "!pip -q install ctranslate2\n",
        "!pip -q install transformers[torch]>=4.23\n",
        "!pip -q install faster-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01da6dd2"
      },
      "source": [
        "### 2.\tDownloading sample audio files\n",
        "Two are downloaded from our GitHub repository to test the Whisper model's transcription capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6586c658"
      },
      "outputs": [],
      "source": [
        "!wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.mp3\n",
        "!wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Sample_Audio02.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.\tPreprocessing audio files\n",
        "The audio files are loaded and resampled to a sampling frequency of 16,000 Hz using librosa. This step is crucial for ensuring that the audio data is in the correct format for processing by the Whisper model."
      ],
      "metadata": {
        "id": "KAv51EQ89gqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ctranslate2\n",
        "from IPython.display import Audio\n",
        "import librosa\n",
        "import transformers\n",
        "# Load and resample the audio file.\n",
        "sampling_frequency = 16000\n",
        "audio, _ = librosa.load(\"Learn_OAI_Whisper_Sample_Audio01.mp3\", sr=sampling_frequency, mono=True)\n",
        "Audio(audio, rate=sampling_frequency)"
      ],
      "metadata": {
        "id": "VjuySaOut4Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "this_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "IRS8EXpwFZlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.\tConverting to CTranslate2 format:\n",
        "In this step, we convert the Whisper models `openai/whisper-tiny` and `openai/whisper-base` to the CTranslate2 format, a more efficient inference format."
      ],
      "metadata": {
        "id": "BeOVLXc9-KDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ct2-transformers-converter --force --model openai/whisper-tiny --output_dir whisper-tiny-ct2"
      ],
      "metadata": {
        "id": "zp5qynTd_TmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ct2-transformers-converter --force --model openai/whisper-base --output_dir whisper-base-ct2"
      ],
      "metadata": {
        "id": "fQ99FsC_2H4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.\tPerforming quantization\n",
        "The models are then quantized to an 8-bit integer format (int8)"
      ],
      "metadata": {
        "id": "fIU4n9TXHzwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ct2-transformers-converter --force --model openai/whisper-tiny --output_dir whisper-tiny-ct2-int8 \\\n",
        "--copy_files tokenizer.json preprocessor_config.json --quantization int8"
      ],
      "metadata": {
        "id": "kf7x_H3Yx1ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ct2-transformers-converter --force --model openai/whisper-base --output_dir whisper-base-ct2-int8 \\\n",
        "--copy_files tokenizer.json preprocessor_config.json --quantization int8"
      ],
      "metadata": {
        "id": "1d6SbjT016P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Detecting language\n",
        "The quantized model detects the language of the provided audio samples"
      ],
      "metadata": {
        "id": "9gCwGLiRIDZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model on device\n",
        "model = ctranslate2.models.Whisper(\"whisper-tiny-ct2-int8\", device=this_device)"
      ],
      "metadata": {
        "id": "u3OnrmPPEpNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = transformers.WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
        "inputs = processor(audio, return_tensors=\"np\", sampling_rate=sampling_frequency)\n",
        "features = ctranslate2.StorageView.from_array(inputs.input_features)"
      ],
      "metadata": {
        "id": "JUhzv7uDuM78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute and display the features of the first 30 seconds of audio."
      ],
      "metadata": {
        "id": "bknPsi8S-BFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect the language.\n",
        "results = model.detect_language(features)\n",
        "language, probability = results[0][0]\n",
        "print(\"Detected language %s with probability %f\" % (language, probability))"
      ],
      "metadata": {
        "id": "1IuCSDAjvBkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.\tTranscribing audio files\n",
        "The quantized model generates transcriptions for the audio samples"
      ],
      "metadata": {
        "id": "eCrkFYTAKFCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe the task in the prompt.\n",
        "# See the prompt format in https://github.com/openai/whisper.\n",
        "prompt = processor.tokenizer.convert_tokens_to_ids(\n",
        "    [\n",
        "        \"<|startoftranscript|>\",\n",
        "        language,\n",
        "        \"<|transcribe|>\",\n",
        "        \"<|notimestamps|>\",  # Remove this token to generate timestamps.\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "YTW2uEJhwjHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model on device\n",
        "model = ctranslate2.models.Whisper(\"whisper-tiny-ct2-int8\", device=this_device)"
      ],
      "metadata": {
        "id": "xyVmTN3DMs0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run generation for the 30-second window.\n",
        "results = model.generate(features, [prompt])\n",
        "transcription = processor.decode(results[0].sequences_ids[0])\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "QkZ5f9Hawn_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.\tEvaluating performance\n",
        " After the audio transcription, the code evaluates the performance of the quantized model, such as measuring the time taken for transcription."
      ],
      "metadata": {
        "id": "XP3q2FNRKNWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and resample the audio file.\n",
        "sampling_frequency = 16000\n",
        "audio, _ = librosa.load(\"Learn_OAI_Whisper_Sample_Audio02.mp3\", sr=sampling_frequency, mono=True)\n",
        "Audio(audio, rate=sampling_frequency)"
      ],
      "metadata": {
        "id": "V9zHyN5_rTGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "model_size = \"whisper-tiny-ct2\"\n",
        "model = WhisperModel(model_size, device=this_device, compute_type=\"int8\")\n",
        "segments, info = model.transcribe(\"Learn_OAI_Whisper_Sample_Audio02.mp3\", beam_size=5)\n",
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
        "\n",
        "start = time.time()\n",
        "for segment in segments:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "# Print the end time and the delta in seconds and fractions of a second.\n",
        "end = time.time()\n",
        "print('start: ', start)\n",
        "print('end: ', end)\n",
        "print('delta: ', end - start)\n",
        "print('delta: ', datetime.timedelta(seconds=end - start))"
      ],
      "metadata": {
        "id": "sJb7OLnNzfaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "model_size = \"whisper-tiny-ct2-int8\"\n",
        "model = WhisperModel(model_size, device=this_device, compute_type=\"int8\")\n",
        "segments, info = model.transcribe(\"Learn_OAI_Whisper_Sample_Audio02.mp3\", beam_size=5)\n",
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
        "\n",
        "start = time.time()\n",
        "for segment in segments:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "# Print the end time and the delta in seconds and fractions of a second.\n",
        "end = time.time()\n",
        "print('start: ', start)\n",
        "print('end: ', end)\n",
        "print('delta: ', end - start)\n",
        "print('delta: ', datetime.timedelta(seconds=end - start))"
      ],
      "metadata": {
        "id": "JS2BNDdfmWks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "model_size = \"whisper-base-ct2\"\n",
        "model = WhisperModel(model_size, device=this_device, compute_type=\"int8\")\n",
        "segments, info = model.transcribe(\"Learn_OAI_Whisper_Sample_Audio02.mp3\", beam_size=5)\n",
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
        "\n",
        "start = time.time()\n",
        "for segment in segments:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "# Print the end time and the delta in seconds and fractions of a second.\n",
        "end = time.time()\n",
        "print('start: ', start)\n",
        "print('end: ', end)\n",
        "print('delta: ', end - start)\n",
        "print('delta: ', datetime.timedelta(seconds=end - start))"
      ],
      "metadata": {
        "id": "7VZFnxA6os7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faster_whisper import WhisperModel\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "model_size = \"whisper-base-ct2-int8\"\n",
        "model = WhisperModel(model_size, device=this_device, compute_type=\"int8\")\n",
        "segments, info = model.transcribe(\"Learn_OAI_Whisper_Sample_Audio02.mp3\", beam_size=5)\n",
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
        "\n",
        "start = time.time()\n",
        "for segment in segments:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "# Print the end time and the delta in seconds and fractions of a second.\n",
        "end = time.time()\n",
        "print('start: ', start)\n",
        "print('end: ', end)\n",
        "print('delta: ', end - start)\n",
        "print('delta: ', datetime.timedelta(seconds=end - start))"
      ],
      "metadata": {
        "id": "lN7SaPUnsWBw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
