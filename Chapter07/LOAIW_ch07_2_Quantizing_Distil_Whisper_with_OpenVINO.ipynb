{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63b2fd12-a1a3-45a7-949e-64b903c5d2d5"
      },
      "source": [
        "# Learn OpenAI Whisper - Chapter 7\n",
        "\n",
        "## Notebook 2: Quantizing Distil-Whisper with OpenVINO\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1VbveJKfwuNS_P7-myiKq_d_IH-svQ5es)\n",
        "\n",
        "![ch07_2-quantizing-distil-whisper-openvino.png](https://raw.githubusercontent.com/PacktPublishing/Learn-OpenAI-Whisper/main/Chapter07/ch07_2-quantizing-distil-whisper-openvino.png)\n",
        "\n",
        "In this tutorial, we explore how to leverage the power of [Distil-Whisper](https://huggingface.co/distil-whisper/distil-large-v2), a distilled variant of OpenAI's  [Whisper](https://huggingface.co/openai/whisper-large-v2) model, in combination with OpenVINO for efficient and accurate automatic speech recognition (ASR). Distil-Whisper, proposed in the paper [Robust Knowledge Distillation via Large-Scale Pseudo Labelling](https://arxiv.org/abs/2311.00430), offers significant speed improvements and parameter reduction while maintaining comparable performance to the original Whisper model.\n",
        "\n",
        "We delve into the architecture of Whisper, a Transformer-based encoder-decoder model that maps audio spectrogram features to text tokens. The process involves converting raw audio inputs to a log-Mel spectrogram, encoding the spectrogram using a Transformer encoder, and autoregressively predicting text tokens using a decoder.\n",
        "\n",
        "To simplify the integration of Distil-Whisper with OpenVINO, we utilize the [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) library for loading the pre-trained model and the [Hugging Face Optimum](https://huggingface.co/docs/optimum) library for converting the model to OpenVINO™ IR format. Additionally, we apply `INT8` post-training quantization from [NNCF](https://github.com/openvinotoolkit/nncf/) to further improve the performance of the OpenVINO Distil-Whisper model.\n",
        "\n",
        "This tutorial covers the following topics:\n",
        "\n",
        "1. [Prerequisites](#Prerequisites): Setting up the necessary dependencies and environment.\n",
        "2. [Loading PyTorch model](#Loading-PyTorch-model): Loading the Distil-Whisper model using PyTorch.\n",
        "   - [Preparing input sample](#Preparing-input-sample): Preparing an audio input sample for inference.\n",
        "   - [Running model inference](#Running-model-inference): Running inference on the PyTorch model.\n",
        "3. [Loading OpenVINO model using Optimum library](#Loading-OpenVINO-model-using-Optimum-library): Converting the model to OpenVINO format using the Optimum library.\n",
        "   - [Selecting Inference device](#Selecting-Inference-device): Choosing the appropriate inference device.\n",
        "   - [Compiling OpenVINO model](#Compiling-OpenVINO-model): Compiling the OpenVINO model for optimal performance.\n",
        "   - [Running OpenVINO model inference](#Running-OpenVINO-model-inference): Running inference on the OpenVINO model.\n",
        "4. [Comparing performance PyTorch vs OpenVINO](#Comparing-performance-PyTorch-vs-OpenVINO): Benchmarking the performance of the PyTorch and OpenVINO models.\n",
        "   - [Comparing with OpenAI Whisper](#Comparing-with-OpenAI-Whisper): Comparing Distil-Whisper with the original OpenAI Whisper model.\n",
        "5. [Using OpenVINO model with HuggingFace pipelines](#Using-OpenVINO-model-with-HuggingFace-pipelines): Integrating the OpenVINO model with HuggingFace pipelines for seamless usage.\n",
        "6. [Quantizing](#Quantizing): Applying post-training quantization to further optimize the model.\n",
        "   - [Preparing calibration datasets](#Preparing-calibration-datasets): Preparing datasets for quantization calibration.\n",
        "   - [Quantizing Distil-Whisper encoder and decoder models](#Quantizing-Distil-Whisper-encoder-and-decoder-models): Quantizing the encoder and decoder components of the Distil-Whisper model.\n",
        "   - [Running quantized model inference](#Runnig-quantized-model-inference): Running inference on the quantized model.\n",
        "   - [Comparing performance and accuracy of the original and quantized models](#Comparing-performance-and-accuracy-of-the-original-and-quantized-models): Evaluating the impact of quantization on performance and accuracy.\n",
        "7. [Running interactive demo](#Running-interactive-demo): Exploring an interactive demonstration of the Distil-Whisper model.\n",
        "\n",
        "By the end of this tutorial, you will have a comprehensive understanding of how to leverage Distil-Whisper and OpenVINO for efficient and accurate automatic speech recognition. Let's get started!"
      ],
      "id": "63b2fd12-a1a3-45a7-949e-64b903c5d2d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22bf06fc-5988-4e3d-9d81-7fe23ff18131"
      },
      "source": [
        "## Prerequisites\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Before diving into the tutorial, ensure that you have the necessary prerequisites in place. This includes authenticating with the Hugging Face Hub using your token and verifying the authentication by running the provided code cells. These steps are crucial for accessing the required models and datasets throughout the tutorial.\n",
        "\n"
      ],
      "id": "22bf06fc-5988-4e3d-9d81-7fe23ff18131"
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your token in the output prompt below and hit enter on Windows or return on Mac\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "OHNuVY_RTujq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "OHNuVY_RTujq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify authentication\n",
        "from huggingface_hub import whoami\n",
        "whoami()\n",
        "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
      ],
      "metadata": {
        "id": "MMHkYq7oF6y4"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MMHkYq7oF6y4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb9fc7f3-cea0-4adf-9ee6-4a3d15931db7"
      },
      "outputs": [],
      "source": [
        "%pip install -q sentence-transformers==2.3.1\n",
        "%pip install -q \"transformers==4.37.2\"\n",
        "%pip install -q onnx \"git+https://github.com/huggingface/optimum-intel.git\" \"peft==0.6.2\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "%pip install -q \"openvino>=2023.2.0\" datasets  \"gradio>=4.0\" \"librosa\" \"soundfile\"\n",
        "%pip install -q \"nncf>=2.6.0\" \"jiwer\""
      ],
      "id": "bb9fc7f3-cea0-4adf-9ee6-4a3d15931db7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Restart_the_runtime.png](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter07/Restart_the_runtime.png)"
      ],
      "metadata": {
        "id": "cvZJLpQfnZOJ"
      },
      "id": "cvZJLpQfnZOJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34bbdf5e-0e4c-482c-a08a-395972c8b56f"
      },
      "source": [
        "## Loading the PyTorch model\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Loading the PyTorch Whisper model is a straightforward process using the transformers library. The `AutoModelForSpeechSeq2Seq.from_pretrained` method is employed to initialize the model. In this tutorial, we will use the `distil-whisper/distil-large-v2` model as the default example. Please note that the model will be downloaded during the first run, which may take some time.\n",
        "\n",
        "However, you have the flexibility to choose from a variety of models available in the [Distil-Whisper Hugging Face collection](https://huggingface.co/collections/distil-whisper/distil-whisper-models-65411987e6727569748d2eb6). Some alternative options include `distil-whisper/distil-medium.en` and `distil-whisper/distil-small.en`. Additionally, models of the original Whisper architecture are also accessible, which you can explore further [here](https://huggingface.co/openai).\n",
        "\n",
        "It's important to highlight the significance of preprocessing and post-processing in the model's usage. The `AutoProcessor` class, specifically the `WhisperProcessor`, plays a crucial role in preparing the audio input data for the model. It handles tasks such as converting the audio to a Mel-spectrogram representation and decoding the predicted output token IDs back into a string using the tokenizer.\n",
        "\n",
        "To ensure a smooth and efficient workflow, the `AutoProcessor` class streamlines the preprocessing and post-processing steps, allowing you to focus on the core functionality of the Whisper model. By leveraging this class, you can easily integrate the Whisper model into your speech recognition pipeline, regardless of the specific model variant you choose."
      ],
      "id": "34bbdf5e-0e4c-482c-a08a-395972c8b56f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqh7J71HNsYG"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "model_ids = {\n",
        "    \"Distil-Whisper\": [\n",
        "        \"distil-whisper/distil-large-v2\",\n",
        "        \"distil-whisper/distil-medium.en\",\n",
        "        \"distil-whisper/distil-small.en\"\n",
        "    ],\n",
        "    \"Whisper\": [\n",
        "        \"openai/whisper-large-v3\",\n",
        "        \"openai/whisper-large-v2\",\n",
        "        \"openai/whisper-large\",\n",
        "        \"openai/whisper-medium\",\n",
        "        \"openai/whisper-small\",\n",
        "        \"openai/whisper-base\",\n",
        "        \"openai/whisper-tiny\",\n",
        "        \"openai/whisper-medium.en\",\n",
        "        \"openai/whisper-small.en\",\n",
        "        \"openai/whisper-base.en\",\n",
        "        \"openai/whisper-tiny.en\",\n",
        "    ]\n",
        "}\n",
        "\n",
        "model_type = widgets.Dropdown(\n",
        "    options=model_ids.keys(),\n",
        "    value=\"Distil-Whisper\",\n",
        "    description=\"Model type:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "model_type"
      ],
      "id": "uqh7J71HNsYG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHkwWcsrNsYG"
      },
      "outputs": [],
      "source": [
        "model_id = widgets.Dropdown(\n",
        "    options=model_ids[model_type.value],\n",
        "    value=model_ids[model_type.value][2],\n",
        "    description=\"Model:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "model_id"
      ],
      "id": "WHkwWcsrNsYG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5382431-497e-4688-b4ec-8958a92163e7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id.value)\n",
        "\n",
        "pt_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id.value)\n",
        "pt_model.eval();"
      ],
      "id": "e5382431-497e-4688-b4ec-8958a92163e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbe82d01-ea1e-433f-92c1-570f9c51c456"
      },
      "source": [
        "### Preparing the Input Sample\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "To use the Whisper model for speech recognition, we need to properly prepare the input audio sample. The `WhisperProcessor` expects the audio data to be in the form of a NumPy array, along with information about the audio sampling rate. It then processes the audio and returns the `input_features` tensor, which is used for making predictions.\n",
        "\n",
        "The conversion of the audio file to the required NumPy format is conveniently handled by the Hugging Face Datasets library. This library provides a seamless interface for loading and preprocessing audio data, making it easier to integrate with the Whisper model.\n",
        "\n",
        "To prepare the input sample, the next Python code:\n",
        "\n",
        "1. Loads the audio file using the Hugging Face Datasets library.\n",
        "2. Extracts the audio data as a NumPy array and obtain the sampling rate.\n",
        "3. Passes the audio array and sampling rate to the `WhisperProcessor`.\n",
        "4. Retrieves the `input_features` tensor from the processor."
      ],
      "id": "bbe82d01-ea1e-433f-92c1-570f9c51c456"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df5a5952-0457-4f1e-9dfe-0446c4cb0111"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def extract_input_features(sample):\n",
        "    input_features = processor(\n",
        "        sample[\"audio\"][\"array\"],\n",
        "        sampling_rate=sample[\"audio\"][\"sampling_rate\"],\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_features\n",
        "    return input_features\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
        ")\n",
        "sample = dataset[0]\n",
        "input_features = extract_input_features(sample)"
      ],
      "id": "df5a5952-0457-4f1e-9dfe-0446c4cb0111"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ff96530-4d3c-4a20-8ac0-b475794b54b5"
      },
      "source": [
        "### Running Model Inference\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "With the input sample prepared, we can now perform speech recognition using the Whisper model. The model provides a convenient `generate` interface that simplifies the inference process. Here's how you can run the model inference:\n",
        "\n",
        "1. Pass the `input_features` tensor to the `generate` method of the Whisper model.\n",
        "2. The model will process the input and generate the predicted token IDs.\n",
        "3. Once the generation is complete, use the `processor.batch_decode` method to decode the predicted token IDs into human-readable text transcription.\n",
        "\n",
        "The `generate` method handles the complex task of sequence generation, taking into account the model's architecture and the provided input features. It produces the predicted token IDs, which represent the transcribed text in a encoded format.\n",
        "\n",
        "By leveraging the `generate` interface and the `processor.batch_decode` method, you can easily perform speech recognition with the Whisper model. The model takes care of the complex task of mapping the audio input to text output, while the processor handles the necessary decoding step to provide you with the final transcription."
      ],
      "id": "5ff96530-4d3c-4a20-8ac0-b475794b54b5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9618867-beae-4875-a5be-0e0a3b453414"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "predicted_ids = pt_model.generate(input_features)\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
        "print(f\"Reference: {sample['text']}\")\n",
        "print(f\"Result: {transcription[0]}\")"
      ],
      "id": "c9618867-beae-4875-a5be-0e0a3b453414"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "219ed303-d323-4a07-8a92-66a2e96e1ec5"
      },
      "source": [
        "## Loading OpenVINO Model using Optimum Library\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "To further optimize the performance of the Whisper model, we can leverage the Hugging Face Optimum API. This high-level API enables us to convert and quantize models from the Hugging Face Transformers library to the OpenVINO™ IR format, resulting in faster inference and reduced memory footprint. For more details, refer to the [Hugging Face Optimum documentation](https://huggingface.co/docs/optimum/intel/inference).\n",
        "\n",
        "The Optimum Intel library provides a seamless way to load optimized models from the [Hugging Face Hub](https://huggingface.co/docs/optimum/intel/hf.co/models) and create pipelines for running inference with the OpenVINO Runtime using Hugging Face APIs. The Optimum Inference models are API-compatible with Hugging Face Transformers models, which means we only need to replace the `AutoModelForXxx` class with the corresponding `OVModelForXxx` class.\n",
        "\n",
        "To initialize the model class, we call the `from_pretrained` method. When downloading and converting the Transformers model, it's important to add the `export=True` parameter to ensure the model is properly converted to the OpenVINO format. After conversion, we can save the model for future use with the `save_pretrained` method.\n",
        "\n",
        "One advantage of using the Optimum library is that the tokenizers and processors distributed with the models are also compatible with the OpenVINO model. This means we can reuse the previously initialized processor without any modifications.\n"
      ],
      "id": "219ed303-d323-4a07-8a92-66a2e96e1ec5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ef523e8-b70f-4d86-a7d1-81f761c3eac0"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from optimum.intel.openvino import OVModelForSpeechSeq2Seq\n",
        "\n",
        "model_path = Path(model_id.value.replace('/', '_'))\n",
        "ov_config = {\"CACHE_DIR\": \"\"}\n",
        "\n",
        "if not model_path.exists():\n",
        "    ov_model = OVModelForSpeechSeq2Seq.from_pretrained(\n",
        "        model_id.value, ov_config=ov_config, export=True, compile=False, load_in_8bit=False\n",
        "    )\n",
        "    ov_model.half()\n",
        "    ov_model.save_pretrained(model_path)\n",
        "else:\n",
        "    ov_model = OVModelForSpeechSeq2Seq.from_pretrained(\n",
        "        model_path, ov_config=ov_config, compile=False\n",
        "    )"
      ],
      "id": "7ef523e8-b70f-4d86-a7d1-81f761c3eac0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a3dffb-5476-4a5d-843f-c7a7cbbf2154"
      },
      "source": [
        "### Selecting the Inference Device\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "When running inference with the OpenVINO runtime, it's important to select the appropriate inference device based on your hardware capabilities and performance requirements. The OpenVINO toolkit supports a wide range of devices, including CPUs, GPUs, and specialized accelerators.\n",
        "\n",
        "To select the inference device, you can use the `ov.Core` class from the `openvino` package. This class provides methods to query the available devices and set the desired device for inference.\n"
      ],
      "id": "99a3dffb-5476-4a5d-843f-c7a7cbbf2154"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b1bd73b-bcc8-4f72-b896-63e11f33f607"
      },
      "outputs": [],
      "source": [
        "import openvino as ov\n",
        "import ipywidgets as widgets\n",
        "\n",
        "core = ov.Core()\n",
        "\n",
        "device = widgets.Dropdown(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value=\"AUTO\",\n",
        "    description=\"Device:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "device"
      ],
      "id": "4b1bd73b-bcc8-4f72-b896-63e11f33f607"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45cd85e8-63e4-402c-86bc-2023ed5775a8"
      },
      "source": [
        "### Compiling the OpenVINO Model\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "After selecting the inference device, the next step is to compile the OpenVINO model for optimal performance on the chosen device. Compiling the model involves applying device-specific optimizations and generating an executable representation of the model that can be efficiently run on the target device.\n",
        "\n",
        "To compile the OpenVINO model, you can use the `compile` method of the `OVModelForSpeechSeq2Seq` class.\n"
      ],
      "id": "45cd85e8-63e4-402c-86bc-2023ed5775a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "057328a7-dc25-4c54-a438-85467e0076de"
      },
      "outputs": [],
      "source": [
        "ov_model.to(device.value)\n",
        "ov_model.compile()"
      ],
      "id": "057328a7-dc25-4c54-a438-85467e0076de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3590030d-5149-4f83-9e78-f6a582e1511a"
      },
      "source": [
        "### Running Inference with the OpenVINO Model\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "With the OpenVINO model compiled and optimized for the selected inference device, we can now run inference to perform speech recognition. The process of running inference with the OpenVINO model is similar to the PyTorch model, but with the added benefits of improved performance and efficiency.\n",
        "\n",
        "To run inference with the OpenVINO model, we follow these steps:\n",
        "\n",
        "1. Prepare the input sample by extracting the audio features using the `WhisperProcessor`, as explained in the [Preparing input sample](#Preparing-input-sample) section.\n",
        "\n",
        "2. Pass the input features to the `generate` method of the OpenVINO model, just like we did with the PyTorch model:\n",
        "\n",
        "   ```python\n",
        "   predicted_ids = ov_model.generate(input_features)\n",
        "   ```\n",
        "\n",
        "   The `generate` method takes the input features and performs the necessary computations using the optimized OpenVINO model to generate the predicted token IDs.\n",
        "\n",
        "3. Decode the predicted token IDs into human-readable text using the `processor.batch_decode` method:\n",
        "\n",
        "   ```python\n",
        "   transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "   ```\n",
        "\n",
        "   This step converts the predicted token IDs back into the corresponding text transcription, just like we did with the PyTorch model.\n"
      ],
      "id": "3590030d-5149-4f83-9e78-f6a582e1511a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68a94f38-09e9-48fc-9df0-6c954a82f2fb"
      },
      "outputs": [],
      "source": [
        "predicted_ids = ov_model.generate(input_features)\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
        "print(f\"Reference: {sample['text']}\")\n",
        "print(f\"Result: {transcription[0]}\")"
      ],
      "id": "68a94f38-09e9-48fc-9df0-6c954a82f2fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d69046e-707f-4c07-af24-389b125b3abd"
      },
      "source": [
        "## Comparing Performance: PyTorch vs OpenVINO\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Now that we have successfully run inference with both the PyTorch and OpenVINO models, it's crucial to compare their performance to understand the benefits of using OpenVINO optimization. Performance comparison helps us evaluate the speed and efficiency improvements achieved by converting the model to the OpenVINO format and utilizing the optimized inference runtime.\n",
        "\n",
        "To compare the performance of the PyTorch and OpenVINO models, we can measure and analyze the following metrics:\n",
        "\n",
        "1. **Inference Time**: Measure the time taken by each model to process an input audio sample and generate the transcription. This includes the time spent on preprocessing, inference, and postprocessing steps. By comparing the inference times, we can determine which model offers faster execution.\n",
        "\n",
        "2. **Throughput**: Evaluate the number of audio samples that each model can process per unit of time. Higher throughput indicates better efficiency and the ability to handle a larger volume of audio data in a given timeframe. OpenVINO optimization aims to improve throughput by leveraging hardware-specific optimizations and efficient resource utilization.\n",
        "\n",
        "3. **Memory Usage**: Monitor the memory consumption of each model during inference. Efficient memory management is crucial, especially when dealing with limited resources or running inference on edge devices. OpenVINO optimization techniques, such as model compression and quantization, can help reduce memory usage without significant impact on accuracy.\n",
        "\n",
        "4. **CPU and GPU Utilization**: Analyze the utilization of CPU and GPU resources during inference for both models. OpenVINO optimization aims to maximize the utilization of available hardware resources, enabling efficient parallel processing and reducing idle time. By comparing resource utilization, we can assess how effectively each model harnesses the capabilities of the underlying hardware.\n",
        "\n",
        "To perform the performance comparison, you can use the `measure_perf` function provided in the notebook. This function takes the model and input sample as arguments and measures the inference time over a specified number of iterations. It provides a reliable estimate of the model's performance.\n",
        "\n",
        "```python\n",
        "perf_torch = measure_perf(pt_model, sample)\n",
        "perf_ov = measure_perf(ov_model, sample)\n",
        "```\n",
        "\n",
        "By comparing the performance metrics between the PyTorch and OpenVINO models, you can quantify the speed and efficiency improvements achieved through OpenVINO optimization. This comparison highlights the benefits of using OpenVINO for accelerating inference and making the most of the available hardware resources.\n",
        "\n"
      ],
      "id": "8d69046e-707f-4c07-af24-389b125b3abd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ddafe5c-3238-40d3-b8ed-9d50c73f0d8a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def measure_perf(model, sample, n=10):\n",
        "    timers = []\n",
        "    input_features = extract_input_features(sample)\n",
        "    for _ in tqdm(range(n), desc=\"Measuring performance\"):\n",
        "        start = time.perf_counter()\n",
        "        model.generate(input_features)\n",
        "        end = time.perf_counter()\n",
        "        timers.append(end - start)\n",
        "    return np.median(timers)"
      ],
      "id": "6ddafe5c-3238-40d3-b8ed-9d50c73f0d8a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94025726-5c09-42b8-9046-9fbbe73afc47"
      },
      "outputs": [],
      "source": [
        "perf_torch = measure_perf(pt_model, sample)\n",
        "perf_ov = measure_perf(ov_model, sample)"
      ],
      "id": "94025726-5c09-42b8-9046-9fbbe73afc47"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31a08241-497e-4fd9-9ca9-d59c2602b8d4"
      },
      "outputs": [],
      "source": [
        "print(f\"Mean torch {model_id.value} generation time: {perf_torch:.3f}s\")\n",
        "print(f\"Mean openvino {model_id.value} generation time: {perf_ov:.3f}s\")\n",
        "print(f\"Performance {model_id.value} openvino speedup: {perf_torch / perf_ov:.3f}\")"
      ],
      "id": "31a08241-497e-4fd9-9ca9-d59c2602b8d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "776cdf70"
      },
      "source": [
        "## Integrating the OpenVINO Model with Hugging Face Pipelines\n",
        "\n",
        "One of the key advantages of using the OpenVINO model is its seamless compatibility with the Hugging Face [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) interface for `automatic-speech-recognition`. This compatibility allows us to leverage the powerful features and utilities provided by the Hugging Face ecosystem while benefiting from the performance optimizations of OpenVINO.\n",
        "\n",
        "The `pipeline` interface is particularly useful for transcribing long audio files. Distil-Whisper employs a chunked algorithm that efficiently processes long-form audio by breaking it down into smaller segments. This chunked approach has been shown to be 9 times faster than the sequential algorithm proposed in the original OpenAI Whisper paper. To enable chunking, you can pass the `chunk_length_s` parameter to the pipeline, specifying the desired chunk length in seconds. For Distil-Whisper, a chunk length of 15 seconds has been found to be optimal.\n",
        "\n",
        "In the next Python code cells, we create a pipeline for automatic speech recognition using the OpenVINO model (`ov_model`). We set the `generation_config` of the OpenVINO model to match that of the PyTorch model to ensure consistent behavior. The `tokenizer` and `feature_extractor` from the `processor` are also passed to the pipeline to handle the necessary preprocessing and postprocessing steps.\n",
        "\n",
        "By specifying `max_new_tokens`, we limit the maximum number of tokens generated during transcription. The `chunk_length_s` parameter is set to 15 seconds to enable efficient chunking of long audio files. Additionally, we can activate batching by passing the `batch_size` argument, which allows the pipeline to process multiple audio samples concurrently.\n",
        "\n",
        "Using the pipeline with the OpenVINO model offers several benefits:\n",
        "\n",
        "1. **Simplified API**: The pipeline provides a high-level and intuitive interface for performing automatic speech recognition, abstracting away the complexities of the underlying model and preprocessing steps.\n",
        "\n",
        "2. **Long Audio Support**: With the chunked algorithm and the ability to specify the chunk length, the pipeline can efficiently handle long audio files, making it suitable for various real-world scenarios.\n",
        "\n",
        "3. **Batching**: By enabling batching through the `batch_size` parameter, the pipeline can process multiple audio samples simultaneously, improving overall throughput and utilization of hardware resources.\n",
        "\n",
        "4. **Performance Optimization**: The OpenVINO model, being optimized for inference, delivers faster and more efficient transcription compared to the PyTorch model, while maintaining comparable accuracy.\n",
        "\n",
        "By leveraging the Hugging Face pipeline with the OpenVINO model, you can easily integrate state-of-the-art speech recognition capabilities into your applications, benefiting from the ease of use, flexibility, and performance optimizations provided by this powerful combination.\n"
      ],
      "id": "776cdf70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68632b9f-28b8-4e91-854c-b014aed67e5d"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ov_model.generation_config = pt_model.generation_config\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=ov_model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    chunk_length_s=15,\n",
        "    batch_size=16,\n",
        ")"
      ],
      "id": "68632b9f-28b8-4e91-854c-b014aed67e5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ba3f7a-9dbd-4392-a98c-e1eca9372854"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
        "sample_long = dataset[0]\n",
        "\n",
        "\n",
        "def format_timestamp(seconds: float):\n",
        "    \"\"\"\n",
        "    format time in srt-file expected format\n",
        "    \"\"\"\n",
        "    assert seconds >= 0, \"non-negative timestamp expected\"\n",
        "    milliseconds = round(seconds * 1000.0)\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    return (\n",
        "        f\"{hours}:\" if hours > 0 else \"00:\"\n",
        "    ) + f\"{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
        "\n",
        "\n",
        "def prepare_srt(transcription):\n",
        "    \"\"\"\n",
        "    Format transcription into srt file format\n",
        "    \"\"\"\n",
        "    segment_lines = []\n",
        "    for idx, segment in enumerate(transcription[\"chunks\"]):\n",
        "        segment_lines.append(str(idx + 1) + \"\\n\")\n",
        "        timestamps = segment[\"timestamp\"]\n",
        "        time_start = format_timestamp(timestamps[0])\n",
        "        time_end = format_timestamp(timestamps[1])\n",
        "        time_str = f\"{time_start} --> {time_end}\\n\"\n",
        "        segment_lines.append(time_str)\n",
        "        segment_lines.append(segment[\"text\"] + \"\\n\\n\")\n",
        "    return segment_lines"
      ],
      "id": "53ba3f7a-9dbd-4392-a98c-e1eca9372854"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtaining Timestamps and Generating Subtitles\n",
        "The `pipeline` interface provides a convenient way to obtain timestamps associated with each processed chunk of audio. By passing the `return_timestamps` argument to the pipeline, you can retrieve the start and end timestamps of the speech segments corresponding to each chunk.\n",
        "\n",
        "Timestamps can be incredibly useful in various scenarios, such as:\n",
        "\n",
        "1. **Speech Separation**: If you have an audio file containing multiple speakers, the timestamps can help you identify and separate the speech segments corresponding to each speaker. This information can be used to create speaker-specific transcriptions or to isolate individual speakers for further analysis.\n",
        "\n",
        "2. **Video Subtitles**: Timestamps play a crucial role in generating subtitles for videos. By aligning the transcribed text with the appropriate timestamps, you can create synchronized subtitles that match the speech in the video. This enhances the accessibility and comprehension of the video content.\n",
        "\n",
        "In the provided example, we demonstrate how to format the transcription output in the popular SRT (SubRip Text) format, which is widely used for video subtitles. The SRT format consists of four parts for each subtitle:\n",
        "\n",
        "1. Subtitle number\n",
        "2. Start and end timestamps\n",
        "3. Subtitle text\n",
        "4. Blank line"
      ],
      "metadata": {
        "id": "EzSPOc9608yz"
      },
      "id": "EzSPOc9608yz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3219bb35-955a-4032-acf1-d83e5dab09bd"
      },
      "outputs": [],
      "source": [
        "result = pipe(sample_long[\"audio\"].copy(), return_timestamps=True)"
      ],
      "id": "3219bb35-955a-4032-acf1-d83e5dab09bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd7ef03b-3c71-4f3a-9a9c-40549256b447"
      },
      "outputs": [],
      "source": [
        "srt_lines = prepare_srt(result)\n",
        "\n",
        "display(\n",
        "    ipd.Audio(sample_long[\"audio\"][\"array\"], rate=sample_long[\"audio\"][\"sampling_rate\"])\n",
        ")\n",
        "print(\"\".join(srt_lines))"
      ],
      "id": "bd7ef03b-3c71-4f3a-9a9c-40549256b447"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b36d31bc"
      },
      "source": [
        "## Quantizing the model using NNCF\n",
        "\n",
        "Quantization is a popular technique for reducing the memory footprint and computational complexity of deep learning models. It involves converting the model's weights and activations from floating-point precision to lower-precision representations, such as INT8. Quantization can significantly speed up inference and make models more efficient, especially for deployment on resource-constrained devices.\n",
        "\n",
        "The [Neural Network Compression Framework (NNCF)](https://github.com/openvinotoolkit/nncf/) is a powerful tool that enables post-training quantization of models. It works by adding quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these quantization layers. NNCF is designed to minimize the modifications required to your original training code, making the quantization process straightforward.\n",
        "\n",
        "The quantization process using NNCF involves the following steps:\n",
        "\n",
        "1. **Create a calibration dataset**: Select a representative subset of your training data to be used for calibrating the quantization parameters. This dataset should cover a wide range of input variations to ensure accurate quantization.\n",
        "\n",
        "2. **Run `nncf.quantize`**: Pass your model and the calibration dataset to the `nncf.quantize` function. NNCF will add quantization layers to the model graph and optimize the quantization parameters using the calibration dataset. This step produces quantized versions of the encoder and decoder models.\n",
        "\n",
        "3. **Serialize the quantized model**: Use the `openvino.save_model` function to serialize the quantized INT8 model. This step saves the quantized model in a format that can be efficiently loaded and executed using the OpenVINO runtime.\n",
        "\n",
        "It's important to note that quantization is a time and memory-consuming operation, especially for large models like Distil-Whisper. Running the quantization code provided in this notebook may take some time, depending on the size of your calibration dataset and the available computational resources.\n",
        "\n",
        "To facilitate experimentation, the notebook includes a checkbox that allows you to select whether you would like to run the quantization process for Distil-Whisper. If you choose to proceed with quantization, the notebook will guide you through the necessary steps and provide detailed explanations along the way.\n",
        "\n",
        "Quantization can yield significant performance improvements and memory savings, making it a valuable technique for deploying models in production environments. By leveraging NNCF and the OpenVINO runtime, you can easily quantize your Distil-Whisper model and benefit from faster inference and reduced memory consumption."
      ],
      "id": "b36d31bc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58d361c3"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "to_quantize = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Quantization',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "to_quantize"
      ],
      "id": "58d361c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46cc97d3"
      },
      "outputs": [],
      "source": [
        "# Fetch notebook_utils module\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/skip_kernel_extension.py',\n",
        "    filename='skip_kernel_extension.py'\n",
        ")\n",
        "\n",
        "%load_ext skip_kernel_extension"
      ],
      "id": "46cc97d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfb2c2a7"
      },
      "source": [
        "### Preparing Calibration Datasets\n",
        "\n",
        "The first step in the quantization process is to prepare calibration datasets. Calibration datasets are used to optimize the quantization parameters and ensure that the quantized model maintains acceptable accuracy. Since we quantize the Whisper encoder and decoder separately, we need to prepare a separate calibration dataset for each model.\n",
        "\n",
        "To collect the calibration data, we use the `InferRequestWrapper` class, which intercepts the model inputs during inference and collects them into a list. This allows us to capture the input data that will be used for calibration.\n",
        "\n",
        "Here's an overview of the steps to prepare the calibration datasets:\n",
        "\n",
        "1. Import the `InferRequestWrapper` class from the `optimum.intel.openvino.quantization` module.\n",
        "\n",
        "2. Initialize an instance of the `InferRequestWrapper` class for both the encoder and decoder models. This wrapper will intercept the model inputs during inference.\n",
        "\n",
        "3. Run model inference on a small subset of audio samples. These samples should be representative of the data the model will encounter in production.\n",
        "\n",
        "4. The `InferRequestWrapper` will collect the input data for each model and store it in separate lists for the encoder and decoder.\n",
        "\n",
        "5. The collected input data will serve as the calibration datasets for quantization.\n",
        "\n",
        "It's important to note that the size of the calibration dataset can impact the quality of the quantized model. Generally, increasing the calibration dataset size improves quantization quality. However, using a larger dataset also increases the time and computational resources required for calibration.\n",
        "\n",
        "When selecting the calibration dataset, aim for a balance between representativeness and size. Choose a diverse set of audio samples that cover different speakers, accents, and recording conditions. This will help the quantization process capture a wide range of input variations and optimize the quantization parameters accordingly."
      ],
      "id": "cfb2c2a7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96d6b01e"
      },
      "outputs": [],
      "source": [
        "%%skip not $to_quantize.value\n",
        "\n",
        "from itertools import islice\n",
        "from optimum.intel.openvino.quantization import InferRequestWrapper\n",
        "\n",
        "\n",
        "def collect_calibration_dataset(ov_model: OVModelForSpeechSeq2Seq, calibration_dataset_size: int):\n",
        "    # Overwrite model request properties, saving the original ones for restoring later\n",
        "    original_encoder_request = ov_model.encoder.request\n",
        "    original_decoder_with_past_request = ov_model.decoder_with_past.request\n",
        "    encoder_calibration_data = []\n",
        "    decoder_calibration_data = []\n",
        "    ov_model.encoder.request = InferRequestWrapper(original_encoder_request, encoder_calibration_data)\n",
        "    ov_model.decoder_with_past.request = InferRequestWrapper(original_decoder_with_past_request,\n",
        "                                                             decoder_calibration_data)\n",
        "\n",
        "    calibration_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\n",
        "    for sample in tqdm(islice(calibration_dataset, calibration_dataset_size), desc=\"Collecting calibration data\",\n",
        "                       total=calibration_dataset_size):\n",
        "        input_features = extract_input_features(sample)\n",
        "        ov_model.generate(input_features)\n",
        "\n",
        "    ov_model.encoder.request = original_encoder_request\n",
        "    ov_model.decoder_with_past.request = original_decoder_with_past_request\n",
        "\n",
        "    return encoder_calibration_data, decoder_calibration_data"
      ],
      "id": "96d6b01e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "023f2eff"
      },
      "source": [
        "### Quantizing Distil-Whisper Encoder and Decoder Models\n",
        "\n",
        "With the calibration datasets prepared, we can now proceed to quantize the Distil-Whisper encoder and decoder models. The `quantize` function in the notebook encapsulates the quantization process by calling `nncf.quantize` on the encoder and decoder-with-past models.\n",
        "\n",
        "Here's a breakdown of the quantization steps:\n",
        "\n",
        "1. The `quantize` function takes the OpenVINO model (`ov_model`) and the calibration dataset size as arguments.\n",
        "\n",
        "2. Inside the function, we first check if the quantized model path already exists. If not, we proceed with the quantization process.\n",
        "\n",
        "3. The calibration datasets for the encoder and decoder models are collected using the `collect_calibration_dataset` function, which utilizes the `InferRequestWrapper` to intercept model inputs during inference.\n",
        "\n",
        "4. We quantize the encoder model by calling `nncf.quantize` with the following arguments:\n",
        "   - The encoder model (`ov_model.encoder.model`)\n",
        "   - The encoder calibration dataset\n",
        "   - The subset size (equal to the size of the encoder calibration dataset)\n",
        "   - The model type (set to `nncf.ModelType.TRANSFORMER`)\n",
        "   - Advanced quantization parameters (e.g., `smooth_quant_alpha` for Smooth Quant algorithm)\n",
        "\n",
        "5. The quantized encoder model is saved using the `openvino.save_model` function.\n",
        "\n",
        "6. We repeat the quantization process for the decoder-with-past model, using the decoder calibration dataset and appropriate quantization parameters.\n",
        "\n",
        "7. The quantized decoder-with-past model is saved using the `openvino.save_model` function.\n",
        "\n",
        "It's important to note that we don't quantize the first-step-decoder model because its contribution to the overall inference time is negligible. Quantizing the encoder and decoder-with-past models, which account for the majority of the computation, provides the most significant performance improvements.\n",
        "\n",
        "By quantizing the encoder and decoder models separately, we can optimize each model's quantization parameters based on its specific input characteristics and computational demands. This targeted quantization approach ensures that we achieve the best balance between performance and accuracy for each model component.\n",
        "\n",
        "The `quantize` function abstracts away the complexities of the quantization process, making it easy to apply quantization to the Distil-Whisper model using NNCF. Once the quantization is complete, the quantized models are saved and ready for inference.\n"
      ],
      "id": "023f2eff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0de8bd26"
      },
      "outputs": [],
      "source": [
        "%%skip not $to_quantize.value\n",
        "\n",
        "import gc\n",
        "import shutil\n",
        "import nncf\n",
        "\n",
        "CALIBRATION_DATASET_SIZE = 40 # reducing from original 50\n",
        "quantized_model_path = Path(f\"{model_path}_quantized\")\n",
        "\n",
        "\n",
        "def quantize(ov_model: OVModelForSpeechSeq2Seq, calibration_dataset_size: int):\n",
        "    if not quantized_model_path.exists():\n",
        "        encoder_calibration_data, decoder_calibration_data = collect_calibration_dataset(\n",
        "            ov_model, calibration_dataset_size\n",
        "        )\n",
        "        print(\"Quantizing encoder\")\n",
        "        quantized_encoder = nncf.quantize(\n",
        "            ov_model.encoder.model,\n",
        "            nncf.Dataset(encoder_calibration_data),\n",
        "            subset_size=len(encoder_calibration_data),\n",
        "            model_type=nncf.ModelType.TRANSFORMER,\n",
        "            # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
        "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.50)\n",
        "        )\n",
        "        ov.save_model(quantized_encoder, quantized_model_path / \"openvino_encoder_model.xml\")\n",
        "        del quantized_encoder\n",
        "        del encoder_calibration_data\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Quantizing decoder with past\")\n",
        "        quantized_decoder_with_past = nncf.quantize(\n",
        "            ov_model.decoder_with_past.model,\n",
        "            nncf.Dataset(decoder_calibration_data),\n",
        "            subset_size=len(decoder_calibration_data),\n",
        "            model_type=nncf.ModelType.TRANSFORMER,\n",
        "            # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
        "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.95)\n",
        "        )\n",
        "        ov.save_model(quantized_decoder_with_past, quantized_model_path / \"openvino_decoder_with_past_model.xml\")\n",
        "        del quantized_decoder_with_past\n",
        "        del decoder_calibration_data\n",
        "        gc.collect()\n",
        "\n",
        "        # Copy the config file and the first-step-decoder manually\n",
        "        shutil.copy(model_path / \"config.json\", quantized_model_path / \"config.json\")\n",
        "        shutil.copy(model_path / \"openvino_decoder_model.xml\", quantized_model_path / \"openvino_decoder_model.xml\")\n",
        "        shutil.copy(model_path / \"openvino_decoder_model.bin\", quantized_model_path / \"openvino_decoder_model.bin\")\n",
        "\n",
        "    quantized_ov_model = OVModelForSpeechSeq2Seq.from_pretrained(quantized_model_path, ov_config=ov_config, compile=False)\n",
        "    quantized_ov_model.to(device.value)\n",
        "    quantized_ov_model.compile()\n",
        "    return quantized_ov_model\n",
        "\n",
        "\n",
        "ov_quantized_model = quantize(ov_model, CALIBRATION_DATASET_SIZE)"
      ],
      "id": "0de8bd26"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b06ca107"
      },
      "source": [
        "### Running Inference with the Quantized Model\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Now that we have quantized the Distil-Whisper encoder and decoder models, let's compare the transcription results between the original and quantized models. Running inference with the quantized model allows us to assess the impact of quantization on the model's output and verify that the quantized model maintains acceptable accuracy.\n",
        "\n",
        "To run inference with the quantized model, we follow these steps:\n",
        "\n",
        "1. Load a sample audio file for transcription. This can be done using the `load_dataset` function from the Hugging Face Datasets library.\n",
        "\n",
        "2. Prepare the audio input for the model by extracting the input features using the `extract_input_features` function. This function applies the necessary preprocessing steps, such as converting the audio to a spectrogram representation.\n",
        "\n",
        "3. Run inference with the original OpenVINO model (`ov_model`) using the `generate` method. This generates the predicted token IDs for the original model.\n",
        "\n",
        "4. Decode the predicted token IDs from the original model into a human-readable transcription using the `processor.batch_decode` function.\n",
        "\n",
        "5. Run inference with the quantized OpenVINO model (`ov_quantized_model`) using the `generate` method. This generates the predicted token IDs for the quantized model.\n",
        "\n",
        "6. Decode the predicted token IDs from the quantized model into a human-readable transcription using the `processor.batch_decode` function.\n",
        "\n",
        "7. Compare the transcription results from the original and quantized models to assess the impact of quantization on the model's output.\n"
      ],
      "id": "b06ca107"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b6eed2a"
      },
      "outputs": [],
      "source": [
        "%%skip not $to_quantize.value\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
        ")\n",
        "sample = dataset[0]\n",
        "input_features = extract_input_features(sample)\n",
        "\n",
        "predicted_ids = ov_model.generate(input_features)\n",
        "transcription_original = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "predicted_ids = ov_quantized_model.generate(input_features)\n",
        "transcription_quantized = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n",
        "print(f\"Original : {transcription_original[0]}\")\n",
        "print(f\"Quantized: {transcription_quantized[0]}\")"
      ],
      "id": "7b6eed2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3228cf53"
      },
      "source": [
        "Results are the same!"
      ],
      "id": "3228cf53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c68cb960"
      },
      "source": [
        "### Comparing performance and accuracy of the original and quantized models\n",
        "\n",
        "To evaluate the effectiveness of the quantization process, we need to compare the performance and accuracy of the original and quantized Distil-Whisper models. This comparison will help us understand the trade-offs introduced by quantization and assess whether the quantized model meets the desired performance and accuracy criteria.\n",
        "\n",
        "#### Measuring Accuracy\n",
        "To measure the accuracy of the models, we use the Word Error Rate (WER) as the evaluation metric. WER is a common metric used in speech recognition tasks, and it quantifies the percentage of words that are incorrectly recognized by the model. A lower WER indicates better accuracy.\n",
        "\n",
        "In this comparison, we calculate the accuracy using the formula: `Accuracy = 1 - WER`. By subtracting the WER from 1, we obtain the percentage of correctly recognized words.\n",
        "\n",
        "To calculate the WER, we compare the transcriptions generated by the models against the ground truth transcriptions. The WER is computed by aligning the predicted and ground truth transcriptions and counting the number of word insertions, deletions, and substitutions.\n",
        "\n",
        "#### Measuring Inference Time\n",
        "In addition to accuracy, we also measure the inference time of the models to assess their performance. Inference time is a critical metric, especially in real-time or resource-constrained scenarios where fast processing is essential.\n",
        "\n",
        "We measure the inference time separately for three components of the Distil-Whisper model:\n",
        "1. Encoder: We measure the time taken by the encoder model to process the input audio features and generate the encoded representation.\n",
        "2. Decoder-with-past: We measure the time taken by the decoder-with-past model to generate the output tokens based on the encoded representation and the previous tokens.\n",
        "3. Whole model: We measure the end-to-end inference time, including the time taken by both the encoder and decoder-with-past models, as well as any additional processing steps.\n",
        "\n",
        "By measuring the inference time for each component, we can identify potential bottlenecks and understand the impact of quantization on different parts of the model.\n",
        "\n",
        "#### Comparing Performance and Accuracy\n",
        "To compare the performance and accuracy of the original and quantized models, we follow these steps:\n",
        "\n",
        "1. Prepare a test dataset of audio samples that are representative of the intended use case.\n",
        "\n",
        "2. Run inference on the test dataset using both the original and quantized models.\n",
        "\n",
        "3. Calculate the accuracy (1 - WER) for each model by comparing the generated transcriptions against the ground truth.\n",
        "\n",
        "4. Measure the inference time for the encoder, decoder-with-past, and whole model for both the original and quantized models.\n",
        "\n",
        "5. Compare the accuracy and inference time results between the original and quantized models.\n",
        "\n",
        "By analyzing the accuracy and inference time metrics, we can determine the impact of quantization on the model's performance. Ideally, the quantized model should achieve similar accuracy to the original model while providing a significant reduction in inference time.\n",
        "\n",
        "If the quantized model maintains comparable accuracy and demonstrates improved inference speed, it indicates that the quantization process has successfully optimized the model for faster execution without compromising its recognition quality.\n",
        "\n",
        "The comparison of performance and accuracy between the original and quantized models is crucial for making informed decisions about deploying the quantized model in production environments. It helps strike the right balance between performance and accuracy based on the specific requirements of the application."
      ],
      "id": "c68cb960"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7133f52f"
      },
      "outputs": [],
      "source": [
        "%%skip not $to_quantize.value\n",
        "\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from jiwer import wer, wer_standardize\n",
        "\n",
        "\n",
        "TEST_DATASET_SIZE = 50\n",
        "MEASURE_TIME = False\n",
        "\n",
        "@contextmanager\n",
        "def time_measurement():\n",
        "    global MEASURE_TIME\n",
        "    try:\n",
        "        MEASURE_TIME = True\n",
        "        yield\n",
        "    finally:\n",
        "        MEASURE_TIME = False\n",
        "\n",
        "def time_fn(obj, fn_name, time_list):\n",
        "    original_fn = getattr(obj, fn_name)\n",
        "\n",
        "    def wrapper(*args, **kwargs):\n",
        "        if not MEASURE_TIME:\n",
        "            return original_fn(*args, **kwargs)\n",
        "        start_time = time.perf_counter()\n",
        "        result = original_fn(*args, **kwargs)\n",
        "        end_time = time.perf_counter()\n",
        "        time_list.append(end_time - start_time)\n",
        "        return result\n",
        "\n",
        "    setattr(obj, fn_name, wrapper)\n",
        "\n",
        "def calculate_transcription_time_and_accuracy(ov_model, test_samples):\n",
        "    encoder_infer_times = []\n",
        "    decoder_with_past_infer_times = []\n",
        "    whole_infer_times = []\n",
        "    time_fn(ov_model, \"generate\", whole_infer_times)\n",
        "    time_fn(ov_model.encoder, \"forward\", encoder_infer_times)\n",
        "    time_fn(ov_model.decoder_with_past, \"forward\", decoder_with_past_infer_times)\n",
        "\n",
        "    ground_truths = []\n",
        "    predictions = []\n",
        "    for data_item in tqdm(test_samples, desc=\"Measuring performance and accuracy\"):\n",
        "        input_features = extract_input_features(data_item)\n",
        "\n",
        "        with time_measurement():\n",
        "            predicted_ids = ov_model.generate(input_features)\n",
        "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "\n",
        "        ground_truths.append(data_item[\"text\"])\n",
        "        predictions.append(transcription[0])\n",
        "\n",
        "    word_accuracy = (1 - wer(ground_truths, predictions, reference_transform=wer_standardize,\n",
        "                             hypothesis_transform=wer_standardize)) * 100\n",
        "    mean_whole_infer_time = sum(whole_infer_times)\n",
        "    mean_encoder_infer_time = sum(encoder_infer_times)\n",
        "    mean_decoder_with_time_infer_time = sum(decoder_with_past_infer_times)\n",
        "    return word_accuracy, (mean_whole_infer_time, mean_encoder_infer_time, mean_decoder_with_time_infer_time)\n",
        "\n",
        "test_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\", streaming=True)\n",
        "test_dataset = test_dataset.shuffle(seed=42).take(TEST_DATASET_SIZE)\n",
        "test_samples = [sample for sample in test_dataset]\n",
        "\n",
        "accuracy_original, times_original = calculate_transcription_time_and_accuracy(ov_model, test_samples)\n",
        "accuracy_quantized, times_quantized = calculate_transcription_time_and_accuracy(ov_quantized_model, test_samples)\n",
        "print(f\"Encoder performance speedup: {times_original[1] / times_quantized[1]:.3f}\")\n",
        "print(f\"Decoder with past performance speedup: {times_original[2] / times_quantized[2]:.3f}\")\n",
        "print(f\"Whole pipeline performance speedup: {times_original[0] / times_quantized[0]:.3f}\")\n",
        "print(f\"Whisper transcription word accuracy. Original model: {accuracy_original:.2f}%. Quantized model: {accuracy_quantized:.2f}%.\")\n",
        "print(f\"Accuracy drop: {accuracy_original - accuracy_quantized:.2f}%.\")"
      ],
      "id": "7133f52f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29bcda7d"
      },
      "source": [
        "As we can see quantization significantly improves model inference time without major accuracy drop!"
      ],
      "id": "29bcda7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "450e13ed-197c-4743-94f8-4e0a206c1b76"
      },
      "source": [
        "## Interactive Demo: Experience Distil-Whisper in Action\n",
        "\n",
        "To showcase the capabilities of the Distil-Whisper model and provide a hands-on experience, we have developed an interactive demo using the Gradio interface. This demo allows you to test the model's speech recognition performance on your own audio data, making it easy to explore and evaluate the model's effectiveness.\n",
        "\n",
        "### Features of the Interactive Demo\n",
        "1. **Audio Upload**: You can upload your own audio files using the provided upload button. This enables you to test the model's performance on a wide range of audio samples, including different speakers, accents, and recording conditions.\n",
        "\n",
        "2. **Microphone Input**: In addition to audio file upload, the demo also supports real-time audio input through your microphone. You can record your own speech directly within the interface and observe how the model transcribes it in real-time.\n",
        "\n",
        "3. **Transcription Output**: The demo displays the transcribed text output for each audio input, allowing you to evaluate the model's accuracy and performance instantly. You can compare the generated transcriptions against the actual spoken content to assess the model's recognition quality.\n",
        "\n",
        "4. **Quantized Model Comparison**: If you have chosen to run the quantization process, the demo provides a side-by-side comparison of the original and quantized models. You can input the same audio and observe the transcription results from both models, enabling you to evaluate the impact of quantization on the model's output.\n",
        "\n",
        "### Limitations and Future Enhancements\n",
        "Please note that the current version of Distil-Whisper is specifically trained for English speech recognition. While it excels at transcribing English audio, it may not perform optimally for other languages. However, we are actively working on extending the model's capabilities to support multiple languages in the future.\n",
        "\n",
        "Multilingual support will greatly expand the utility of Distil-Whisper, enabling its application in a wider range of scenarios and catering to a global user base. Stay tuned for updates on the multilingual version of Distil-Whisper, which will be released in the near future.\n",
        "\n",
        "### Getting Started with the Interactive Demo\n",
        "To access the interactive demo, simply run the provided code cells in the notebook. The demo will launch using the Gradio interface, and you can start experimenting with your own audio data right away.\n",
        "\n",
        "We encourage you to explore the demo extensively, testing the model's performance on various audio samples and evaluating its transcription accuracy. Provide feedback and report any issues you encounter, as your input is valuable in improving the model and the demo experience.\n",
        "\n",
        "By interacting with the demo, you will gain a hands-on understanding of Distil-Whisper's capabilities and potential applications. Whether you are a researcher, developer, or enthusiast in the field of speech recognition, this interactive demo provides a convenient way to explore and leverage the power of the Distil-Whisper model."
      ],
      "id": "450e13ed-197c-4743-94f8-4e0a206c1b76"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96b18c5b-caaf-4f2a-8535-61f4b807ac94"
      },
      "outputs": [],
      "source": [
        "from transformers.pipelines.audio_utils import ffmpeg_read\n",
        "import gradio as gr\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    url=\"https://huggingface.co/spaces/distil-whisper/whisper-vs-distil-whisper/resolve/main/assets/example_1.wav\",\n",
        "    filename=\"example_1.wav\",\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_AUDIO_MINS = 30  # maximum audio input in minutes\n",
        "\n",
        "\n",
        "generate_kwargs = {\"language\": \"en\", \"task\": \"transcribe\"} if not model_id.value.endswith(\".en\") else {}\n",
        "ov_pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=ov_model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    max_new_tokens=128,\n",
        "    chunk_length_s=15,\n",
        "    generate_kwargs=generate_kwargs,\n",
        ")\n",
        "ov_pipe_forward = ov_pipe._forward\n",
        "\n",
        "if to_quantize.value:\n",
        "    ov_quantized_model.generation_config = ov_model.generation_config\n",
        "    ov_quantized_pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=ov_quantized_model,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        feature_extractor=processor.feature_extractor,\n",
        "        max_new_tokens=128,\n",
        "        chunk_length_s=15,\n",
        "        generate_kwargs=generate_kwargs,\n",
        "    )\n",
        "    ov_quantized_pipe_forward = ov_quantized_pipe._forward\n",
        "\n",
        "\n",
        "def transcribe(inputs, quantized=False):\n",
        "    pipe = ov_quantized_pipe if quantized else ov_pipe\n",
        "    pipe_forward = ov_quantized_pipe_forward if quantized else ov_pipe_forward\n",
        "\n",
        "    if inputs is None:\n",
        "        raise gr.Error(\n",
        "            \"No audio file submitted! Please record or upload an audio file before submitting your request.\"\n",
        "        )\n",
        "\n",
        "    with open(inputs, \"rb\") as f:\n",
        "        inputs = f.read()\n",
        "\n",
        "    inputs = ffmpeg_read(inputs, pipe.feature_extractor.sampling_rate)\n",
        "    audio_length_mins = len(inputs) / pipe.feature_extractor.sampling_rate / 60\n",
        "\n",
        "    if audio_length_mins > MAX_AUDIO_MINS:\n",
        "        raise gr.Error(\n",
        "            f\"To ensure fair usage of the Space, the maximum audio length permitted is {MAX_AUDIO_MINS} minutes.\"\n",
        "            f\"Got an audio of length {round(audio_length_mins, 3)} minutes.\"\n",
        "        )\n",
        "\n",
        "    inputs = {\"array\": inputs, \"sampling_rate\": pipe.feature_extractor.sampling_rate}\n",
        "\n",
        "    def _forward_ov_time(*args, **kwargs):\n",
        "        global ov_time\n",
        "        start_time = time.time()\n",
        "        result = pipe_forward(*args, **kwargs)\n",
        "        ov_time = time.time() - start_time\n",
        "        ov_time = round(ov_time, 2)\n",
        "        return result\n",
        "\n",
        "    pipe._forward = _forward_ov_time\n",
        "    ov_text = pipe(inputs.copy(), batch_size=BATCH_SIZE)[\"text\"]\n",
        "    return ov_text, ov_time\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "                <div style=\"text-align: center; max-width: 700px; margin: 0 auto;\">\n",
        "                  <div\n",
        "                    style=\"\n",
        "                      display: inline-flex; align-items: center; gap: 0.8rem; font-size: 1.75rem;\n",
        "                    \"\n",
        "                  >\n",
        "                    <h1 style=\"font-weight: 900; margin-bottom: 7px; line-height: normal;\">\n",
        "                      OpenVINO Distil-Whisper demo\n",
        "                    </h1>\n",
        "                  </div>\n",
        "                </div>\n",
        "            \"\"\"\n",
        "    )\n",
        "    audio = gr.components.Audio(type=\"filepath\", label=\"Audio input\")\n",
        "    with gr.Row():\n",
        "        button = gr.Button(\"Transcribe\")\n",
        "        if to_quantize.value:\n",
        "            button_q = gr.Button(\"Transcribe quantized\")\n",
        "    with gr.Row():\n",
        "        infer_time = gr.components.Textbox(\n",
        "            label=\"OpenVINO Distil-Whisper Transcription Time (s)\"\n",
        "        )\n",
        "        if to_quantize.value:\n",
        "            infer_time_q = gr.components.Textbox(\n",
        "                label=\"OpenVINO Quantized Distil-Whisper Transcription Time (s)\"\n",
        "            )\n",
        "    with gr.Row():\n",
        "        transcription = gr.components.Textbox(\n",
        "            label=\"OpenVINO Distil-Whisper Transcription\", show_copy_button=True\n",
        "        )\n",
        "        if to_quantize.value:\n",
        "            transcription_q = gr.components.Textbox(\n",
        "                label=\"OpenVINO Quantized Distil-Whisper Transcription\", show_copy_button=True\n",
        "            )\n",
        "    button.click(\n",
        "        fn=transcribe,\n",
        "        inputs=audio,\n",
        "        outputs=[transcription, infer_time],\n",
        "    )\n",
        "    if to_quantize.value:\n",
        "        button_q.click(\n",
        "            fn=transcribe,\n",
        "            inputs=[audio, gr.Number(value=1, visible=False)],\n",
        "            outputs=[transcription_q, infer_time_q],\n",
        "        )\n",
        "    gr.Markdown(\"## Examples\")\n",
        "    gr.Examples(\n",
        "        [[\"./example_1.wav\"]],\n",
        "        audio,\n",
        "        outputs=[transcription, infer_time],\n",
        "        fn=transcribe,\n",
        "        cache_examples=False,\n",
        "    )\n",
        "# if you are launching remotely, specify server_name and server_port\n",
        "# demo.launch(server_name='your server name', server_port='server port in int')\n",
        "# Read more in the docs: https://gradio.app/docs/\n",
        "try:\n",
        "    demo.launch(debug=True)\n",
        "except Exception:\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "id": "96b18c5b-caaf-4f2a-8535-61f4b807ac94"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_0IjXfx8-On"
      },
      "source": [],
      "id": "I_0IjXfx8-On"
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}