{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface - [Image-to-Text](https://huggingface.co/tasks/image-to-text)\n",
        "\n",
        "Run Llava model on a Google Colab!\n",
        "\n",
        "https://huggingface.co/llava-hf/llava-1.5-7b-hf\n",
        "\n",
        "Llava is a multi-modal image-text to text model that can be seen as an \"open source version of GPT4\". It yields to very nice results as we will see in this Google Colab demo.\n",
        "\n",
        "![image/png](https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/FPshq08TKYD0e-qwPLDVO.png)\n",
        "\n",
        "The architecutre is a pure decoder-based text model that takes concatenated vision hidden states with text hidden states.\n",
        "\n",
        "We will leverage QLoRA quantization method and use `pipeline` to run our model."
      ],
      "metadata": {
        "id": "heEwc80MMkdv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4UbrCWk0esW"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers==4.37.2\n",
        "!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "# !pip install -q openai\n",
        "!pip install -q gTTS\n",
        "!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the quantization config to load the model in 4bit precision"
      ],
      "metadata": {
        "id": "Xvljx3nLMkdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "GzUH8GUVMkdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will leverage the `image-to-text` pipeline from transformers !"
      ],
      "metadata": {
        "id": "nKqh1XAjMkdw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b045a39e-2a3e-4153-bdb5-281500bcd348"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "\n",
        "whoami()\n",
        "# you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}"
      ],
      "metadata": {
        "id": "t-Cj-HImlUSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})"
      ],
      "metadata": {
        "id": "K_XzVqVpMkdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59mp2cs106RA"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import gradio as gr\n",
        "import time\n",
        "import warnings\n",
        "# import json\n",
        "# import openai\n",
        "import os\n",
        "from gtts import gTTS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to load the model in 4-bit precision, we need to pass a `quantization_config` to our model. Let's do that in the cells below"
      ],
      "metadata": {
        "id": "lvfB88HkMkdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load an image"
      ],
      "metadata": {
        "id": "gAonn0uNMkdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the image that has been used for Llava demo\n",
        "\n",
        "And ask the model to describe that image!"
      ],
      "metadata": {
        "id": "x7CxJjlPMkdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 11):\n",
        "    # num = (str(i).zfill(2))\n",
        "    # print(f\"https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter05/images/LOAIW_ch05_image_{str(i).zfill(2)}.jpg\")\n",
        "    !wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter05/images/LOAIW_ch05_image_{str(i).zfill(2)}.jpg"
      ],
      "metadata": {
        "id": "SfBaccz5cp1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "from PIL import Image\n",
        "\n",
        "# image_url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
        "# image_path = \"/content/Whisper-FT-ES-001.jpg\"\n",
        "# image_path = \"/content/Blaky.jpg\"\n",
        "image_path = \"/content/LOAIW_ch05_image_03.jpg\"\n",
        "image = Image.open((image_path))\n",
        "image"
      ],
      "metadata": {
        "id": "aqZfz0FWMkdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model using `pipeline`"
      ],
      "metadata": {
        "id": "dkDv9DJ-QjDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK helps to split the transcription sentence by sentence\n",
        "# and shows it in a neat manner one below another. You will see it in the output below.\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize"
      ],
      "metadata": {
        "id": "gDZPO6dPr8Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to prompt the model wth a specific format, which is:\n",
        "```bash\n",
        "USER: <image>\\n<prompt>\\nASSISTANT:\n",
        "```"
      ],
      "metadata": {
        "id": "mqG1M4vyMkdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getlocale())  # Before running the pipeline\n",
        "# Run the pipeline\n",
        "print(locale.getlocale())  # After running the pipeline"
      ],
      "metadata": {
        "id": "Dq5Ni3byu5z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 200\n",
        "\n",
        "prompt_instructions = \"\"\"\n",
        "Describe the image using as much detail as possible, is it a painting, a photograph, what colors are predominant, what is the image about?\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n",
        "\n",
        "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "# outputs\n",
        "# print(outputs[0][\"generated_text\"])\n",
        "for sent in sent_tokenize(outputs[0][\"generated_text\"]):\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "W48r3NxDRskb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getlocale())  # Before running the pipeline\n",
        "# Run the pipeline\n",
        "print(locale.getlocale())  # After running the pipeline"
      ],
      "metadata": {
        "id": "cC7ZIsFcvIE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-jz-G891ZY_"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a GPU is the preferred way to use Whisper. If you are using a local machine, you can check if you have a GPU available. The first line results `False`, if Cuda compatible Nvidia GPU is not available and `True` if it is available. The second line of code sets the model to preference GPU whenever it is available."
      ],
      "metadata": {
        "id": "pblcrNgZfK-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://lablab.ai/t/whisper-tutorial\n",
        "import warnings\n",
        "from gtts import gTTS\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using torch {torch.__version__} ({DEVICE})\")"
      ],
      "metadata": {
        "id": "K4c5uwE0e5q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please keep in mind, that there are multiple different models available. You can find all of them [here](https://github.com/openai/whisper/blob/main/model-card.md). Each one of them has tradeoffs between accuracy and speed (compute needed). We will use the 'small' model for this tutorial."
      ],
      "metadata": {
        "id": "mANIL6_Hf6X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can load the Whipser model. The model is loaded with the following command:\n",
        "import whisper\n",
        "model = whisper.load_model(\"medium\", device=DEVICE)\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ],
      "metadata": {
        "id": "iZm7wQ6vfdRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "input_text = 'What color is the flag in the image?'\n",
        "input_image = '/content/LOAIW_ch05_image_10.jpg'\n",
        "\n",
        "# load the image\n",
        "image = Image.open(input_image)\n",
        "\n",
        "# prompt_instructions = \"\"\"\n",
        "# Describe the image using as much detail as possible, is it a painting, a photograph, what colors are predominant, what is the image about?\n",
        "# \"\"\"\n",
        "\n",
        "# print(input_text)\n",
        "prompt_instructions = \"\"\"\n",
        "Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:\n",
        "\"\"\" + input_text\n",
        "prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n",
        "\n",
        "# print(prompt)\n",
        "\n",
        "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "\n",
        "match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n",
        "\n",
        "if match:\n",
        "    # Extract the text after \"ASSISTANT:\"\n",
        "    extracted_text = match.group(1)\n",
        "    print(extracted_text)\n",
        "else:\n",
        "    print(\"No match found.\")\n",
        "\n",
        "for sent in sent_tokenize(outputs[0][\"generated_text\"]):\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "wHv6fE6y1r09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I859xTaaiCZ_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "# import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DcwSW-8izb3"
      },
      "outputs": [],
      "source": [
        "## Logger file\n",
        "tstamp = datetime.datetime.now()\n",
        "tstamp = str(tstamp).replace(' ','_')\n",
        "logfile = f'{tstamp}_log.txt'\n",
        "def writehistory(text):\n",
        "    with open(logfile, 'a', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "        f.write('\\n')\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "def img2txt(input_text, input_image):\n",
        "\n",
        "    # load the image\n",
        "    image = Image.open(input_image)\n",
        "\n",
        "    writehistory(f\"Input text: {input_text} - Type: {type(input_text)} - Dir: {dir(input_text)}\")\n",
        "    if type(input_text) == tuple:\n",
        "        prompt_instructions = \"\"\"\n",
        "        Describe the image using as much detail as possible, is it a painting, a photograph, what colors are predominant, what is the image about?\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt_instructions = \"\"\"\n",
        "        Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:\n",
        "        \"\"\" + input_text\n",
        "\n",
        "    writehistory(f\"prompt_instructions: {prompt_instructions}\")\n",
        "    prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n",
        "\n",
        "    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "\n",
        "    # Properly extract the response text\n",
        "    if outputs is not None and len(outputs[0][\"generated_text\"]) > 0:\n",
        "        match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n",
        "        if match:\n",
        "            # Extract the text after \"ASSISTANT:\"\n",
        "            reply = match.group(1)\n",
        "        else:\n",
        "            reply = \"No response found.\"\n",
        "    else:\n",
        "        reply = \"No response generated.\"\n",
        "\n",
        "    return reply"
      ],
      "metadata": {
        "id": "DVo04xYV_HxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe(audio):\n",
        "\n",
        "    # Check if the audio input is None or empty\n",
        "    if audio is None or audio == '':\n",
        "        return ('','',None)  # Return empty strings and None audio file\n",
        "\n",
        "    # language = 'en'\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    result_text = result.text\n",
        "\n",
        "    return result_text"
      ],
      "metadata": {
        "id": "_muMSOTq7Sto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_speech(text, file_path):\n",
        "    language = 'en'\n",
        "\n",
        "    audioobj = gTTS(text = text,\n",
        "                    lang = language,\n",
        "                    slow = False)\n",
        "\n",
        "    audioobj.save(file_path)\n",
        "\n",
        "    return file_path"
      ],
      "metadata": {
        "id": "zWy3yHj9QBJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# A function to handle audio and image inputs\n",
        "def process_inputs(audio_path, image_path):\n",
        "    # Process the audio file (assuming this is handled by a function called 'transcribe')\n",
        "    speech_to_text_output = transcribe(audio_path)\n",
        "\n",
        "    # Handle the image input\n",
        "    if image_path:\n",
        "        chatgpt_output = img2txt(speech_to_text_output, image_path)\n",
        "    else:\n",
        "        chatgpt_output = \"No image provided.\"\n",
        "\n",
        "    # Assuming 'transcribe' also returns the path to a processed audio file\n",
        "    processed_audio_path = text_to_speech(chatgpt_output, \"Temp3.mp3\")  # Replace with actual path if different\n",
        "\n",
        "    return speech_to_text_output, chatgpt_output, processed_audio_path\n",
        "\n",
        "# Create the interface\n",
        "iface = gr.Interface(\n",
        "    fn=process_inputs,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
        "        gr.Image(type=\"filepath\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Speech to Text\"),\n",
        "        gr.Textbox(label=\"ChatGPT Output\"),\n",
        "        gr.Audio(\"Temp.mp3\")\n",
        "    ],\n",
        "    title=\"Learn OpenAI Whisper: Image processing with Whisper and Llava\",\n",
        "    description=\"Upload an image and interact via voice input and audio response.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "QVZe05yhLkdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}