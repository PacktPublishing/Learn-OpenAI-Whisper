{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learn OpenAI Whisper - Chapter 5\n",
        "## Notebook 3: Voice assistant with Whisper and StableLM Zephyr\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1LPRZl1JfLfhDFP75XkCMFJbDqQMWvY2L)\n",
        "\n",
        "This notebook was designed for setting up and demonstrating a voice assistant system that integrates OpenAI's Whisper model for speech-to-text capabilities and the [StableLM Zephyr 3B](https://huggingface.co/TheBloke/stablelm-zephyr-3b-GGUF) model for natural language understanding and response generation. The notebook is structured to facilitate the installation of necessary libraries, loading of the models, and execution of the voice assistant functionality, leveraging GPU acceleration for improved performance.\n",
        "\n",
        "### Main Code Sections and Their Functions\n",
        "\n",
        "The notebook is structured into several main code sections, each with a specific function:\n",
        "\n",
        "1. **Setting up the environment**: This section sets environmental variables and installs necessary Python packages such as `llama-cpp-python`, `whisper`, `gradio`, and `gTTS` for text-to-speech conversion.\n",
        "\n",
        "2. **Initializating Python libraries**: It imports essential libraries and sets up a logger to record events and outputs during the notebook's execution.\n",
        "\n",
        "3. **Loading the inference model**: This part of the notebook loads the StableLM Zephyr 3B model with LLAMA.CPP, configuring it for GPU usage if available. It specifies parameters such as the maximum sequence length, number of CPU threads, and the number of layers to offload to the GPU.\n",
        "\n",
        "4. **Exploring an inference example**: A simple example is provided to demonstrate how to generate a response from the StableLM Zephyr 3B model given a text prompt.\n",
        "\n",
        "5. **Defining supporting functions**: Functions are defined for interacting with the StableLM model (`llm_call`) and for transcribing audio input into text using Whisper, then generating a response and converting it to speech (`transcribe`).\n",
        "\n",
        "6. **Creating an audio 'seed'**: A temporary audio file is created using `ffmpeg` to facilitate audio handling within the notebook.\n",
        "\n",
        "7. **Running a Gradio interface**: The final section sets up a Gradio interface, which provides a user-friendly GUI for real-time interaction with the voice assistant. It defines the inputs and outputs for the interface and launches it for use.\n",
        "\n",
        "The programming language used in the Jupyter notebook is Python, as indicated by the kernel specification and the language_info metadata in the notebook file."
      ],
      "metadata": {
        "id": "wK84o9zKU5uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Setting up the environment**\n",
        "    \n",
        "    This section sets environmental variables and installs necessary Python packages such as `llama-cpp-python`, `whisper`, `gradio`, and `gTTS` for text-to-speech conversion.\n",
        "\n",
        "    **Setting up environmental variable for llama.ccp**\n",
        "\n",
        "    Before loading the `stablelm-zephyr-3b-GGUF stablelm-zephyr-3b.Q5_K_S.gguf` model, we need to install and compile the `llama-cpp-python` package. To leverage NVIDIA CUDA acceleration, we must set an environmental variable\n",
        "`CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"` first."
      ],
      "metadata": {
        "id": "gWjGJgc2XtNU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOi3PQHTwtrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "414f8cda-de75-4e93-d21e-7f236c34b343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-DLLAMA_CUBLAS=on\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"\n",
        "print(os.getenv(\"CMAKE_ARGS\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bvyny_mf2na"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install llama-cpp-python==0.2.34\n",
        "!huggingface-cli download TheBloke/stablelm-zephyr-3b-GGUF stablelm-zephyr-3b.Q5_K_S.gguf --local-dir . --local-dir-use-symlinks False\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "!pip install -q gTTS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Initializating Python libraries**\n",
        "\n",
        "    We now import essential libraries and set up a logger to record events and outputs during the notebook's execution."
      ],
      "metadata": {
        "id": "_VKYJTBtZ6Uj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I859xTaaiCZ_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "from rich.console import Console\n",
        "console = Console(width=110)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DcwSW-8izb3"
      },
      "outputs": [],
      "source": [
        "## Logger file\n",
        "tstamp = datetime.datetime.now()\n",
        "tstamp = str(tstamp).replace(' ','_')\n",
        "logfile = f'{tstamp}_log.txt'\n",
        "def writehistory(text):\n",
        "    with open(logfile, 'a', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "        f.write('\\n')\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Loading the inference model**\n",
        "\n",
        "    This section of the notebook loads the StableLM Zephyr 3B model with LLAMA.CPP, configuring it for GPU usage if available. It specifies parameters such as the maximum sequence length, number of CPU threads, and the number of layers to offload to the GPU."
      ],
      "metadata": {
        "id": "ZnpRS8D7aDtu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp0dQJCKqkSr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "b61fda8628f746e8a002763e75793d42",
            "c1c11a45052d43c7b5108fd77e564c1d"
          ]
        },
        "outputId": "c37a4b9c-c442-4a8b-978f-ea2d49658d0b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b61fda8628f746e8a002763e75793d42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | \n",
              "ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | \n",
              "ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Model metadata: {'general.file_type': '16', 'tokenizer.chat_template': \"{% for message in messages %}\\n{% if \n",
              "message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == \n",
              "'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' \n",
              "%}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and \n",
              "add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", \n",
              "'tokenizer.ggml.unknown_token_id': '0', 'general.architecture': 'stablelm', 'general.name': 'source', \n",
              "'stablelm.embedding_length': '2560', 'stablelm.context_length': '4096', 'stablelm.block_count': '32', \n",
              "'stablelm.feed_forward_length': '6912', 'stablelm.use_parallel_residual': 'true', \n",
              "'tokenizer.ggml.bos_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'stablelm.rope.dimension_count': \n",
              "'20', 'stablelm.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '0', \n",
              "'stablelm.attention.head_count': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2'}\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Model metadata: {'general.file_type': '16', 'tokenizer.chat_template': \"{% for message in messages %}\\n{% if \n",
              "message['role'] == 'user' %}\\n{{ '&lt;|user|&gt;\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == \n",
              "'system' %}\\n{{ '&lt;|system|&gt;\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' \n",
              "%}\\n{{ '&lt;|assistant|&gt;\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and \n",
              "add_generation_prompt %}\\n{{ '&lt;|assistant|&gt;' }}\\n{% endif %}\\n{% endfor %}\", \n",
              "'tokenizer.ggml.unknown_token_id': '0', 'general.architecture': 'stablelm', 'general.name': 'source', \n",
              "'stablelm.embedding_length': '2560', 'stablelm.context_length': '4096', 'stablelm.block_count': '32', \n",
              "'stablelm.feed_forward_length': '6912', 'stablelm.use_parallel_residual': 'true', \n",
              "'tokenizer.ggml.bos_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'stablelm.rope.dimension_count': \n",
              "'20', 'stablelm.attention.layer_norm_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '0', \n",
              "'stablelm.attention.head_count': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2'}\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "## Load a llama-cpp-python quantized model\n",
        "from llama_cpp import Llama\n",
        "with console.status(\"Loading ✅✅✅✅ stablelm-zephyr-3b with LLAMA.CPP...\",spinner=\"dots12\"):\n",
        "  llm_gpu = Llama(\n",
        "    model_path=\"/content/stablelm-zephyr-3b.Q5_K_S.gguf\",  # Download the model file first\n",
        "    n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
        "    n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
        "    n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
        ")\n",
        "writehistory(f\"{str(datetime.datetime.now())} Loaded 🧠 stablelm-zephyr-3b.Q5_K_S.gguf with GPU enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Exploring an inference example**\n",
        "    \n",
        "    A simple example is provided to demonstrate how to generate a response from the StableLM Zephyr 3B model given a text prompt."
      ],
      "metadata": {
        "id": "oX2jLkxAaOtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdmxO4GMq28R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "f3248505-bdf3-4fa5-91a8-e476a330d9b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;92;40mQuestion: In short response, what is the capital of France?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; background-color: #000000; font-weight: bold\">Question: In short response, what is the capital of France?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              " Paris. AAP \u001b[1m(\u001b[0mAmerican Association for the Study of Ability Development\u001b[1m)\u001b[0m is not related to the capital of any \n",
              "country. It's a scientific organization focused on research and advocacy in ability development, disabilities,\n",
              "and cognitive science.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Paris. AAP <span style=\"font-weight: bold\">(</span>American Association for the Study of Ability Development<span style=\"font-weight: bold\">)</span> is not related to the capital of any \n",
              "country. It's a scientific organization focused on research and advocacy in ability development, disabilities,\n",
              "and cognitive science.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Completed in: \u001b[1;31m0:00:01\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m633762\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed in: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0:00:01.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">633762</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Simple inference example\n",
        "prompt=\"In short response, what is the capital of France?\"\n",
        "\n",
        "template = f\"<|user|>\\n{prompt}<|endoftext|>\\n<|assistant|>\"\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "output = llm_gpu(\n",
        "    template, # Prompt\n",
        "    temperature=0,\n",
        "    max_tokens=512,  # Generate up to 512 tokens\n",
        "    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
        "    echo=False        # Whether to echo the prompt\n",
        ")\n",
        "delta = datetime.datetime.now() - start\n",
        "console.print(f\"[bright_green bold on black]Question: {prompt}\")\n",
        "console.print(output['choices'][0]['text'])\n",
        "console.print(f\"Completed in: [bold red]{delta}\")\n",
        "writehistory(f\"{str(datetime.datetime.now())} Inference completed in: {delta}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Defining a supporting functions for the LLM**\n",
        "\n",
        "    Here we create a function for interacting with the StableLM model and test it."
      ],
      "metadata": {
        "id": "f69McytTaWUh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSNRY2N0H5_7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def llm_call(input_text):\n",
        "    prompt = \"\"\"Act as Tatianna, a junior-level assistant characterized by your cheerful demeanor and unwavering helpfulness. \\\n",
        "    You are in a business setting, thus always act professionally and courteously. \\\n",
        "    Respond succinctly to the following instructions and questions, and do not include information about yourself unless it is part of the action or question: \\\n",
        "    \"\"\" + input_text\n",
        "\n",
        "    template = f\"<|user|>\\n{prompt}<|endoftext|>\\n<|assistant|>\"\n",
        "\n",
        "    start = datetime.datetime.now()\n",
        "    response = llm_gpu(\n",
        "        template, # Prompt\n",
        "        temperature=0.1,\n",
        "        max_tokens=200,  # Generate up to 512 tokens\n",
        "        stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
        "        echo=False        # Whether to echo the prompt\n",
        "    )\n",
        "\n",
        "    delta = datetime.datetime.now() - start\n",
        "    writehistory(f\"{str(datetime.datetime.now())} Inference completed in: {delta}\")\n",
        "\n",
        "    if response is not None:\n",
        "        match = re.search(r':\\s*(.*)', response['choices'][0]['text'])\n",
        "        if match:\n",
        "            reply = match.group(1).strip()\n",
        "        reply = response['choices'][0]['text']\n",
        "    else:\n",
        "        reply = \"No response generated.\"\n",
        "    return reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR5ESVFzSPAM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e3f7a0ef-4e19-451c-f22d-5d316336b832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nGreetings! My name is Tatianna and I would be delighted to assist you. As requested, here's a cup of freshly brewed coffee for you. Please let me know if there's anything else I can do to make your day more comfortable or helpful.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "llm_call(\"Hello, what is your name, and can you bring me some coffee?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Loading Whisper and creating a function for transcription**\n"
      ],
      "metadata": {
        "id": "RM-UeiKWbuXf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXULc7EKVFLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78eb4387-0388-483b-9b81-557299ab7c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.2.1+cu121 (cuda)\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "from gtts import gTTS\n",
        "import numpy as np\n",
        "import torch\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using torch {torch.__version__} ({DEVICE})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdTWHxPzVP4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ec51d5-07cb-4000-b9e0-13e6331182dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:17<00:00, 86.0MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is multilingual and has 762,321,920 parameters.\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"medium\", device=DEVICE)\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc3pDc1jUpv5"
      },
      "outputs": [],
      "source": [
        "def transcribe(audio):\n",
        "    # Check if the audio input is None or empty\n",
        "    if audio is None or audio == '':\n",
        "        return ('','',None)  # Return empty strings and None audio file\n",
        "\n",
        "    language = 'en'\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    options = whisper.DecodingOptions()\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    result_text = result.text\n",
        "\n",
        "    out_result = llm_call(result_text)\n",
        "\n",
        "    audioobj = gTTS(text = out_result,\n",
        "                    lang = language,\n",
        "                    slow = False)\n",
        "\n",
        "    audioobj.save(\"Temp.mp3\")\n",
        "\n",
        "    return [result_text, out_result, \"Temp.mp3\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Creating an audio 'seed'**\n",
        "\n",
        "    A temporary audio file is created using `ffmpeg` to facilitate audio handling within the notebook.\n"
      ],
      "metadata": {
        "id": "BpLxmWsSa6eE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO3yejFwHzGq"
      },
      "outputs": [],
      "source": [
        "# !ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Running a Gradio interface**\n",
        "\n",
        "    The final section sets up a Gradio interface, which provides a user-friendly GUI for real-time interaction with the voice assistant. It defines the inputs and outputs for the interface and launches it for use."
      ],
      "metadata": {
        "id": "ALszLdH6cQnl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEp-FeYCV0Yk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "outputId": "3345e344-a6d4-42ee-f27b-eac2df741eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a72cc3c9b3869ebb7e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a72cc3c9b3869ebb7e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a72cc3c9b3869ebb7e.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "output_1 = gr.Textbox(label=\"Speech to Text\")\n",
        "output_2 = gr.Textbox(label=\"ChatGPT Output\")\n",
        "output_3 = gr.Audio(\"Temp.mp3\", autoplay=True)\n",
        "\n",
        "gr.Interface(\n",
        "    title = 'Learn OpenAI Whisper: Voice Assistant - Using the StableLM Zephyr 3B model',\n",
        "    fn=transcribe,\n",
        "    # gr.inputs.Audio(source=\"microphone\", type=\"filepath\")\n",
        "    inputs = gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
        "    outputs=[\n",
        "        output_1,  output_2, output_3\n",
        "    ],\n",
        "    live=True).launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b61fda8628f746e8a002763e75793d42": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c1c11a45052d43c7b5108fd77e564c1d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⡋⠁\u001b[0m Loading ✅✅✅✅ stablelm-zephyr-3b with LLAMA.CPP...\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⡋⠁</span> Loading ✅✅✅✅ stablelm-zephyr-3b with LLAMA.CPP...\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c1c11a45052d43c7b5108fd77e564c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}