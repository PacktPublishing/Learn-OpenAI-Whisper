{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learn OpenAI Whisper - Chapter 5\n",
        "## Transcription and translation with Whisper\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1iAkLCMvhRcU50HM0AzY3pjgtE2RS65tR)\n",
        "\n",
        "This notebook provides a comprehensive guide for using OpenAI's Whisper model for multilingual automatic speech recognition (ASR) and translation. It's tailored for anyone eager to dive into the capabilities of Whisper, particularly in handling diverse languages. Here's a breakdown of its purpose, the steps involved, and the benefits it offers:\n",
        "\n",
        "### Purpose:\n",
        "- **Demonstrate Whisper's Installation and Usage:** Shows how to set up Whisper in a Python environment, including the installation of necessary libraries.\n",
        "- **Multilingual ASR and Translation:** Illustrates how to perform speech recognition and translation across various languages using Whisper.\n",
        "- **Dataset Handling:** Utilizes the Fleurs dataset for demonstrating Whisper's capabilities on multilingual audio data.\n",
        "- **Interactive Demonstration:** Incorporates an interactive Gradio interface, allowing users to experiment with Whisper's transcription and translation features on selected audio samples.\n",
        "\n",
        "### High-Level Steps:\n",
        "1. **Installation:** Installs Whisper and other required Python packages like `librosa`, `gradio`, and `kaleido` for audio processing, visualization, and creating interactive web apps.\n",
        "2. **Environment Setup:** Prepares the Python environment, including handling potential compatibility issues (e.g., with TensorFlow in Colab) and setting up the device for computation.\n",
        "3. **Dataset Loading:** Provides a widget for selecting a language from the Fleurs dataset, demonstrating how to dynamically work with multilingual data.\n",
        "4. **Data Preprocessing:** Implements a custom `Fleurs` class to download, extract, and preprocess audio files from the selected language dataset.\n",
        "5. **Whisper Model Loading and Inference:** Loads the Whisper model, runs it on the dataset to perform transcription and translation, and collects the results.\n",
        "6. **Interactive Exploration with Gradio:** Sets up a Gradio interface to allow users to interactively select audio samples, adjust inference parameters, and view the ASR and translation results alongside the original audio.\n",
        "\n",
        "### Benefits:\n",
        "- **Practical Learning:** By following this code, learners can gain hands-on experience with one of the most advanced ASR and translation models available, understanding both its strengths and limitations across different languages.\n",
        "- **Interactive Experience:** The inclusion of Gradio for interactive exploration makes it easy for readers to experiment with various settings and hear the outcomes directly, enhancing the learning experience.\n",
        "- **Comprehensive Approach:** This example covers the entire workflow from data preparation to model inference and result visualization, providing a solid foundation for building similar applications.\n",
        "- **Multilingual Support:** Demonstrates Whisper's multilingual capabilities, offering insights into working with non-English audio data, which is invaluable for global applications and research.\n",
        "\n",
        "This guide is an excellent starting point for anyone interested in exploring the intersection of speech processing and machine learning, offering practical insights into using OpenAI's Whisper model effectively."
      ],
      "metadata": {
        "id": "psB2mdC-keT8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hvo8QWN-a9"
      },
      "source": [
        "# Installing Whisper\n",
        "\n",
        "The commands below will install the Python packages needed to use Whisper models and evaluate the transcription results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsJUxc0aRsAf"
      },
      "outputs": [],
      "source": [
        "!pip install -q cohere openai tiktoken\n",
        "!pip install -q librosa\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install gradio kaleido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CqtR2Fi5-vP"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import tarfile\n",
        "import whisper\n",
        "import torchaudio\n",
        "\n",
        "from scipy.io import wavfile\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "pd.options.display.max_rows = 100\n",
        "pd.options.display.max_colwidth = 1000\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IMEkgyagYto"
      },
      "source": [
        "# Loading the Fleurs dataset\n",
        "\n",
        "Select the language of the Fleur dataset to download. Please note that the transcription and translation performance varies widely depending on the language.\n",
        "\n",
        "[**FLEURS**](https://arxiv.org/abs/2205.12446) stands for **Few-shot Learning Evaluation of Universal Representations of Speech**. It's a benchmark designed to evaluate the performance of universal speech representations in a **few-shot learning** scenario.\n",
        "\n",
        "Here are the key details about FLEURS:\n",
        "\n",
        "- **Dataset**: FLEURS is an **n-way parallel speech dataset** that spans **102 languages**. It is built on top of the machine translation benchmark called **FLoRes-101**.\n",
        "- **Speech Supervision**: Each language in FLEURS has approximately **12 hours of speech supervision**.\n",
        "- **Tasks**: FLEURS can be used for various speech-related tasks, including:\n",
        "    - **Automatic Speech Recognition (ASR)**: Converting spoken language into text.\n",
        "    - **Speech Language Identification (Speech LangID)**: Identifying the language spoken in an audio clip.\n",
        "    - **Translation and Retrieval**: Leveraging universal speech representations for translation and retrieval tasks.\n",
        "\n",
        "The goal of FLEURS is to enable speech technology in more languages and catalyze research in **low-resource speech understanding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4lPK5106Of2"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "languages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\n",
        "selection = widgets.Dropdown(\n",
        "    options=[(\"Select language\", None), (\"----------\", None)] + sorted([(f\"{v} ({k})\", k) for k, v in languages.items()]),\n",
        "    value=\"ko_kr\",\n",
        "    description='Language:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eihI6oK6Of2"
      },
      "outputs": [],
      "source": [
        "lang = selection.value\n",
        "language = languages[lang]\n",
        "\n",
        "assert lang is not None, \"Please select a language\"\n",
        "print(f\"Selected language: {language} ({lang})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuCCB2KYOJCE"
      },
      "outputs": [],
      "source": [
        "def download(url: str, target_path: str):\n",
        "    with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n",
        "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
        "            while True:\n",
        "                buffer = source.read(8192)\n",
        "                if not buffer:\n",
        "                    break\n",
        "\n",
        "                output.write(buffer)\n",
        "                loop.update(len(buffer))\n",
        "\n",
        "\n",
        "class Fleurs(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, lang, split=\"test\", subsample_rate=1, device=DEVICE):\n",
        "        url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n",
        "        tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n",
        "        os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n",
        "\n",
        "        if not os.path.exists(tar_path):\n",
        "            download(url, tar_path)\n",
        "\n",
        "        all_audio = {}\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            for member in tar.getmembers():\n",
        "                name = member.name\n",
        "                if name.endswith(f\"{split}.tsv\"):\n",
        "                    # Ensure you're using the correct method to read the TSV file.\n",
        "                    # Just make sure it properly handles the TSV data.\n",
        "                    labels = pd.read_table(tar.extractfile(member), names=(\"id\", \"file_name\", \"raw_transcription\", \"transcription\", \"_\", \"num_samples\", \"gender\"))\n",
        "\n",
        "                if f\"/{split}/\" in name and name.endswith(\".wav\"):\n",
        "                    audio_bytes = tar.extractfile(member).read()\n",
        "                    # all_audio[os.path.basename(name)] = wavfile.read(io.BytesIO(audio_bytes))[1]\n",
        "                    # Use io.BytesIO to create a buffer from the audio bytes\n",
        "                    # and scipy.io.wavfile.read to read from this buffer.\n",
        "                    try:\n",
        "                        rate, data = wavfile.read(io.BytesIO(audio_bytes))\n",
        "                        all_audio[os.path.basename(name)] = data\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Error reading {name}: {e}\")\n",
        "\n",
        "        self.labels = labels.to_dict(\"records\")[::subsample_rate]\n",
        "        self.all_audio = all_audio\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        record = self.labels[item]\n",
        "        audio = torch.from_numpy(self.all_audio[record[\"file_name\"]].copy())\n",
        "        text = record[\"transcription\"]\n",
        "\n",
        "        return (audio, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YcRU5jqNqo2"
      },
      "outputs": [],
      "source": [
        "dataset = Fleurs(lang, subsample_rate=5)  # subsample 10% of the dataset for a quick demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ljocCNuUAde"
      },
      "source": [
        "# Running inference on the dataset using a medium Whisper model\n",
        "\n",
        "The following will take a few minutes to transcribe and translate utterances in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PokfNJtOYNu"
      },
      "outputs": [],
      "source": [
        "model = whisper.load_model(\"medium\")\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F74Yfr696Of5"
      },
      "outputs": [],
      "source": [
        "options = dict(language=language, beam_size=5, best_of=5, temperature=0)\n",
        "transcribe_options = dict(task=\"transcribe\", **options)\n",
        "translate_options = dict(task=\"translate\", **options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OWTn_KvNk59",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "references = []\n",
        "transcriptions = []\n",
        "translations = []\n",
        "\n",
        "for audio, text in tqdm(dataset):\n",
        "    transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "    translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
        "\n",
        "    transcriptions.append(transcription)\n",
        "    translations.append(translation)\n",
        "    references.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nTyynELQ42j",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(dict(reference=references, transcription=transcriptions, translation=translations))\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexa = 0\n",
        "audio, text = dataset[indexa]\n",
        "print(f\"Reference: {text}\")\n",
        "print(f\"Transcription: {data.iloc[indexa].transcription}\")\n",
        "print(f\"Translation: {data.iloc[indexa].translation}\")\n",
        "\n",
        "from IPython.display import Audio\n",
        "Audio(data=audio, rate=16000)"
      ],
      "metadata": {
        "id": "LMC7vYcmvh0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "\n",
        "def process_audio(index, beam_size, best_of, temperature):\n",
        "    audio, text = dataset[index]\n",
        "\n",
        "    # Save the tensor to an audio file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmpfile:\n",
        "        temp_filepath = tmpfile.name\n",
        "        torchaudio.save(temp_filepath, audio.unsqueeze(0), 16000)  # Adjust the sample rate as needed\n",
        "\n",
        "    # Use the user-provided beam_size and best_of for transcription and translation\n",
        "    options = dict(language=language, beam_size=beam_size, best_of=best_of, temperature=temperature)\n",
        "    transcribe_options = dict(task=\"transcribe\", **options)\n",
        "    translate_options = dict(task=\"translate\", **options)\n",
        "\n",
        "    transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "    translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
        "\n",
        "    return text, transcription, translation, temp_filepath\n",
        "\n",
        "def gradio_interface(index, beam_size, best_of, temperature):\n",
        "    reference, transcription, translation, temp_filepath = process_audio(index, beam_size, best_of, temperature)\n",
        "\n",
        "    # Display the results using the file path\n",
        "    return reference, transcription, translation, gr.Audio(temp_filepath)\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[\n",
        "        gr.Slider(minimum=0, maximum=len(dataset)-1, step=1, label=\"Select Audio Sample\"),\n",
        "        gr.Slider(minimum=0, maximum=5, step=1, label=\"Beam Size\", value=5),\n",
        "        gr.Slider(minimum=0, maximum=5, step=1, label=\"Best Of\", value=5),\n",
        "        gr.Slider(minimum=0, maximum=1, step=.1, label=\"Temperature\", value=0.2)\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Reference Text\"),\n",
        "        gr.Textbox(label=\"Transcription\"),\n",
        "        gr.Textbox(label=\"Translation\"),\n",
        "        gr.Audio(label=\"Audio\")\n",
        "    ],\n",
        "    title=\"Learn OpenAI Whisper: Audio Processing\",\n",
        "    description=\"Slide to select an audio sample and adjust 'beam size', 'best of', and 'temperature' to view its reference text, transcription, and translation along with an audio player.\"\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "BAl3GEShCryL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83LRfd1plYEb"
      },
      "source": [
        "# Word-level timestamps using attention weights\n",
        "\n",
        "This section demonstrates an advanced technique for extracting word-level timestamps from audio transcriptions using OpenAI's Whisper model, leveraging cross-attention weights. It's designed for readers who are interested in deep learning, speech processing, and specifically those working with the Whisper model for speech-to-text tasks.\n",
        "\n",
        "### Requirements and Dependencies:\n",
        "- **Python Libraries:** The script requires `dtw-python` for dynamic time warping, `matplotlib` for visualization, and `pandas` for data handling, among others.\n",
        "- **Whisper Model:** Understanding the inner workings of OpenAI's Whisper model is crucial, as the script directly interacts with its layers and functions.\n",
        "\n",
        "### Purpose:\n",
        "- **Word-Level Timestamps Extraction:** The primary goal is to align the words in the transcript with specific times in the audio recording. This is particularly useful for applications like subtitle generation, detailed audio analysis, and improving accessibility features in media.\n",
        "- **Granular Analysis with Attention Weights:** It utilizes the model's cross-attention weights to achieve a finer granularity in alignment, beyond what is typically available through simpler transcription methods.\n",
        "\n",
        "### How It Works:\n",
        "1. **Dynamic Time Warping (DTW):** The code uses DTW, a well-known algorithm in speech recognition, to find the optimal alignment between two sequences (here, the audio and the text). DTW is particularly effective in compensating for speed variations in speech.\n",
        "2. **Attention Weights:** By tapping into the cross-attention layers of the Whisper model, the script calculates how each word in the transcript correlates with segments of the audio, providing a basis for timestamping.\n",
        "3. **Data Preparation and Processing:** It includes preprocessing steps like median filtering to smooth the attention weights and normalization to ensure meaningful comparisons.\n",
        "4. **Visualization and Output:** The script visualizes the alignment and provides detailed timestamps for each word, making it easy to see how the text matches the audio. It also demonstrates how to handle languages with different characteristics (e.g., space usage in text) for accurate word splitting.\n",
        "\n",
        "### Applicability:\n",
        "- **Educational Content:** Readers of \"Learn OpenAI Whisper\" can use this as a practical example of applying Whisper for detailed speech analysis, going beyond basic transcription to understand the dynamics of attention mechanisms in audio processing.\n",
        "- **Research and Development:** This approach can inspire researchers and developers to explore advanced speech processing techniques, such as improving automatic subtitle generation, enhancing language learning tools, or creating more interactive and accessible media content.\n",
        "- **Language-Specific Handling:** The code is designed to accommodate languages with different characteristics, providing insights into handling multilingual audio data effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Necessary Libraries\n",
        "First, we install `dtw-python` to use Dynamic Time Warping (DTW) for aligning audio and text sequences based on their temporal similarities.\n"
      ],
      "metadata": {
        "id": "Q9qWatWxa-75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGtZWNaQlOVC"
      },
      "outputs": [],
      "source": [
        "! pip install dtw-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries and Setting Up Environment\n",
        "We import necessary Python libraries for data manipulation, visualization, and handling audio files. This includes `matplotlib` for plotting, `dtw` for dynamic time warping, and Whisper's tokenizer for processing text.\n"
      ],
      "metadata": {
        "id": "2yLvYMqUbBPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HTv8KmzlZtc"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from whisper.tokenizer import get_tokenizer\n",
        "from dtw import dtw\n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = \"retina\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constants and Configuration\n",
        "Here, we define constants related to audio processing in Whisper and set up the matplotlib environment for better visual output. This includes setting the audio sample rate and configuring the inline backend for high-resolution figures.\n"
      ],
      "metadata": {
        "id": "og8t7Q_abO7w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqSiARgqlb6X"
      },
      "outputs": [],
      "source": [
        "AUDIO_SAMPLES_PER_TOKEN = whisper.audio.HOP_LENGTH * 2\n",
        "AUDIO_TIME_PER_TOKEN = AUDIO_SAMPLES_PER_TOKEN / whisper.audio.SAMPLE_RATE\n",
        "\n",
        "medfilt_width = 7\n",
        "qk_scale = 1.0\n",
        "\n",
        "tokenizer = get_tokenizer(model.is_multilingual, language=languages[lang])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Fonts for Visualization\n",
        "This section deals with downloading and setting up fonts compatible with various languages for visualization purposes. It ensures that the plotted figures can correctly display characters from different language scripts."
      ],
      "metadata": {
        "id": "TB9My55hb5Oo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTVen0YkldgU"
      },
      "outputs": [],
      "source": [
        "# This part downloads a repackaged version of the Noto Sans font (either CJK or non-CJK)\n",
        "# to render various languages in Matplotlib figures.\n",
        "\n",
        "if languages[lang] in {\"Chinese\", \"Japanese\", \"Korean\"}:\n",
        "    font = \"GoNotoCJKCore.ttf\"\n",
        "else:\n",
        "    font = \"GoNotoCurrent.ttf\"\n",
        "\n",
        "font_release = \"https://github.com/satbyy/go-noto-universal/releases/download/v5.2\"\n",
        "if not os.path.exists(font):\n",
        "    download(f\"{font_release}/{font}\", font)\n",
        "\n",
        "prop = fm.FontProperties(fname=font)\n",
        "props = {'fontproperties': prop}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing for Text Processing\n",
        "We initialize the tokenizer from Whisper, adjusting for multilingual support and specific language requirements. This is crucial for accurately converting text sequences into tokens that Whisper can process.\n",
        "#### Utility Functions for Token Splitting\n",
        "Two utility functions are defined to split tokens based on unicode characters and spaces. These functions help in identifying word boundaries in the transcribed text, which is essential for generating word-level timestamps later."
      ],
      "metadata": {
        "id": "_P7I2IitbXYn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiOFv8X5lhQA"
      },
      "outputs": [],
      "source": [
        "def split_tokens_on_unicode(tokens: torch.Tensor):\n",
        "    words = []\n",
        "    word_tokens = []\n",
        "    current_tokens = []\n",
        "\n",
        "    for token in tokens.tolist():\n",
        "        current_tokens.append(token)\n",
        "        decoded = tokenizer.decode_with_timestamps(current_tokens)\n",
        "        if \"\\ufffd\" not in decoded:\n",
        "            words.append(decoded)\n",
        "            word_tokens.append(current_tokens)\n",
        "            current_tokens = []\n",
        "\n",
        "    return words, word_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkhsL9xUmHjt"
      },
      "outputs": [],
      "source": [
        "def split_tokens_on_spaces(tokens: torch.Tensor):\n",
        "    subwords, subword_tokens_list = split_tokens_on_unicode(tokens)\n",
        "    words = []\n",
        "    word_tokens = []\n",
        "\n",
        "    for subword, subword_tokens in zip(subwords, subword_tokens_list):\n",
        "        special = subword_tokens[0] >= tokenizer.eot\n",
        "        with_space = subword.startswith(\" \")\n",
        "        punctuation = subword.strip() in string.punctuation\n",
        "        if special or with_space or punctuation:\n",
        "            words.append(subword)\n",
        "            word_tokens.append(subword_tokens)\n",
        "        else:\n",
        "            words[-1] = words[-1] + subword\n",
        "            word_tokens[-1].extend(subword_tokens)\n",
        "\n",
        "    return words, word_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQOl_4TSmIgB"
      },
      "outputs": [],
      "source": [
        "if languages[lang] in {\"Chinese\", \"Japanese\", \"Thai\", \"Lao\", \"Myanmar\"}:\n",
        "    # These languages don't typically use spaces, so it is difficult to split words\n",
        "    # without morpheme analysis. Here, we instead split words at any\n",
        "    # position where the tokens are decoded as valid unicode points\n",
        "    split_tokens = split_tokens_on_unicode\n",
        "else:\n",
        "    split_tokens = split_tokens_on_spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Hooks for Attention Weights\n",
        "Hooks are installed on the cross-attention layers of the Whisper model to capture the attention weights during inference. These weights are crucial for understanding how the model aligns audio segments with specific words.\n"
      ],
      "metadata": {
        "id": "i3hdD3X9cqjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install hooks on the cross attention layers to retrieve the attention weights\n",
        "QKs = [None] * model.dims.n_text_layer\n",
        "\n",
        "for i, block in enumerate(model.decoder.blocks):\n",
        "    block.cross_attn.register_forward_hook(\n",
        "        lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1])\n",
        "    )"
      ],
      "metadata": {
        "id": "DGwrymCo5SUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Audio and Generating Timestamps\n",
        "\n",
        "For a subset of the dataset, this loop processes each audio file, generates its mel spectrogram, and performs inference using the Whisper model. It then uses the captured attention weights and DTW to align words in the transcription with their corresponding audio segments, creating word-level timestamps.\n"
      ],
      "metadata": {
        "id": "FPKvQJTgcz27"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q-mBLVNmJ1i"
      },
      "outputs": [],
      "source": [
        "# for the first 10 examples in the dataset\n",
        "for (audio, label), transcription in zip(dataset, transcriptions[:10]):\n",
        "    print(transcription)\n",
        "\n",
        "    duration = len(audio)\n",
        "    mel = whisper.log_mel_spectrogram(whisper.pad_or_trim(audio)).cuda()\n",
        "    tokens = torch.tensor(\n",
        "        [\n",
        "            *tokenizer.sot_sequence,\n",
        "            tokenizer.timestamp_begin,\n",
        "        ] + tokenizer.encode(transcription) + [\n",
        "            tokenizer.timestamp_begin + duration // AUDIO_SAMPLES_PER_TOKEN,\n",
        "            tokenizer.eot,\n",
        "        ]\n",
        "    ).cuda()\n",
        "    with torch.no_grad():\n",
        "        logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))\n",
        "\n",
        "    weights = torch.cat(QKs)  # layers * heads * tokens * frames\n",
        "    weights = weights[:, :, :, : duration // AUDIO_SAMPLES_PER_TOKEN].cpu()\n",
        "    weights = median_filter(weights, (1, 1, 1, medfilt_width))\n",
        "    weights = torch.tensor(weights * qk_scale).softmax(dim=-1)\n",
        "\n",
        "    w = weights / weights.norm(dim=-2, keepdim=True)\n",
        "    matrix = w[-6:].mean(axis=(0, 1))\n",
        "\n",
        "    alignment = dtw(-matrix.double().numpy())\n",
        "\n",
        "    jumps = np.pad(np.diff(alignment.index1s), (1, 0), constant_values=1).astype(bool)\n",
        "    jump_times = alignment.index2s[jumps] * AUDIO_TIME_PER_TOKEN\n",
        "    words, word_tokens = split_tokens(tokens)\n",
        "\n",
        "    '''\n",
        "    Visualizing Alignment and Attention Weights\n",
        "    This part visualizes the alignment between audio and text using the calculated attention weights.\n",
        "    It plots the cross-attention matrix and overlays the DTW path, showing how each word aligns with segments of the audio.\n",
        "    '''\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(matrix, aspect=\"auto\")\n",
        "    plt.plot(alignment.index2s, alignment.index1s, color=\"red\")\n",
        "\n",
        "    xticks = np.arange(0, matrix.shape[1], 1 / AUDIO_TIME_PER_TOKEN)\n",
        "    xticklabels = (xticks * AUDIO_TIME_PER_TOKEN).round().astype(np.int32)\n",
        "    plt.xticks(xticks, xticklabels)\n",
        "    plt.xlabel(\"Time (s)\")\n",
        "\n",
        "    # display tokens and words as tick labels\n",
        "    ylims = plt.gca().get_ylim()\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.tick_params('both', length=0, width=0, which='minor', pad=6)\n",
        "\n",
        "    ax.yaxis.set_ticks_position(\"left\")\n",
        "    ax.yaxis.set_label_position(\"left\")\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_ylim(ylims)\n",
        "\n",
        "    major_ticks = [-0.5]\n",
        "    minor_ticks = []\n",
        "    current_y = 0\n",
        "\n",
        "    for word, word_token in zip(words, word_tokens):\n",
        "        minor_ticks.append(current_y + len(word_token) / 2 - 0.5)\n",
        "        current_y += len(word_token)\n",
        "        major_ticks.append(current_y - 0.5)\n",
        "\n",
        "    ax.yaxis.set_minor_locator(ticker.FixedLocator(minor_ticks))\n",
        "    ax.yaxis.set_minor_formatter(ticker.FixedFormatter(words))\n",
        "    ax.set_yticks(major_ticks)\n",
        "    ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
        "\n",
        "    for label in ax.get_yminorticklabels():\n",
        "        label.set_fontproperties(prop)\n",
        "\n",
        "    plt.ylabel(\"Words\")\n",
        "    plt.show()\n",
        "\n",
        "    '''\n",
        "    Generating Word-Level Timestamps\n",
        "    Finally, the code calculates and displays the start and end times for each word in the transcription.\n",
        "    This allows for a granular analysis of the speech, which can be used for detailed subtitles, speech analysis, and more interactive applications.\n",
        "    '''\n",
        "    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n",
        "    begin_times = jump_times[word_boundaries[:-1]]\n",
        "    end_times = jump_times[word_boundaries[1:]]\n",
        "\n",
        "    data = [\n",
        "        dict(word=word, begin=begin, end=end)\n",
        "        for word, begin, end in zip(words[:-1], begin_times, end_times)\n",
        "        if not word.startswith(\"<|\") and word.strip() not in \".,!?、。\"\n",
        "    ]\n",
        "\n",
        "    display(pd.DataFrame(data))\n",
        "    display(HTML(\"<hr>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentation"
      ],
      "metadata": {
        "id": "rlFkJoh6DU7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import tempfile\n",
        "import gradio as gr\n",
        "\n",
        "def process_audio(index):\n",
        "    audio, text = dataset[index]\n",
        "\n",
        "    # Assuming 'audio' is a PyTorch tensor\n",
        "    # Save the tensor to an audio file\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmpfile:\n",
        "        temp_filepath = tmpfile.name\n",
        "        torchaudio.save(temp_filepath, audio.unsqueeze(0), 16000)  # Adjust the sample rate as needed\n",
        "\n",
        "    # Process transcription and translation as before\n",
        "    options = dict(language=language, beam_size=5, best_of=5)\n",
        "    transcribe_options = dict(task=\"transcribe\", **options)\n",
        "    translate_options = dict(task=\"translate\", **options)\n",
        "\n",
        "    transcription = model.transcribe(audio, **transcribe_options)[\"text\"]\n",
        "    translation = model.transcribe(audio, **translate_options)[\"text\"]\n",
        "\n",
        "    return text, transcription, translation, temp_filepath\n",
        "\n",
        "# Update the gradio_interface function to use the file path\n",
        "def gradio_interface(index):\n",
        "    reference, transcription, translation, temp_filepath = process_audio(index)\n",
        "\n",
        "    # Display the results using the file path\n",
        "    return reference, transcription, translation, gr.Audio(temp_filepath)\n",
        "\n",
        "# Setup the Gradio app as before\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Slider(minimum=0, maximum=len(dataset)-1, step=1, label=\"Select Audio Sample\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Reference Text\"),\n",
        "        gr.Textbox(label=\"Transcription\"),\n",
        "        gr.Textbox(label=\"Translation\"),\n",
        "        gr.Audio(label=\"Audio\")\n",
        "    ],\n",
        "    title=\"Whisper Audio Processing\",\n",
        "    description=\"Slide to select an audio sample to view its reference text, transcription, and translation along with an audio player.\"\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "NRyIg4bY98E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio, text = dataset[2]\n",
        "\n",
        "# Assuming 'audio' is a PyTorch tensor\n",
        "# Save the tensor to an audio file\n",
        "with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmpfile:\n",
        "    temp_filepath = tmpfile.name\n",
        "    print(f\"Saving audio to {temp_filepath}\")\n",
        "    torchaudio.save(temp_filepath, audio.unsqueeze(0), 16000)  # Adjust the sample rate as needed\n"
      ],
      "metadata": {
        "id": "I04gKlPn87Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "widgets.Audio.from_file(temp_filepath, autoplay=False, loop=False)"
      ],
      "metadata": {
        "id": "mRmwiyJo9a3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
