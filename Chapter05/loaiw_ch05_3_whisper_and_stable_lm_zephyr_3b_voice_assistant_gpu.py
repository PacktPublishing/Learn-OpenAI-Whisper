# -*- coding: utf-8 -*-
"""LOAIW_ch05_3_Whisper_and_Stable_LM_Zephyr_3B_voice_assistant_GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LPRZl1JfLfhDFP75XkCMFJbDqQMWvY2L

# Learn OpenAI Whisper - Chapter 5
## Notebook 3: Voice assistant with Whisper and StableLM Zephyr

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1LPRZl1JfLfhDFP75XkCMFJbDqQMWvY2L)

This notebook was designed for setting up and demonstrating a voice assistant system that integrates OpenAI's Whisper model for speech-to-text capabilities and the [StableLM Zephyr 3B](https://huggingface.co/TheBloke/stablelm-zephyr-3b-GGUF) model for natural language understanding and response generation. The notebook is structured to facilitate the installation of necessary libraries, loading of the models, and execution of the voice assistant functionality, leveraging GPU acceleration for improved performance.

### Main Code Sections and Their Functions

The notebook is structured into several main code sections, each with a specific function:

1. **Setting up the environment**: This section sets environmental variables and installs necessary Python packages such as `llama-cpp-python`, `whisper`, `gradio`, and `gTTS` for text-to-speech conversion.

2. **Initializating Python libraries**: It imports essential libraries and sets up a logger to record events and outputs during the notebook's execution.

3. **Loading the inference model**: This part of the notebook loads the StableLM Zephyr 3B model with LLAMA.CPP, configuring it for GPU usage if available. It specifies parameters such as the maximum sequence length, number of CPU threads, and the number of layers to offload to the GPU.

4. **Exploring an inference example**: A simple example is provided to demonstrate how to generate a response from the StableLM Zephyr 3B model given a text prompt.

5. **Defining supporting functions**: Functions are defined for interacting with the StableLM model (`llm_call`) and for transcribing audio input into text using Whisper, then generating a response and converting it to speech (`transcribe`).

6. **Creating an audio 'seed'**: A temporary audio file is created using `ffmpeg` to facilitate audio handling within the notebook.

7. **Running a Gradio interface**: The final section sets up a Gradio interface, which provides a user-friendly GUI for real-time interaction with the voice assistant. It defines the inputs and outputs for the interface and launches it for use.

The programming language used in the Jupyter notebook is Python, as indicated by the kernel specification and the language_info metadata in the notebook file.

1. **Setting up the environment**
    
    This section sets environmental variables and installs necessary Python packages such as `llama-cpp-python`, `whisper`, `gradio`, and `gTTS` for text-to-speech conversion.

    **Setting up environmental variable for llama.ccp**

    Before loading the `stablelm-zephyr-3b-GGUF stablelm-zephyr-3b.Q5_K_S.gguf` model, we need to install and compile the `llama-cpp-python` package. To leverage NVIDIA CUDA acceleration, we must set an environmental variable
`CMAKE_ARGS="-DLLAMA_CUBLAS=on"` first.
"""

import os
os.environ["CMAKE_ARGS"] = "-DLLAMA_CUBLAS=on"
print(os.getenv("CMAKE_ARGS"))

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install llama-cpp-python==0.2.34
# !huggingface-cli download TheBloke/stablelm-zephyr-3b-GGUF stablelm-zephyr-3b.Q5_K_S.gguf --local-dir . --local-dir-use-symlinks False
# !pip install -q git+https://github.com/openai/whisper.git
# !pip install -q gradio
# !pip install -q gTTS

"""2. **Initializating Python libraries**

    We now import essential libraries and set up a logger to record events and outputs during the notebook's execution.
"""

import datetime
import os
from rich.console import Console
console = Console(width=110)

## Logger file
tstamp = datetime.datetime.now()
tstamp = str(tstamp).replace(' ','_')
logfile = f'{tstamp}_log.txt'
def writehistory(text):
    with open(logfile, 'a', encoding='utf-8') as f:
        f.write(text)
        f.write('\n')
    f.close()

"""3. **Loading the inference model**

    This section of the notebook loads the StableLM Zephyr 3B model with LLAMA.CPP, configuring it for GPU usage if available. It specifies parameters such as the maximum sequence length, number of CPU threads, and the number of layers to offload to the GPU.
"""

## Load a llama-cpp-python quantized model
from llama_cpp import Llama
with console.status("Loading âœ…âœ…âœ…âœ… stablelm-zephyr-3b with LLAMA.CPP...",spinner="dots12"):
  llm_gpu = Llama(
    model_path="/content/stablelm-zephyr-3b.Q5_K_S.gguf",  # Download the model file first
    n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources
    n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance
    n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available
)
writehistory(f"{str(datetime.datetime.now())} Loaded ðŸ§  stablelm-zephyr-3b.Q5_K_S.gguf with GPU enabled")

"""4. **Exploring an inference example**
    
    A simple example is provided to demonstrate how to generate a response from the StableLM Zephyr 3B model given a text prompt.
"""

# Simple inference example
prompt="In short response, what is the capital of France?"

template = f"<|user|>\n{prompt}<|endoftext|>\n<|assistant|>"

start = datetime.datetime.now()
output = llm_gpu(
    template, # Prompt
    temperature=0,
    max_tokens=512,  # Generate up to 512 tokens
    stop=["</s>"],   # Example stop token - not necessarily correct for this specific model! Please check before using.
    echo=False        # Whether to echo the prompt
)
delta = datetime.datetime.now() - start
console.print(f"[bright_green bold on black]Question: {prompt}")
console.print(output['choices'][0]['text'])
console.print(f"Completed in: [bold red]{delta}")
writehistory(f"{str(datetime.datetime.now())} Inference completed in: {delta}")

"""5. **Defining a supporting functions for the LLM**

    Here we create a function for interacting with the StableLM model and test it.
"""

import re

def llm_call(input_text):
    prompt = """Act as Tatianna, a junior-level assistant characterized by your cheerful demeanor and unwavering helpfulness. \
    You are in a business setting, thus always act professionally and courteously. \
    Respond succinctly to the following instructions and questions, and do not include information about yourself unless it is part of the action or question: \
    """ + input_text

    template = f"<|user|>\n{prompt}<|endoftext|>\n<|assistant|>"

    start = datetime.datetime.now()
    response = llm_gpu(
        template, # Prompt
        temperature=0.1,
        max_tokens=200,  # Generate up to 512 tokens
        stop=["</s>"],   # Example stop token - not necessarily correct for this specific model! Please check before using.
        echo=False        # Whether to echo the prompt
    )

    delta = datetime.datetime.now() - start
    writehistory(f"{str(datetime.datetime.now())} Inference completed in: {delta}")

    if response is not None:
        match = re.search(r':\s*(.*)', response['choices'][0]['text'])
        if match:
            reply = match.group(1).strip()
        reply = response['choices'][0]['text']
    else:
        reply = "No response generated."
    return reply

llm_call("Hello, what is your name, and can you bring me some coffee?")

"""6. **Loading Whisper and creating a function for transcription**

"""

import warnings
from gtts import gTTS
import numpy as np
import torch
warnings.filterwarnings("ignore")
torch.cuda.is_available()
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using torch {torch.__version__} ({DEVICE})")

import whisper
model = whisper.load_model("medium", device=DEVICE)
print(
    f"Model is {'multilingual' if model.is_multilingual else 'English-only'} "
    f"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters."
)

def transcribe(audio):
    # Check if the audio input is None or empty
    if audio is None or audio == '':
        return ('','',None)  # Return empty strings and None audio file

    language = 'en'

    audio = whisper.load_audio(audio)
    audio = whisper.pad_or_trim(audio)

    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    _, probs = model.detect_language(mel)

    options = whisper.DecodingOptions()
    result = whisper.decode(model, mel, options)
    result_text = result.text

    out_result = llm_call(result_text)

    audioobj = gTTS(text = out_result,
                    lang = language,
                    slow = False)

    audioobj.save("Temp.mp3")

    return [result_text, out_result, "Temp.mp3"]

"""7. **Creating an audio 'seed'**

    A temporary audio file is created using `ffmpeg` to facilitate audio handling within the notebook.

"""

# !ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3

"""8. **Running a Gradio interface**

    The final section sets up a Gradio interface, which provides a user-friendly GUI for real-time interaction with the voice assistant. It defines the inputs and outputs for the interface and launches it for use.
"""

import gradio as gr

output_1 = gr.Textbox(label="Speech to Text")
output_2 = gr.Textbox(label="ChatGPT Output")
output_3 = gr.Audio("Temp.mp3", autoplay=True)

gr.Interface(
    title = 'Learn OpenAI Whisper: Voice Assistant - Using the StableLM Zephyr 3B model',
    fn=transcribe,
    # gr.inputs.Audio(source="microphone", type="filepath")
    inputs = gr.Audio(sources=["microphone"], type="filepath"),
    outputs=[
        output_1,  output_2, output_3
    ],
    live=True).launch(debug=True)