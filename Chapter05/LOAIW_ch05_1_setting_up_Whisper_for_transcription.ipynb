{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVHm8JGYOH2U"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt install ffmpeg\n",
        "!pip install -q cohere openai tiktoken\n",
        "!pip install -q git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download audio from GitHub repository\n",
        "!wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.mp3\n",
        "!wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3\n",
        "!wget -nv https://cdn.openai.com/API/examples/data/bbq_plans.wav\n",
        "!wget -nv https://cdn.openai.com/API/examples/data/product_names.wav\n",
        "\n",
        "audiofiles = ['Learn_OAI_Whisper_Sample_Audio01.mp3', 'Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3', 'bbq_plans.wav', 'product_names.wav']"
      ],
      "metadata": {
        "id": "BzFAqXNdP_3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a GPU is the preferred way to use Whisper. If you are using a local machine, you can check if you have a GPU available. The first line results `False`, if Cuda compatible Nvidia GPU is not available and `True` if it is available. The second line of code sets the model to preference GPU whenever it is available."
      ],
      "metadata": {
        "id": "pblcrNgZfK-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://lablab.ai/t/whisper-tutorial\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using torch {torch.__version__} ({DEVICE})\")"
      ],
      "metadata": {
        "id": "K4c5uwE0e5q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please keep in mind, that there are multiple different models available. You can find all of them [here](https://github.com/openai/whisper/blob/main/model-card.md). Each one of them has tradeoffs between accuracy and speed (compute needed). We will use the 'small' model for this tutorial."
      ],
      "metadata": {
        "id": "mANIL6_Hf6X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can load the Whipser model. The model is loaded with the following command:\n",
        "import whisper\n",
        "model = whisper.load_model(\"medium\", device=DEVICE)\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ],
      "metadata": {
        "id": "iZm7wQ6vfdRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK helps to split the transcription sentence by sentence\n",
        "# and shows it in a neat manner one below another. You will see it in the output below.\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize"
      ],
      "metadata": {
        "id": "gDZPO6dPr8Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for audiofile in audiofiles:\n",
        "    # Load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audiofile)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    # Make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "    #The `detect_language` function detects the language of your audio file\n",
        "    _, probs = model.detect_language(mel)\n",
        "    detected_language = max(probs, key=probs.get)\n",
        "    print(f\"----\\nDetected language: {detected_language}\")\n",
        "    # Set up the decoding options\n",
        "    options = whisper.DecodingOptions(language=detected_language, without_timestamps=True, fp16=(DEVICE == \"cuda\"))\n",
        "    # Decode the audio and print the recognized text\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    print(\"Transcription of file '\" + audiofile + \"':\")\n",
        "    for sent in sent_tokenize(result.text):\n",
        "        print(sent)"
      ],
      "metadata": {
        "id": "8jxdUn0aP5_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(audiofile, model, w_options, w_translate=False):\n",
        "\n",
        "    # Load audio\n",
        "    audio = whisper.load_audio(audiofile)\n",
        "    transcribe_options = dict(task=\"transcribe\", **w_options)\n",
        "    translate_options = dict(task=\"translate\", **w_options)\n",
        "\n",
        "    transcription = model.transcribe(audiofile, **transcribe_options)[\"text\"]\n",
        "    if w_translate:\n",
        "        translation = model.transcribe(audiofile, **translate_options)[\"text\"]\n",
        "    else:\n",
        "        translation = \"N/A\"\n",
        "    return transcription, translation"
      ],
      "metadata": {
        "id": "gs5XatDs2VQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_options = dict(without_timestamps=True, fp16=(DEVICE == \"cuda\"))\n",
        "audiofile = 'Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3'\n",
        "transcription, translation = process_file(audiofile, model, w_options, False)\n",
        "\n",
        "print(\"------\\nTranscription of file '\" + audiofile + \"':\")\n",
        "for sent in sent_tokenize(transcription):\n",
        "    print(sent)\n",
        "print(\"------\\nTranslation of file '\" + audiofile + \"':\")\n",
        "for sent in sent_tokenize(translation):\n",
        "    print(sent)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "widgets.Audio.from_file(audiofile, autoplay=False, loop=False)"
      ],
      "metadata": {
        "id": "h_oUto7xJ1Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_options = dict(without_timestamps=True, fp16=(DEVICE == \"cuda\"), temperature=0, initial_prompt=\"\")\n",
        "audiofile = 'product_names.wav'\n",
        "transcription, translation = process_file(audiofile, model, w_options)\n",
        "\n",
        "print(\"------\\nTranscription of file '\" + audiofile + \"':\")\n",
        "for sent in sent_tokenize(transcription):\n",
        "    print(sent)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "widgets.Audio.from_file(audiofile, autoplay=False, loop=False)"
      ],
      "metadata": {
        "id": "nP2w21IoTB8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_options = dict(without_timestamps=True, fp16=(DEVICE == \"cuda\"), temperature=0, initial_prompt=\"Quirk Quid Quill Inc., P3-Quattro, O3-Omni, B3-BondX, E3-Equity, W3-WrapZ, O2-Outlier, U3-UniFund, M3-Mover\")\n",
        "audiofile = 'product_names.wav'\n",
        "transcription, translation = process_file(audiofile, model, w_options)\n",
        "\n",
        "print(\"------\\nTranscription of file '\" + audiofile + \"':\")\n",
        "for sent in sent_tokenize(transcription):\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "WCjxOqnxQQem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_options = dict(without_timestamps=True, fp16=(DEVICE == \"cuda\"), temperature=0, initial_prompt=\"\")\n",
        "audiofile = 'bbq_plans.wav'\n",
        "transcription, translation = process_file(audiofile, model, w_options)\n",
        "\n",
        "print(\"------\\nTranscription of file '\" + audiofile + \"':\")\n",
        "for sent in sent_tokenize(transcription):\n",
        "    print(sent)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "widgets.Audio.from_file(audiofile, autoplay=False, loop=False)"
      ],
      "metadata": {
        "id": "206ZmGBFXApN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_options = dict(without_timestamps=True, fp16=(DEVICE == \"cuda\"), temperature=0, initial_prompt=\"Friends: Aimee, Shawn\")\n",
        "audiofile = 'bbq_plans.wav'\n",
        "transcription, translation = process_file(audiofile, model, w_options)\n",
        "\n",
        "print(\"------\\nTranscription of file '\" + audiofile + \"':\")\n",
        "for sent in sent_tokenize(transcription):\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "PnA8yY1DXApO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_options = dict(without_timestamps=True, fp16=(DEVICE == \"cuda\"), temperature=0, initial_prompt=\"\"\"\"Aimee and Shawn had whisky, doughnuts, omelets at a BBQ.\"\"\")\n",
        "audiofile = 'bbq_plans.wav'\n",
        "transcription, translation = process_file(audiofile, model, w_options)\n",
        "\n",
        "print(\"------\\nTranscription of file '\" + audiofile + \"':\")\n",
        "for sent in sent_tokenize(transcription):\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "_35FCBquYSv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cXT4e_I9upeU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}