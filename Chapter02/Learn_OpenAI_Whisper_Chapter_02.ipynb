{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4cb7560a",
      "metadata": {
        "id": "4cb7560a"
      },
      "source": [
        "# Learn OpenAI's Whisper - Chapter 2 complementary Python code\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Lj895IOyp0OL4RJMk3m6aPZ1w_NcPyp2)\n",
        "\n",
        "This notebook is designed to serve as a comprehensive guide for handling and analyzing audio data, with practical examples and explanations tailored to the context of Whisper and automatic speech recognition (ASR).\n",
        "\n",
        "**Understanding audio signals**\n",
        "\n",
        "An audio signal is a digital representation of sound waves. When we explore audio processing, especially with Python's Librosa library, we are essentially transforming these sound waves into a format that can be manipulated and analyzed computationally, a crucial step in Whisper's speech recognition process.\n",
        "\n",
        "Sound waves are vibrations that travel through mediums like air and are captured by our ears, enabling us to perceive sound. In the digital world, these vibrations are captured by microphones, which convert the analog waves into electrical signals. These signals are then digitized, resulting in a digital audio signal that can be stored, transmitted, and processed by computers, like Whisper's ASR system.\n",
        "\n",
        "The digital audio signal is typically represented as a sequence of numbers, each corresponding to the air pressure at a specific point in time. This sequence, known as a waveform, can be manipulated using various digital signal processing techniques to extract meaningful information or to alter the sound in some way, a process integral to Whisper's operation.\n",
        "\n",
        "**Introducing Librosa**\n",
        "\n",
        "[Librosa](https://librosa.org/doc/latest/index.html) is a powerful Python library designed for audio and music analysis. It simplifies the process of working with audio signals by providing a comprehensive toolkit for loading, analyzing, and manipulating these signals. With Librosa, one can perform tasks such as feature extraction, which includes obtaining Mel-frequency cepstral coefficients (MFCCs), spectral contrast, and tonnetz. These features are crucial for various applications in music information retrieval, speech recognition, and audio classification, including Whisper's ASR system.\n",
        "\n",
        "Working with audio signals in Python using Librosa involves loading the audio file into a NumPy array, which then allows for the application of various processing techniques. Librosa's functionality is not limited to feature extraction; it also includes capabilities for beat tracking, pitch estimation, and creating visual representations of audio signals, such as spectrograms, all of which can be beneficial in the context of Whisper and ASR."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load required Python libraries"
      ],
      "metadata": {
        "id": "v_g0FsjagJUe"
      },
      "id": "v_g0FsjagJUe"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sounddevice\n",
        "# !pip install soundfile\n",
        "!pip install librosa\n",
        "!sudo apt-get install libportaudio2\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "'''\n",
        "https://pypi.org/project/gdown/\n",
        "Download a large file from Google Drive.\n",
        "If you use curl/wget, it fails with a large file because of the security warning from Google Drive.\n",
        "Supports downloading from Google Drive folders (max 50 files per folder).\n",
        "'''\n",
        "!pip install --upgrade gdown"
      ],
      "metadata": {
        "id": "b2Q_CpQ5oWFK"
      },
      "id": "b2Q_CpQ5oWFK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e946be5",
      "metadata": {
        "id": "5e946be5"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f52b8c9c",
      "metadata": {
        "id": "f52b8c9c"
      },
      "outputs": [],
      "source": [
        "# import sounddevice as sd\n",
        "# import soundfile as sf\n",
        "import numpy as np\n",
        "import wave\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01da6dd2",
      "metadata": {
        "id": "01da6dd2"
      },
      "source": [
        "# Loading sample audio files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6586c658"
      },
      "outputs": [],
      "source": [
        "# Download using Google Drive Ids\n",
        "\n",
        "file_id1 = '11ur787d87Z49luASqxNUqyiRsEIAR0O7'\n",
        "file_id2 = '1gAcVBDBO9MJw-8TEHGORRd0cKjrRtQEY'\n",
        "\n",
        "!gdown $file_id1 -O Learn_OAI_Whisper_Sample_Audio01.mp3\n",
        "!gdown $file_id2 -O Learn_OAI_Whisper_Sample_Audio02.mp3"
      ],
      "id": "6586c658"
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the uploaded audio file\n",
        "audio_file_path = '/content/Learn_OAI_Whisper_Sample_Audio01.mp3'\n",
        "\n",
        "# Load the audio file\n",
        "audio, sampling_frequency = librosa.load(audio_file_path, mono=False)  # You can adjust the sample rate as needed"
      ],
      "metadata": {
        "id": "lM6lURQcs4PY"
      },
      "id": "lM6lURQcs4PY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55e69fae",
      "metadata": {
        "id": "55e69fae"
      },
      "outputs": [],
      "source": [
        "Audio(audio, rate=sampling_frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f58c84",
      "metadata": {
        "id": "14f58c84"
      },
      "source": [
        "## Coverting to MP3 to WAV file format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "audio_file_path = 'Learn_OAI_Whisper_Sample_Audio01.mp3'\n",
        "audio, sampling_frequency = librosa.load(audio_file_path, mono=True)  # You can adjust the sample rate as needed\n",
        "sf.write('Learn_OAI_Whisper_Sample_Audio01.wav', audio, sampling_frequency) # Can only convert monoaural files, due to a bug in C library libsndfile version <= 1.0.25"
      ],
      "metadata": {
        "id": "ICqdGXx0boXP"
      },
      "id": "ICqdGXx0boXP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abc4ce3",
      "metadata": {
        "id": "9abc4ce3"
      },
      "outputs": [],
      "source": [
        "with wave.open('/content/Learn_OAI_Whisper_Sample_Audio01.wav', 'rb') as wav_file:\n",
        "    channels_number, sample_width, framerate, frames_number, compression_type, compression_name = wav_file.getparams()\n",
        "    frames = wav_file.readframes(frames_number)\n",
        "    audio_signal = np.frombuffer(frames, dtype='<i2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a25878",
      "metadata": {
        "id": "e2a25878"
      },
      "outputs": [],
      "source": [
        "channels_number, sample_width, framerate, frames_number, compression_type, compression_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48cf1ab9",
      "metadata": {
        "id": "48cf1ab9"
      },
      "outputs": [],
      "source": [
        "Audio(audio_signal, rate=sampling_frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b04702",
      "metadata": {
        "id": "44b04702"
      },
      "source": [
        "# Librosa use cases"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the audio samples using librosa"
      ],
      "metadata": {
        "id": "jNFVl-f2i4vs"
      },
      "id": "jNFVl-f2i4vs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de4cf7b",
      "metadata": {
        "id": "3de4cf7b"
      },
      "outputs": [],
      "source": [
        "mono_file = \"Learn_OAI_Whisper_Sample_Audio01.mp3\"\n",
        "stereo_file = \"Learn_OAI_Whisper_Sample_Audio02.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "663930c1",
      "metadata": {
        "id": "663930c1"
      },
      "outputs": [],
      "source": [
        "mono_signal, sample_rate = librosa.load(mono_file)\n",
        "stereo_signal, sample_rate = librosa.load(stereo_file, mono=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(data=stereo_signal, rate=sample_rate)"
      ],
      "metadata": {
        "id": "7Y-bKI2a0dHt"
      },
      "id": "7Y-bKI2a0dHt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d02b0a6",
      "metadata": {
        "id": "6d02b0a6"
      },
      "outputs": [],
      "source": [
        "Audio(data=mono_signal, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribing with Whisper"
      ],
      "metadata": {
        "id": "6ktPc6bEFui1"
      },
      "id": "6ktPc6bEFui1"
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Load the small English model\n",
        "model = whisper.load_model(\"small\")"
      ],
      "metadata": {
        "id": "FyTSMCMnImDn"
      },
      "id": "FyTSMCMnImDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe the mono audio file\n",
        "result = model.transcribe(mono_file)\n",
        "print(\"Transcription of mono_file:\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "XQN1vzABIKFl"
      },
      "id": "XQN1vzABIKFl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe the stereo audio file\n",
        "result = model.transcribe(stereo_file)\n",
        "print(\"Transcription of stereo_file:\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "Q3NTddFBFo88"
      },
      "id": "Q3NTddFBFo88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f5b92648",
      "metadata": {
        "id": "f5b92648"
      },
      "source": [
        "## Audio sampling\n",
        "\n",
        "Sampling refers to sample the signal at specific time intervals. Either you can upsample or downsample your audio files based on your requirement.\n",
        "\n",
        "Original sampling rate is at 44 kHz since we have observed previously. We can upsample or downsample it by **librosa.resample()** function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = mono_file\n",
        "# get raw audio features\n",
        "print(\"Sampling Rate : \"+ str(librosa.get_samplerate(audio_file))+\"Hz\")\n",
        "print(\"Duration : \"+ str(librosa.get_duration(path=audio_file))+\"s\")\n",
        "\n",
        "# import the audio files\n",
        "signal, sampling_rate = librosa.load(audio_file, mono=False)\n",
        "# # Shape of features\n",
        "print(\"Shape of Initial Data : \"+ str(signal.shape))"
      ],
      "metadata": {
        "id": "WiUP0RVP3Rra"
      },
      "id": "WiUP0RVP3Rra",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downsample to 22 kHz\n",
        "signal_22k = librosa.resample(signal, orig_sr=sampling_rate, target_sr=8000)\n",
        "# Upsample to 88 kHz\n",
        "signal_88k = librosa.resample(signal, orig_sr=sampling_rate, target_sr=96000)\n",
        "print(signal_22k.shape)\n",
        "print(signal_88k.shape)"
      ],
      "metadata": {
        "id": "M39X7xpYlyGl"
      },
      "id": "M39X7xpYlyGl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e10ca357",
      "metadata": {
        "id": "e10ca357"
      },
      "source": [
        "## Pitch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67cf35e",
      "metadata": {
        "id": "f67cf35e"
      },
      "outputs": [],
      "source": [
        "# Load audio file\n",
        "y, sr = librosa.load(mono_file)\n",
        "\n",
        "# Compute pitch\n",
        "f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "\n",
        "# Plot pitch contour\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveshow(y, sr=sr, alpha=0.5)\n",
        "plt.plot(librosa.frames_to_time(range(len(f0))), f0, color='r')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.title('Pitch Contour')\n",
        "plt.show()\n",
        "Audio(y, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6e54b4",
      "metadata": {
        "id": "df6e54b4"
      },
      "source": [
        "## Pitch shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae591708",
      "metadata": {
        "id": "ae591708"
      },
      "outputs": [],
      "source": [
        "# Shift the pitch down by two semitones\n",
        "pitch_shifted_waveform = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=-4.0)\n",
        "Audio(pitch_shifted_waveform, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4dc9e26",
      "metadata": {
        "id": "e4dc9e26"
      },
      "outputs": [],
      "source": [
        "# Normalize the output signal\n",
        "pitch_shifted = librosa.util.normalize(pitch_shifted_waveform)\n",
        "Audio(pitch_shifted, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80bad36f",
      "metadata": {
        "id": "80bad36f"
      },
      "outputs": [],
      "source": [
        "# Shift the pitch up by five semitones\n",
        "pitch_shifted_helium_voice = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=6.0)\n",
        "Audio(pitch_shifted_helium_voice, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bdf26c3",
      "metadata": {
        "id": "7bdf26c3"
      },
      "source": [
        "## Time Stretch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe0e116",
      "metadata": {
        "id": "9fe0e116"
      },
      "outputs": [],
      "source": [
        "# Stretch the time by a factor of 2\n",
        "time_stretched_waveform = librosa.effects.time_stretch(pitch_shifted_helium_voice, rate=2)\n",
        "Audio(time_stretched_waveform, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2768638",
      "metadata": {
        "id": "f2768638"
      },
      "source": [
        "## Beats generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8cb83e2",
      "metadata": {
        "id": "c8cb83e2"
      },
      "outputs": [],
      "source": [
        "# Set the parameters for the WAV file\n",
        "duration = 2.0  # seconds\n",
        "frequency = 440.0  # Hz (A440)\n",
        "\n",
        "# Generate the audio data for the WAV file\n",
        "num_samples = 50000\n",
        "data = librosa.tone(frequency, sr=22050, length=num_samples)\n",
        "\n",
        "Audio(data, rate=22050)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duration = 2.0  # seconds\n",
        "frequency = 440.0  # Hz (A440)\n",
        "sample_rate = 22050  # Hz\n",
        "\n",
        "# Calculate the number of samples\n",
        "num_samples = int(duration * sample_rate)\n",
        "\n",
        "# Generate the tone\n",
        "data = librosa.tone(frequency, sr=sample_rate, length=num_samples)\n",
        "\n",
        "Audio(data, rate=sample_rate)"
      ],
      "metadata": {
        "id": "eRAW9jd38B9S"
      },
      "id": "eRAW9jd38B9S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.00535,
          "end_time": "2022-12-02T02:20:43.977753",
          "exception": false,
          "start_time": "2022-12-02T02:20:43.972403",
          "status": "completed"
        },
        "tags": [],
        "id": "9ee48beb"
      },
      "source": [
        "## Audio Channels\n",
        "There are two main types of audios as Monophonic (mono) and Stereophonic (stereo) sound. It classified based on the number of channels used to record and playback audio. But there can be more than two channels as well.\n",
        "Stereo sounds are much advance and it enables to localize the data which easy to locate the position of a sound source within a space.Usually we used mono channel audio for preprocessing tasks since it is complex to analyze high-dimensional data.\n",
        "\n",
        "Number of channels can be viewed by **.shape** attribute of the audio signal and librosa has an inbuilt function to convert into mono channel.\n",
        "\n",
        "As in the above example, original audio file is in stereo format as it is in **(2, 1188584)** format. By converting it into mono, it takes the average of each channel and creates a new signal."
      ],
      "id": "9ee48beb"
    },
    {
      "cell_type": "code",
      "source": [
        "print(stereo_signal.shape)# Initially the signal is in stereo form (2 channels)\n",
        "audio_mono = librosa.to_mono(signal)\n",
        "print(audio_mono.shape) # Mono signal"
      ],
      "metadata": {
        "id": "Gjs-Mfcewrue"
      },
      "id": "Gjs-Mfcewrue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3eb9ad70",
      "metadata": {
        "id": "3eb9ad70"
      },
      "source": [
        "## Plotting the signal amplitude\n",
        "\n",
        "**librosa.display** gives a high level representation of the audio in time domain with amplitude in the y axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72bb9453",
      "metadata": {
        "id": "72bb9453"
      },
      "outputs": [],
      "source": [
        "import librosa.display\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "librosa.display.waveshow(mono_signal, sr=sample_rate) # use waveplot should waveshow be unavailable\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Modifying the script to handle both stereo and mono audio signals is a great approach for flexibility. The script will first check if the input is a stereo signal. If it is, it will plot both channels. If it's a mono signal, it will just plot that single channel. Here's the revised script:"
      ],
      "metadata": {
        "id": "wOOwTCFjriZe"
      },
      "id": "wOOwTCFjriZe"
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Use commenting/uncommenting to see the plots\n",
        "# audio_signal_to_plot = mono_signal\n",
        "audio_signal_to_plot = stereo_signal\n",
        "\n",
        "# Ensure audio_signal_to_plot is a NumPy array\n",
        "audio_signal_to_plot = np.array(audio_signal_to_plot)\n",
        "\n",
        "# Check if audio_signal_to_plot is a stereo signal\n",
        "if audio_signal_to_plot.ndim == 2 and audio_signal_to_plot.shape[0] == 2:\n",
        "    # Two channels, each row is a channel\n",
        "    channel_1, channel_2 = audio_signal_to_plot\n",
        "    is_stereo = True\n",
        "elif audio_signal_to_plot.ndim == 2 and audio_signal_to_plot.shape[1] == 2:\n",
        "    # Two channels, each column is a channel\n",
        "    channel_1, channel_2 = audio_signal_to_plot.T\n",
        "    is_stereo = True\n",
        "else:\n",
        "    # It's a mono signal\n",
        "    is_stereo = False\n",
        "\n",
        "# Plotting\n",
        "if is_stereo:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Plotting the first channel\n",
        "    plt.subplot(2, 1, 1)\n",
        "    librosa.display.waveshow(channel_1, sr=sample_rate)\n",
        "    plt.title('Channel 1')\n",
        "    # Plotting the second channel\n",
        "    plt.subplot(2, 1, 2)\n",
        "    librosa.display.waveshow(channel_2, sr=sample_rate)\n",
        "    plt.title('Channel 2')\n",
        "    plt.tight_layout()\n",
        "else:\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    librosa.display.waveshow(audio_signal_to_plot, sr=sample_rate)\n",
        "    plt.title('Mono Audio Signal')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qu4nzzJXD7GG"
      },
      "id": "Qu4nzzJXD7GG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.007814,
          "end_time": "2022-12-02T02:20:48.772809",
          "exception": false,
          "start_time": "2022-12-02T02:20:48.764995",
          "status": "completed"
        },
        "tags": [],
        "id": "ca8cf034"
      },
      "source": [
        "## Time domain vs Frequency domain\n",
        "\n",
        "As you observe in the previous diagram, it is much complex to take a greater understanding about the **.mp3** file features from the raw audio waveplot. The reason is, it visulized in time domain and no any mathematical representation is available. On the contray, frequency domain delivers lot more understanding via decomping complex waves as a sum of sine wave oscillations. This operation often known as **Fourier Transformation**."
      ],
      "id": "ca8cf034"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fast-fourier transformation (FFT)\n",
        "\n",
        "This will result a **spectrum** as time is in x-axis and frequency is in y-axis. **Discrete fourier transformation** or DFT is transform digital signal into frequency representation. But, FFT is widely used as an efficient fourier transformation technique for audio processing. It takes the DFT and its inverse by factorization into sparse matrix."
      ],
      "metadata": {
        "id": "480G0aQfzPUO"
      },
      "id": "480G0aQfzPUO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-02T02:20:49.091497Z",
          "iopub.status.busy": "2022-12-02T02:20:49.089063Z",
          "iopub.status.idle": "2022-12-02T02:20:49.631453Z",
          "shell.execute_reply": "2022-12-02T02:20:49.629867Z"
        },
        "papermill": {
          "duration": 0.561533,
          "end_time": "2022-12-02T02:20:49.634326",
          "exception": false,
          "start_time": "2022-12-02T02:20:49.072793",
          "status": "completed"
        },
        "tags": [],
        "id": "8635afe1"
      },
      "outputs": [],
      "source": [
        "audio = mono_signal\n",
        "# Calculate the Fourier transform of the signal\n",
        "fft = np.fft.fft(audio)\n",
        "# Calculate absolute values on complex numbers to get magnitude\n",
        "spectrum = np.abs(fft)\n",
        "# Create frequency variable\n",
        "f = np.linspace(0, sr, len(spectrum))\n",
        "# Take half of the spectrum and frequency\n",
        "left_spectrum = spectrum[:int(len(spectrum)/2)]\n",
        "left_f = f[:int(len(spectrum)/2)]\n",
        "\n",
        "# Plot spectrum\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(left_f, left_spectrum, alpha=0.4)\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Magnitude\")\n",
        "plt.title(\"Power spectrum\")\n",
        "plt.show()"
      ],
      "id": "8635afe1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.008646,
          "end_time": "2022-12-02T02:20:49.652034",
          "exception": false,
          "start_time": "2022-12-02T02:20:49.643388",
          "status": "completed"
        },
        "tags": [],
        "id": "4b51715e"
      },
      "source": [
        "### Short-time fourier transform (STFF)\n",
        "\n",
        "In FFT, we lose time information when we are converting from time domain to frequency domain representation. As a remedy, SSFT can be applied."
      ],
      "id": "4b51715e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-02T02:20:48.791139Z",
          "iopub.status.busy": "2022-12-02T02:20:48.790682Z",
          "iopub.status.idle": "2022-12-02T02:20:49.053335Z",
          "shell.execute_reply": "2022-12-02T02:20:49.052135Z"
        },
        "papermill": {
          "duration": 0.275564,
          "end_time": "2022-12-02T02:20:49.056482",
          "exception": false,
          "start_time": "2022-12-02T02:20:48.780918",
          "status": "completed"
        },
        "tags": [],
        "id": "dd669ec1"
      },
      "outputs": [],
      "source": [
        "audio = mono_signal\n",
        "n_fft = 2048\n",
        "\n",
        "'''\n",
        "Short-time Fourier transform (STFT).\n",
        "The STFT represents a signal in the time-frequency domain by\n",
        "computing discrete Fourier transforms (DFT) over short overlapping\n",
        "windows.\n",
        "'''\n",
        "\n",
        "ft = np.abs(librosa.stft(audio[:n_fft], n_fft=n_fft, hop_length = n_fft+1))\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(ft);\n",
        "plt.title('Spectrum');\n",
        "plt.xlabel('Frequency Bin');\n",
        "plt.ylabel('Amplitude');"
      ],
      "id": "dd669ec1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-02T02:20:49.671668Z",
          "iopub.status.busy": "2022-12-02T02:20:49.671270Z",
          "iopub.status.idle": "2022-12-02T02:20:51.290641Z",
          "shell.execute_reply": "2022-12-02T02:20:51.288898Z"
        },
        "papermill": {
          "duration": 1.632362,
          "end_time": "2022-12-02T02:20:51.293370",
          "exception": false,
          "start_time": "2022-12-02T02:20:49.661008",
          "status": "completed"
        },
        "tags": [],
        "id": "38c58dff"
      },
      "outputs": [],
      "source": [
        "# load audio file\n",
        "audio_file = mono_file\n",
        "\n",
        "signal, sample_rate = librosa.load(audio_file, sr=22050)\n",
        "\n",
        "hop_length = 512 # In num. of samples\n",
        "n_fft = 2048 # Window in num. of samples\n",
        "\n",
        "# Calculate duration hop length and window in seconds\n",
        "hop_length_duration = float(hop_length)/sample_rate\n",
        "n_fft_duration = float(n_fft)/sample_rate\n",
        "print(\"STFT hop length duration is: {}s\".format(hop_length_duration))\n",
        "print(\"STFT window duration is: {}s\".format(n_fft_duration))\n",
        "\n",
        "# Perform stft and take the absolute value\n",
        "spectrogram = np.abs(librosa.stft(signal, n_fft=n_fft, hop_length=hop_length))\n",
        "\n",
        "# Display spectrogram\n",
        "librosa.display.specshow(spectrogram, sr=sample_rate, hop_length=hop_length)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.colorbar()\n",
        "plt.title(\"Spectrogram\")\n",
        "plt.show()"
      ],
      "id": "38c58dff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.0091,
          "end_time": "2022-12-02T02:20:51.312495",
          "exception": false,
          "start_time": "2022-12-02T02:20:51.303395",
          "status": "completed"
        },
        "tags": [],
        "id": "daa5a955"
      },
      "source": [
        "Above plot not gonna display much information (very tiny area is visible). Reason is human auditory space is very thin and most sounds humans hear are concentrated in very small frequency and amplitude ranges. A small adjustment will display more discriptive view of spectrum as below."
      ],
      "id": "daa5a955"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectral features\n",
        "\n",
        "Spectral features capture the frequency domain representation of an audio signal, providing information about the different frequency components present in the signal. They help to capture the timbre and texture of sounds and the energy distribution across different frequency bands. This information is vital in ASR systems like Whisper, as it helps to distinguish between different phonemes, the smallest units of sound that can distinguish one word from another in a particular language.\n",
        "\n",
        "**Librosa** provides easy-to-use functions for computing spectral features. It offers multiple options for spectral feature extraction, including the mel-spectrogram and its coefficients, and Chroma features. These features are particularly useful in ASR as they provide a compact representation of the spectral content of the audio signal, emphasizing various aspects of the signal, such as its harmonic or percussive content.\n",
        "\n",
        "In the context of ***Whisper***, these spectral features can be used to improve the accuracy of speech recognition. By analyzing the spectral content of the audio signal, Whisper can better understand the nuances of spoken language, making it more robust to variations in speech such as accents, speech speed, and background noise. This allows Whisper to convert spoken language into written text more accurately, making it a valuable tool for transcribing audio files"
      ],
      "metadata": {
        "id": "jOCcEwA3tad1"
      },
      "id": "jOCcEwA3tad1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-02T02:20:51.334032Z",
          "iopub.status.busy": "2022-12-02T02:20:51.333599Z",
          "iopub.status.idle": "2022-12-02T02:20:53.049762Z",
          "shell.execute_reply": "2022-12-02T02:20:53.048409Z"
        },
        "papermill": {
          "duration": 1.730673,
          "end_time": "2022-12-02T02:20:53.052501",
          "exception": false,
          "start_time": "2022-12-02T02:20:51.321828",
          "status": "completed"
        },
        "tags": [],
        "id": "10a8025f"
      },
      "outputs": [],
      "source": [
        "# Magnitude scaling of an amplitude spectrogram to dB-scaled spectrogram.\n",
        "DB = librosa.amplitude_to_db(spectrogram, ref=np.max)\n",
        "librosa.display.specshow(DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='log');\n",
        "plt.colorbar(format='%+2.0f dB');\n",
        "plt.title(\"Spectrogram (dB)\")"
      ],
      "id": "10a8025f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.011129,
          "end_time": "2022-12-02T02:20:53.075312",
          "exception": false,
          "start_time": "2022-12-02T02:20:53.064183",
          "status": "completed"
        },
        "tags": [],
        "id": "200c61aa"
      },
      "source": [
        "Above representation known as **spectrogram**. As display in the code, we applied logarithm to cast amplitude to Decible (DB) scale for previously compute stft spectrogram. As you may have understood, STFT produce by computing FT at different intervals. By doing so it preseves the time dimension information.\n",
        "\n",
        "This STFT information is an essential input for any deep learning based model."
      ],
      "id": "200c61aa"
    },
    {
      "cell_type": "markdown",
      "id": "180cf7d6",
      "metadata": {
        "id": "180cf7d6"
      },
      "source": [
        "### The mel-spectrogram\n",
        "\n",
        "The mel-spectrogram is a pivotal feature in the domain of audio and music analysis, and its utility extends to the sophisticated tasks of sound classification and automatic speech recognition (ASR), such as those performed by OpenAI's Whisper. This spectral representation is adept at encapsulating the timbral and textural nuances of sounds, as well as delineating the energy distribution across various frequency bands.\n",
        "\n",
        "In the realm of ASR, the mel-spectrogram serves as a foundational input, providing a rich, condensed depiction of the audio signal's spectral content. This is particularly beneficial for Whisper, which requires a detailed frequency analysis to accurately transcribe spoken language into written text. The mel-spectrogram's emphasis on the perceptually relevant mel scale aligns closely with human auditory processing, making it an effective tool for capturing the essential characteristics of speech.\n",
        "\n",
        "To compute a mel-spectrogram, one can utilize the `librosa.feature.melspectrogram` function, which processes the audio signal and yields a two-dimensional array representing the power spectrum of sound across mel frequency bands. This can then be visualized using `librosa.display.specshow`, offering insights into the signal's structure that are instrumental for Whisper's learning algorithms.\n",
        "\n",
        "The mel-spectrogram's ability to abstract and highlight salient features of the audio signal makes it an indispensable component in Whisper's ASR system. By leveraging this representation, Whisper can enhance its performance, particularly in challenging acoustic environments with noise, multiple speakers, or diverse accents, thereby solidifying its position as a robust and versatile speech recognition tool[3][5].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9e25d16",
      "metadata": {
        "id": "e9e25d16"
      },
      "outputs": [],
      "source": [
        "# Load the recorded file\n",
        "signal, sr = librosa.load(mono_file)\n",
        "signal, sr = librosa.load(stereo_file)\n",
        "\n",
        "# Compute the mel-spectrogram\n",
        "mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr)\n",
        "\n",
        "# Plot the mel-spectrogram\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(librosa.power_to_db(mel_spectrogram, ref=np.max), sr=sr, hop_length=512, y_axis=\"mel\", x_axis=\"time\")\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel-spectrogram\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's modify the script so that it displays two plots – one for each audio file, \"mono_file\" and \"stereo_file\" – we need to load each file separately, compute their respective mel-spectrograms, and then plot them in separate subplots. In this script:\n",
        "\n",
        "* Each audio file is loaded separately, and their mel-spectrograms are computed.\n",
        "* \"**plt.subplot(2, 1, 1)**\" and \"**plt.subplot(2, 1, 2)**\" are used to create two subplots in a single column.\n",
        "* The mel-spectrograms of **\"mono_file\"** and **\"stereo_file\"** are displayed in the first and second subplots, respectively."
      ],
      "metadata": {
        "id": "xKvyLH4BGapo"
      },
      "id": "xKvyLH4BGapo"
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the first file\n",
        "signal_1, sr_1 = librosa.load(mono_file)\n",
        "\n",
        "# Compute the mel-spectrogram for the first file\n",
        "mel_spectrogram_1 = librosa.feature.melspectrogram(y=signal_1, sr=sr_1)\n",
        "\n",
        "# Load the second file\n",
        "signal_2, sr_2 = librosa.load(stereo_file)\n",
        "\n",
        "# Compute the mel-spectrogram for the second file\n",
        "mel_spectrogram_2 = librosa.feature.melspectrogram(y=signal_2, sr=sr_2)\n",
        "\n",
        "# Create a figure for the subplots\n",
        "plt.figure(figsize=(20, 8))\n",
        "\n",
        "# Plot the mel-spectrogram of the first file\n",
        "plt.subplot(2, 1, 1)\n",
        "librosa.display.specshow(librosa.power_to_db(mel_spectrogram_1, ref=np.max), sr=sr_1, hop_length=512, y_axis=\"mel\", x_axis=\"time\")\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel-spectrogram of mono File\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Plot the mel-spectrogram of the second file\n",
        "plt.subplot(2, 1, 2)\n",
        "librosa.display.specshow(librosa.power_to_db(mel_spectrogram_2, ref=np.max), sr=sr_2, hop_length=512, y_axis=\"mel\", x_axis=\"time\")\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel-spectrogram of Stereo File\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zx55GruwFfgW"
      },
      "id": "zx55GruwFfgW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be asking, \"...is there a way we can represent that in the mel-spectogram?\"\n",
        "\n",
        "Surely! Although representing a stereo, 2-channel audio file in a mel-spectrogram involves a decision on how to handle the two channels. There are a few approaches:\n",
        "\n",
        "* **Average the Channels**: Compute the mel-spectrogram of the average of the two channels. This approach merges the stereo information into a single representation.\n",
        "* **Separate Spectrograms for Each Channel**: Compute and display separate mel-spectrograms for each channel.\n",
        "* **Sum the Channels**: Another method is to sum the channels before computing the mel-spectrogram, although this can sometimes lead to a loss of stereo-specific information.\n",
        "\n",
        "For most analysis purposes, the second approach (separate spectrograms for each channel) is often more informative as it maintains the distinct information from each channel. Here's how you can modify the script to handle a stereo file and plot separate mel-spectrograms for each channel of the \"stereo_file\":"
      ],
      "metadata": {
        "id": "eDz6JJI8Giej"
      },
      "id": "eDz6JJI8Giej"
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the first file (mono)\n",
        "signal_1, sr_1 = librosa.load(mono_file)\n",
        "\n",
        "# Compute and plot the mel-spectrogram for the first file\n",
        "mel_spectrogram_1 = librosa.feature.melspectrogram(y=signal_1, sr=sr_1)\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(librosa.power_to_db(mel_spectrogram_1, ref=np.max), sr=sr_1, hop_length=512, y_axis=\"mel\", x_axis=\"time\")\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel-spectrogram of mono File\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Load the second file (stereo)\n",
        "signal_2, sr_2 = librosa.load(stereo_file, mono=False)\n",
        "\n",
        "# Assuming the stereo signal has two channels\n",
        "channel_1, channel_2 = signal_2\n",
        "\n",
        "# Compute and plot the mel-spectrogram for each channel of the stereo file\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Channel 1\n",
        "mel_spectrogram_2_1 = librosa.feature.melspectrogram(y=channel_1, sr=sr_2)\n",
        "plt.subplot(2, 1, 1)\n",
        "librosa.display.specshow(librosa.power_to_db(mel_spectrogram_2_1, ref=np.max), sr=sr_2, hop_length=512, y_axis=\"mel\", x_axis=\"time\")\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel-spectrogram of Stereo File (Channel 1)\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Channel 2\n",
        "mel_spectrogram_2_2 = librosa.feature.melspectrogram(y=channel_2, sr=sr_2)\n",
        "plt.subplot(2, 1, 2)\n",
        "librosa.display.specshow(librosa.power_to_db(mel_spectrogram_2_2, ref=np.max), sr=sr_2, hop_length=512, y_axis=\"mel\", x_axis=\"time\")\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel-spectrogram of Stereo File (Channel 2)\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4IE_gXmKGWQz"
      },
      "id": "4IE_gXmKGWQz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.013568,
          "end_time": "2022-12-02T02:20:54.904802",
          "exception": false,
          "start_time": "2022-12-02T02:20:54.891234",
          "status": "completed"
        },
        "tags": [],
        "id": "ef09e693"
      },
      "source": [
        "### Mel-frequency cepstral coefficient (MFCC)\n",
        "\n",
        "Mel-frequency cepstral coefficients (MFCCs) are a widely recognized feature set used in the field of automatic speech recognition, including in advanced systems like OpenAI's Whisper. These coefficients provide a compact representation of the power spectrum of an audio signal, emphasizing the perceptual and phonetic characteristics that are crucial for distinguishing between different spoken words and sounds.\n",
        "\n",
        "The process of computing MFCCs involves several steps. First, the audio signal is passed through a mel-scale filter bank, which mimics the human ear's response to different frequencies. The logarithm of the powers at each of the mel frequencies is then taken, and a discrete cosine transform (DCT) is applied to the log-mel spectrum. This results in a set of coefficients that succinctly capture the essential information needed for speech recognition tasks.\n",
        "\n",
        "MFCCs are particularly useful for Whisper's deep learning models, as they encapsulate the relevant features that differentiate one phoneme from another. By incorporating MFCCs into its input features, Whisper can effectively train its neural networks to recognize patterns in speech, leading to more accurate transcriptions.\n",
        "\n",
        "The robustness of MFCCs in various acoustic environments makes them a staple in Whisper's toolkit for processing and understanding human speech. Their ability to capture the nuances of spoken language enables Whisper to perform with high precision across diverse datasets and in real-world scenarios where background noise and different accents are present."
      ],
      "id": "ef09e693"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-02T02:20:54.935780Z",
          "iopub.status.busy": "2022-12-02T02:20:54.935268Z",
          "iopub.status.idle": "2022-12-02T02:20:55.309673Z",
          "shell.execute_reply": "2022-12-02T02:20:55.308399Z"
        },
        "papermill": {
          "duration": 0.392987,
          "end_time": "2022-12-02T02:20:55.312423",
          "exception": false,
          "start_time": "2022-12-02T02:20:54.919436",
          "status": "completed"
        },
        "tags": [],
        "id": "a2995635"
      },
      "outputs": [],
      "source": [
        "hop_length = 512 # In num. of samples\n",
        "n_fft = 2048 # Window in num. of samples\n",
        "audio = mono_signal\n",
        "\n",
        "# Extract 8 MFCCs\n",
        "# MFCC = librosa.feature.mfcc(audio, sr, n_fft=n_fft, hop_length=hop_length, n_mfcc=8)\n",
        "MFCC = librosa.feature.mfcc(y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mfcc=8)\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "librosa.display.specshow(MFCC, sr=sr, hop_length=hop_length)\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"MFCC coefficients\")\n",
        "plt.colorbar()\n",
        "plt.title(\"MFCCs\")\n",
        "plt.show()"
      ],
      "id": "a2995635"
    },
    {
      "cell_type": "markdown",
      "id": "86f25e7b",
      "metadata": {
        "id": "86f25e7b"
      },
      "source": [
        "## Spectral Contrast\n",
        "\n",
        "Spectral contrast is a significant feature in the realm of audio signal processing, and it plays a crucial role in automatic speech recognition systems like OpenAI's Whisper. This feature quantifies the disparity in amplitudes between neighboring frequency bands within a power spectrum, providing a measure of the perceived \"brightness\" or \"spectral shape\" of an audio signal.\n",
        "\n",
        "In Whisper's context, spectral contrast can help differentiate between various phonetic elements in speech, as different sounds will exhibit different spectral shapes. This differentiation is vital for accurately transcribing spoken language into written text.\n",
        "\n",
        "The computation of spectral contrast involves dividing the audio signal's frequency spectrum into multiple frequency bands, typically using a logarithmic scale. The mean magnitude of the frequency spectrum within each sub-band is then computed, followed by the calculation of the standard deviation of the magnitudes across all sub-bands.\n",
        "\n",
        "Python's Librosa library provides the `spectral_contrast` function to compute this feature. By incorporating spectral contrast into its feature set, Whisper can enhance its ability to recognize and transcribe speech, even in challenging acoustic environments with background noise or multiple speakers. This makes spectral contrast a valuable tool in Whisper's toolkit for processing and understanding human speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c71374d",
      "metadata": {
        "id": "2c71374d"
      },
      "outputs": [],
      "source": [
        "# Compute the spectral contrast features\n",
        "spectral_contrast = librosa.feature.spectral_contrast(y=mono_signal, n_fft=2048, hop_length=512)\n",
        "\n",
        "# Plot the spectral contrast features\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(spectral_contrast, sr=sample_rate, hop_length=512, x_axis=\"time\")\n",
        "plt.title(\"Spectral contrast features\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa0f091",
      "metadata": {
        "id": "cfa0f091"
      },
      "source": [
        "### Chroma features\n",
        "\n",
        "Chroma features encapsulate the harmonic content of an audio signal, providing a compact representation that can be used for various audio and music analysis tasks.\n",
        "\n",
        "Chroma features can help distinguish between different phonetic elements in speech, as different sounds will exhibit different harmonic structures. This differentiation is vital for accurately transcribing spoken language into written text.\n",
        "\n",
        "The computation of chroma features involves mapping the spectrum of an audio signal to 12 bins representing the 12 distinct semitones (or chroma) of the musical octave. This process effectively captures the pitch content of the audio signal, disregarding the absolute frequency.\n",
        "\n",
        "Python's Librosa library provides the `chroma_stft` function to compute these features. By incorporating chroma features into its feature set, Whisper can enhance its ability to recognize and transcribe speech, even in challenging acoustic environments with background noise or multiple speakers. This makes chroma features a valuable tool in Whisper's toolkit for processing and understanding human speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e6c8b71",
      "metadata": {
        "id": "4e6c8b71"
      },
      "outputs": [],
      "source": [
        "# Compute the chroma features\n",
        "chroma_features = librosa.feature.chroma_stft(y=mono_signal, sr=sample_rate)\n",
        "\n",
        "# Plot the chroma features\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(chroma_features, sr=sample_rate, hop_length=512, x_axis=\"time\", y_axis=\"chroma\")\n",
        "plt.title(\"Chroma features\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap\n",
        "\n"
      ],
      "metadata": {
        "id": "iJLDMyAU91FL"
      },
      "id": "iJLDMyAU91FL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just for fun!"
      ],
      "metadata": {
        "id": "63ecVzAdE_If"
      },
      "id": "63ecVzAdE_If"
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "\n",
        "# Frequencies for the scale C3 to C4\n",
        "note_frequencies = {\n",
        "    'C3': 130.81,\n",
        "    'D3': 146.83,\n",
        "    'E3': 164.81,\n",
        "    'F3': 174.61,\n",
        "    'G3': 196.00,\n",
        "    'A3': 220.00,\n",
        "    'B3': 246.94,\n",
        "    'C4': 261.63\n",
        "}\n",
        "\n",
        "sample_rate = 22050  # Sample rate in Hz\n",
        "duration = 0.5       # Duration of each note in seconds\n",
        "\n",
        "# Create a list to hold the audio data for each note\n",
        "scale_data = []\n",
        "\n",
        "# Generate a tone for each note and append to the scale_data\n",
        "for note, freq in note_frequencies.items():\n",
        "    tone = librosa.tone(freq, sr=sample_rate, length=int(sample_rate * duration))\n",
        "    scale_data.append(tone)\n",
        "\n",
        "# Concatenate all the notes to form the scale\n",
        "scale = np.concatenate(scale_data)\n",
        "\n",
        "# Play the scale\n",
        "Audio(scale, rate=sample_rate)"
      ],
      "metadata": {
        "id": "6K-HqqG6924f"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6K-HqqG6924f"
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "\n",
        "# Mapping notes to frequencies (in Hz)\n",
        "note_frequencies = {\n",
        "    'A4': 440.0,\n",
        "    'D5': 587.33,\n",
        "    'F4': 349.23,\n",
        "    'G4': 392.00,\n",
        "    'D4': 293.66,\n",
        "    'C5': 523.25,\n",
        "    'E4': 329.63,\n",
        "    'C4': 261.63\n",
        "}\n",
        "\n",
        "# Beats per minute\n",
        "bpm = 120\n",
        "# Duration of one beat in seconds\n",
        "beat_duration = 60 / bpm\n",
        "\n",
        "# Theme song notes and durations (note x duration)\n",
        "theme_song = [\n",
        "    ('A4', 0.25), ('D5', 0.25), ('A4', 0.25), ('D5', 0.25), ('A4', 2), ('F4', 1),\n",
        "    ('G4', 1), ('D4', 3),\n",
        "    ('A4', 0.25), ('D5', 0.25), ('A4', 0.25), ('D5', 0.25), ('A4', 2), ('F4', 1),\n",
        "    ('G4', 1), ('C5', 3),\n",
        "    ('A4', 0.25), ('D5', 0.25), ('A4', 0.25), ('D5', 0.25), ('A4', 2), ('F4', 1),\n",
        "    ('E4', 0.5), ('D4', 0.5), ('C4', 3),\n",
        "    ('A4', 0.25), ('D5', 0.25), ('A4', 0.25), ('D5', 0.25), ('A4', 2), ('G4', 1),\n",
        "    ('D4', 1), ('C4', 3)\n",
        "]\n",
        "\n",
        "# Sample rate\n",
        "sample_rate = 22050\n",
        "\n",
        "# Create a list to hold the audio data for each note\n",
        "scale_data = []\n",
        "\n",
        "# Generate a tone for each note and append to the scale_data\n",
        "for note, duration in theme_song:\n",
        "    freq = note_frequencies[note]\n",
        "    tone_duration = duration * beat_duration\n",
        "    num_samples = int(sample_rate * tone_duration)\n",
        "    tone = librosa.tone(freq, sr=sample_rate, length=num_samples)\n",
        "    scale_data.append(tone)\n",
        "\n",
        "# Concatenate all the notes to form the theme song\n",
        "theme = np.concatenate(scale_data)\n",
        "\n",
        "# Play the theme song\n",
        "Audio(theme, rate=sample_rate)\n"
      ],
      "metadata": {
        "id": "KZ6qABbSELTK"
      },
      "id": "KZ6qABbSELTK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v_g0FsjagJUe",
        "01da6dd2",
        "9ee48beb",
        "ef09e693"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}