{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learn OpenAI Whisper - Chapter 8\n",
        "## Notebook 3: Fine-tuning voice cloning with Deep Learning Art School (DLAS)\n",
        "\n",
        "This notebook complements the book [Learn OpenAI Whisper](https://a.co/d/1p5k4Tg).\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wnomL0dxmU9CgPKIgazR8AocolEYjAe5)\n",
        "\n",
        "This notebook fine-tunes a voice cloning model using the DLAS toolkit. It is based on the Tortoise fine-tuning with DLAS project by James Betker (https://github.com/152334H/DL-Art-School).  The original batch size was 128 but changed to 90 since colab free tier only gives you an NVIDIA T4, a 128 batch size used 16GB of VRAM."
      ],
      "metadata": {
        "id": "YA-Mk63N0ac9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Checking the GPU:\n",
        "The code first checks if an NVIDIA GPU is available using the `nvidia-smi` command. It prints out the GPU information if connected."
      ],
      "metadata": {
        "id": "UjvK4JWKdYqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "Z6kCeDNrdWM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Checking virtual memory:\n",
        "It then checks the available RAM on the runtime using the `psutil` library. It prints a message if using a high-RAM runtime."
      ],
      "metadata": {
        "id": "sEHes5FQXbbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "pVHkIfjHXeIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Mounting Google Drive:\n",
        "The code mounts the user's Google Drive to save trained checkpoints and load the dataset."
      ],
      "metadata": {
        "id": "Agpune8JjsQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "xFt4umCqjlS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Installing requirements:\n",
        "It clones the DLAS repository, downloads pre-trained model checkpoints, and installs the required dependencies."
      ],
      "metadata": {
        "id": "sr_N55tZjFWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqAlGZg0hAw3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/josuebatista/DL-Art-School.git\n",
        "%cd DL-Art-School\n",
        "!wget https://huggingface.co/Gatozu35/tortoise-tts/resolve/main/dvae.pth -O experiments/dvae.pth\n",
        "!wget https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth -O experiments/autoregressive.pth\n",
        "!pip install -r codes/requirements.laxed.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MUST RESTART SESSION BEFORE PROCEEDING\n",
        "\n",
        "#### Check the integrity of the dVAE checkpoint\n",
        "You should see the following message when verified:\n",
        "\n",
        "`a990825371506c16bcf0e8167bf24ccf82f65bb6a1dbcbfcf058d76f9b197e35  ../DL-Art-School/experiments/dvae.pth`\n"
      ],
      "metadata": {
        "id": "6vZtdWXc4g9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Downgrading Transformers to previous version.\n",
        "It seems that last release of transformers broke the model loader.\n",
        "'''\n",
        "!pip install transformers==4.29.2\n",
        "\n",
        "# Check the integrity of the dVAE checkpoint\n",
        "!sha256sum /content/DL-Art-School/experiments/dvae.pth | grep a990825371506c16bcf0e8167bf24ccf82f65bb6a1dbcbfcf058d76f9b197e35 || echo \"SOMETHING IS WRONG WITH THE CHECKPOINT; REPORT THIS AS A GITHUB ISSUE AND DO NOT PROCEED\""
      ],
      "metadata": {
        "id": "9b359b_FnIZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Calculating hyperparameters:\n",
        "The code calculates suggested hyperparameters for training based on the provided dataset paths. It determines the batch sizes, learning rate decay steps, validation frequency, etc."
      ],
      "metadata": {
        "id": "BQTong_qj7sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from math import ceil\n",
        "DEFAULT_TRAIN_BS = 64\n",
        "DEFAULT_VAL_BS = 32\n",
        "#@markdown # Hyperparameter calculation\n",
        "#@markdown Run this cell to obtain suggested parameters for training\n",
        "Dataset_Training_Path = \"/content/gdrive/MyDrive/Generative_AI/Deep_Fakes_Voice/output/Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-16_36/train.txt\" #@param {type:\"string\"}\n",
        "ValidationDataset_Training_Path = \"/content/gdrive/MyDrive/Generative_AI/Deep_Fakes_Voice/output/Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-16_36/valid.txt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### **NOTE**: Dataset must be in the following format.\n",
        "\n",
        "#@markdown  `dataset/`\n",
        "#@markdown * ---├── `val.txt`\n",
        "#@markdown * ---├── `train.txt`\n",
        "#@markdown * ---├── `wavs/`\n",
        "\n",
        "#@markdown `wavs/` directory must contain `.wav` files.\n",
        "\n",
        "#@markdown  Example for `train.txt` and `val.txt`:\n",
        "\n",
        "#@markdown * `wavs/A.wav|Write the transcribed audio here.`\n",
        "\n",
        "#@markdown todo: actually check the dataset structure\n",
        "\n",
        "if Dataset_Training_Path == ValidationDataset_Training_Path:\n",
        "  print(\"WARNING: training dataset path == validation dataset path!!!\")\n",
        "  print(\"\\tThis is technically okay but will make all of the validation metrics useless. \")\n",
        "  print(\"it will also SUBSTANTIALLY slow down the rate of training, because validation datasets are supposed to be much smaller than training ones.\")\n",
        "\n",
        "def txt_file_lines(p: str) -> int:\n",
        "  return len(Path(p).read_text().strip().split('\\n'))\n",
        "training_samples = txt_file_lines(Dataset_Training_Path)\n",
        "val_samples = txt_file_lines(ValidationDataset_Training_Path)\n",
        "\n",
        "if training_samples < 128: print(\"WARNING: very small dataset! the smallest dataset tested thus far had ~200 samples.\")\n",
        "if val_samples < 20: print(\"WARNING: very small validation dataset! val batch size will be scaled down to account\")\n",
        "\n",
        "def div_spillover(n: int, bs: int) -> int: # returns new batch size\n",
        "  epoch_steps,remain = divmod(n,bs)\n",
        "  if epoch_steps*2 > bs: return bs # don't bother optimising this stuff if epoch_steps are high\n",
        "  if not remain: return bs # unlikely but still\n",
        "\n",
        "  if remain*2 < bs: # \"easier\" to get rid of remainder -- should increase bs\n",
        "    target_bs = n//epoch_steps\n",
        "  else: # easier to increase epoch_steps by 1 -- decrease bs\n",
        "    target_bs = n//(epoch_steps+1)\n",
        "  assert n%target_bs < epoch_steps+2 # should be very few extra\n",
        "  return target_bs\n",
        "\n",
        "if training_samples < DEFAULT_TRAIN_BS:\n",
        "  print(\"WARNING: dataset is smaller than a single batch. This will almost certainly perform poorly. Trying anyway\")\n",
        "  train_bs = training_samples\n",
        "else:\n",
        "  train_bs = div_spillover(training_samples, DEFAULT_TRAIN_BS)\n",
        "if val_samples < DEFAULT_VAL_BS:\n",
        "  val_bs = val_samples\n",
        "else:\n",
        "  val_bs = div_spillover(val_samples, DEFAULT_VAL_BS)\n",
        "\n",
        "steps_per_epoch = training_samples//train_bs\n",
        "lr_decay_epochs = [20, 40, 56, 72]\n",
        "lr_decay_steps = [steps_per_epoch * e for e in lr_decay_epochs]\n",
        "print_freq = min(100, max(20, steps_per_epoch))\n",
        "val_freq = save_checkpoint_freq = print_freq * 3\n",
        "\n",
        "print(\"===CALCULATED SETTINGS===\")\n",
        "print(f'{train_bs=} {val_bs=}')\n",
        "print(f'{val_freq=} {lr_decay_steps=}')\n",
        "print(f'{print_freq=} {save_checkpoint_freq=}')"
      ],
      "metadata": {
        "id": "lQQC93cjfmNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training settings:\n",
        "The user can specify various training settings such as experiment name, dataset names, saving training states, keeping checkpoints, etc."
      ],
      "metadata": {
        "id": "AFLjQOYOtqTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##_Settings for normal users:_\n",
        "Experiment_Name = \"Learn_OAI_Whisper_20240316\" #@param {type:\"string\"}\n",
        "Dataset_Training_Name= \"TestDataset\" #@param {type:\"string\"}\n",
        "ValidationDataset_Name = \"TestValidation\" # this seems to be useless??? @param {type:\"string\"}\n",
        "SaveTrainingStates = False # @param {type:\"boolean\"}\n",
        "Keep_Last_N_Checkpoints = 0 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "#@markdown * **NOTE**: 0 means \"keep all models saved\", which could potentially cause out-of-storage issues.\n",
        "#@markdown * Without training states, each model \"only\" takes up ~1.6GB. You should have ~50GB of free space to begin with.\n",
        "#@markdown * With training states, each model (pth+state) takes up ~4.9 GB; Colab will crash around ~10 undeleted checkpoints in this case.\n",
        "\n",
        "#@markdown ##_Other training parameters_\n",
        "Fp16 = False #@param {type:\"boolean\"}\n",
        "Use8bit = True #@param {type:\"boolean\"}\n",
        "#@markdown * **NOTE**: for some reason, fp16 does not seem to improve vram use when combined with 8bit [citation needed]. To be verified later...\n",
        "TrainingRate = \"1e-5\" #@param {type:\"string\"}\n",
        "TortoiseCompat = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown * **NOTE**: TortoiseCompat introduces some breaking changes to the training process. **If you want to reproduce older models**, disable this checkbox.\n",
        "\n",
        "#@markdown ##_Calculated settings_ override\n",
        "#@markdown #####Blank entries rely on the calculated defaults from the cell above.\n",
        "#@markdown ######**Leave them blank unless you want to adjust them manually**\n",
        "TrainBS = \"\" #@param {type:\"string\"}\n",
        "ValBS = \"\" #@param {type:\"string\"}\n",
        "ValFreq = \"\" #@param {type:\"string\"}\n",
        "LRDecaySteps = \"\" #@param {type:\"string\"}\n",
        "PrintFreq = \"\" #@param {type:\"string\"}\n",
        "SaveCheckpointFreq = \"\" #@param {type:\"string\"}\n",
        "\n",
        "def take(orig, override):\n",
        "  if override == \"\": return orig\n",
        "  return type(orig)(override)\n",
        "\n",
        "train_bs = take(train_bs, TrainBS)\n",
        "val_bs = take(val_bs, ValBS)\n",
        "val_freq = take(val_freq, ValFreq)\n",
        "lr_decay_steps = eval(LRDecaySteps) if LRDecaySteps else lr_decay_steps\n",
        "print_freq = take(print_freq, PrintFreq)\n",
        "save_checkpoint_freq = take(save_checkpoint_freq, SaveCheckpointFreq)\n",
        "assert len(lr_decay_steps) == 4\n",
        "gen_lr_steps = ', '.join(str(v) for v in lr_decay_steps)\n",
        "\n",
        "#@markdown #Run this cell after you finish editing the settings."
      ],
      "metadata": {
        "id": "ryVeEXfxqw3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Applying settings:\n",
        "The code applies the defined settings to a fresh YAML configuration file using `sed` commands."
      ],
      "metadata": {
        "id": "AutsfX4at6kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DL-Art-School\n",
        "# !wget https://raw.githubusercontent.com/152334H/DL-Art-School/master/experiments/EXAMPLE_gpt.yml -O experiments/EXAMPLE_gpt.yml\n",
        "!wget https://raw.githubusercontent.com/josuebatista/DL-Art-School/master/experiments/EXAMPLE_gpt.yml -O experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "import os\n",
        "%cd /content/DL-Art-School\n",
        "!sed -i 's/batch_size: 128/batch_size: '\"$train_bs\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/batch_size: 64/batch_size: '\"$val_bs\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/val_freq: 500/val_freq: '\"$val_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/500, 1000, 1400, 1800/'\"$gen_lr_steps\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/print_freq: 100/print_freq: '\"$print_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/save_checkpoint_freq: 500/save_checkpoint_freq: '\"$save_checkpoint_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "!sed -i 's+CHANGEME_validation_dataset_name+'\"$ValidationDataset_Name\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's+CHANGEME_path_to_validation_dataset+'\"$ValidationDataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "if(Fp16==True):\n",
        "  os.system(\"sed -i 's+fp16: false+fp16: true+g' ./experiments/EXAMPLE_gpt.yml\")\n",
        "!sed -i 's/use_8bit: true/use_8bit: '\"$Use8bit\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "!sed -i 's/disable_state_saving: true/disable_state_saving: '\"$SaveTrainingStates\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/tortoise_compat: True/tortoise_compat: '\"$TortoiseCompat\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/number_of_checkpoints_to_save: 0/number_of_checkpoints_to_save: '\"$Keep_Last_N_Checkpoints\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "\n",
        "!sed -i 's/CHANGEME_training_dataset_name/'\"$Dataset_Training_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's/CHANGEME_your_experiment_name/'\"$Experiment_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
        "!sed -i 's+CHANGEME_path_to_training_dataset+'\"$Dataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
        "\n",
        "\n",
        "if (not TrainingRate==\"1e-5\"):\n",
        "  os.system(\"sed -i 's+!!float 1e-5 # CHANGEME:+!!float '\" + TrainingRate + \"' #+g' ./experiments/EXAMPLE_gpt.yml\")\n"
      ],
      "metadata": {
        "id": "5zaScjLUt4HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Training:\n",
        "Finally, the code starts the training process by running the `train.py` script with the configured YAML file."
      ],
      "metadata": {
        "id": "Ay0tYbU8DUTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Press the stop button for this cell when you are satisfied with the results, and have seen:\n",
        "\n",
        "#@markdown `INFO:base:Saving models and training states.`\n",
        "\n",
        "#@markdown If your training run saves many models, you might exceed the storage limits on the colab runtime. To prevent this, try to delete old checkpoints in /content/DL-Art-School/experiments/$Experiment_Name/(models|training_state)/* via the file explorer panel as the training runs. **Resuming training after a crash requires config editing,** so try to not let that happen.\n",
        "\n",
        "%cd /content/DL-Art-School/codes\n",
        "\n",
        "!python3 train.py -opt ../experiments/EXAMPLE_gpt.yml"
      ],
      "metadata": {
        "id": "ju5yCN_m51r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Exporting to Google Drive:\n",
        "After training, the code provides an option to copy the experiment folder to the user's Google Drive for persistence."
      ],
      "metadata": {
        "id": "iyGVnx5jrnvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/DL-Art-School/experiments/$Experiment_Name /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "id": "BqydhU4Gwlv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}