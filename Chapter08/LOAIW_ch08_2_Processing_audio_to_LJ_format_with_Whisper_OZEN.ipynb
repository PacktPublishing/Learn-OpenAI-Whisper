{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learn OpenAI Whisper - Chapter 8\n",
        "## Notebook 2: Process audio files to a LJ format with Whisper and OZEN\n",
        "\n",
        "This notebook complements the book [Learn OpenAI Whisper](https://a.co/d/1p5k4Tg).\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1wnomL0dxmU9CgPKIgazR8AocolEYjAe5)\n",
        "\n",
        "This notebook is based on the [OZEN Toolkit](https://github.com/devilismyfriend/ozen-toolkit) project. Given a folder of files or a single audio file, it will extract the speech, transcribe using Whisper and save in the LJ format (segmented audio files in WAV format in `wavs` folder, transcriptions in folders `train` and `valid`).\n",
        "\n",
        "**NOTE**: The notebook stores the files using the following format.\n",
        "\n",
        "`dataset/`\n",
        "* ---├── `valid.txt`\n",
        "* ---├── `train.txt`\n",
        "* ---├── `wavs/`\n",
        "\n",
        "`wavs/` directory must contain `.wav` files.\n",
        "\n",
        "Example for `train.txt` and `valid.txt`:\n",
        "\n",
        "* `wavs/A.wav|Write the transcribed audio here.`\n"
      ],
      "metadata": {
        "id": "7-_XzuAwYuok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n6I0nrcQj2F"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/devilismyfriend/ozen-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install huggingface\n",
        "!pip install pydub\n",
        "!pip install yt-dlp\n",
        "!pip install pyannote.audio"
      ],
      "metadata": {
        "id": "gsEWSIVHR5Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESTART"
      ],
      "metadata": {
        "id": "K7msTh8jL6jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "!pip install termcolor\n",
        "!pip install pyfiglet\n",
        "%cd ozen-toolkit"
      ],
      "metadata": {
        "id": "bTqMldutL8-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sample file\n",
        "!wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.mp3"
      ],
      "metadata": {
        "id": "qilVdaLsQHEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQgw3KeV8Yqb"
      },
      "outputs": [],
      "source": [
        "# CUSTOM_VOICE_NAME = \"custom\"\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "custom_voice_folder = \"./\"\n",
        "\n",
        "os.makedirs(custom_voice_folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "for filename, file_data in files.upload().items():\n",
        "    with open(os.path.join(custom_voice_folder, filename), 'wb') as f:\n",
        "        f.write(file_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import configparser\n",
        "\n",
        "# Create a new ConfigParser object\n",
        "config = configparser.ConfigParser()\n",
        "\n",
        "# Add the 'DEFAULT' section and set the options\n",
        "config['DEFAULT'] = {\n",
        "    'hf_token': '<Your HF API key>',\n",
        "    'whisper_model': 'openai/whisper-medium',\n",
        "    'device': 'cuda',\n",
        "    'diaization_model': 'pyannote/speaker-diarization',\n",
        "    'segmentation_model': 'pyannote/segmentation',\n",
        "    'valid_ratio': '0.2',\n",
        "    'seg_onset': '0.7',\n",
        "    'seg_offset': '0.55',\n",
        "    'seg_min_duration': '2.0',\n",
        "    'seg_min_duration_off': '0.0'\n",
        "}\n",
        "\n",
        "# Write the configuration to a file\n",
        "with open('config.ini', 'w') as configfile:\n",
        "    config.write(configfile)\n",
        "\n",
        "# Print the contents of the file\n",
        "with open('config.ini', 'r') as configfile:\n",
        "    print(configfile.read())"
      ],
      "metadata": {
        "id": "MLHFTm15WatB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python ozen.py Learn_OAI_Whisper_Sample_Audio01.mp3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEhKULyV9rT3",
        "outputId": "daa4202e-8894-4fee-c46b-1718183ed52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-16 16:53:42.646606: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-16 16:53:42.646655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-16 16:53:42.648084: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-16 16:53:43.797529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[1m\u001b[40m\u001b[33m  ______    ________   _______ .__   __. \n",
            " /  __  \\  |       /  |   ____||  \\ |  | \n",
            "|  |  |  | `---/  /   |  |__   |   \\|  | \n",
            "|  |  |  |    /  /    |   __|  |  . `  | \n",
            "|  `--'  |   /  /----.|  |____ |  |\\   | \n",
            " \\______/   /________||_______||__| \\__| \n",
            "                                         \n",
            "\u001b[0m\n",
            "\u001b[32mConverting to WAV...\u001b[39m\n",
            "\u001b[32mLoading Segment Model...\u001b[39m\n",
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/2ffce0501d0aecad81b43a06d538186e292d0070/pytorch_model.bin`\n",
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.2.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "\u001b[32mSegmenting...\u001b[39m\n",
            "\u001b[32mLoading Transcribing Model...\u001b[39m\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "\u001b[32mTranscribing...\u001b[39m\n",
            "  0% 0/5 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "100% 5/5 [00:08<00:00,  1.35s/it]\u001b[32m Done!\u001b[39m\n",
            "\n",
            "100% 5/5 [00:09<00:00,  1.88s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive (To save trained checkpoints and to load the dataset from)"
      ],
      "metadata": {
        "id": "Agpune8JjsQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "xFt4umCqjlS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ade8590-3235-46d6-b7b8-90c8b1dfa952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp -r /content/ozen-toolkit/output/ /content/gdrive/MyDrive/"
      ],
      "metadata": {
        "id": "CDHLbbE7GgjQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}