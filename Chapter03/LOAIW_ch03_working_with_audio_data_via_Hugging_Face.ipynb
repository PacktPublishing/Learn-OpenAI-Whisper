{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learn OpenAI Whisper - Chapter 3\n",
        "\n",
        "## Complementary introduction to audio data processing with Hugging Face and Whisper\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bIiGyv_YiTdq97a7KrowCceOrZlG2hXL)\n",
        "\n",
        "In this notebook, we'll explore the essentials of working with audio data in Python, leveraging the power of Hugging Face's `datasets` library and OpenAI's Whisper model. Our journey will take us through setting up a Hugging Face account, loading and exploring an audio dataset, preprocessing that dataset to meet the requirements of machine learning models, and finally, transcribing audio to text using the state-of-the-art Whisper model.\n",
        "\n",
        "Audio data processing is a pivotal step in numerous machine learning applications, from developing voice-activated assistants to creating systems capable of understanding and transcribing spoken language in real-time. By the end of this notebook, you'll have a solid foundation in handling audio data, which you can apply to a wide range of natural language processing tasks.\n"
      ],
      "metadata": {
        "id": "fFBjlhsX3lqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up a Hugging Face Account\n",
        "\n",
        "To access and utilize the vast array of datasets and machine learning models available on Hugging Face, an account is required. A Hugging Face account offers not just access to datasets like \"PolyAI/minds14\" but also allows for collaboration on projects, contribution to the community through dataset/model sharing, and even tracking progress on various machine learning tasks. Follow the steps outlined above to create your account and get started with Hugging Face.\n",
        "\n",
        "## To create a Hugging Face account, you can follow these simple steps:\n",
        "\n",
        "1. Go to the Hugging Face website.\n",
        "2. Click on ‚ÄúJoin Hugging Face‚Äù and enter your email address and password.\n",
        "3. If you‚Äôre part of an organization, use your organization email to easily find and join your company/team org.\n",
        "4. If you already have an account, click on ‚ÄúLog in‚Äù.\n",
        "5. To interact with the Hub, you need to be logged in with a Hugging Face account.\n",
        "\n",
        "## To create a Hugging Face Access Token\n",
        "1. Go to the Access Tokens tab in your Hugging Face settings\n",
        "2. Click on the New token button to create a new User Access Token\n",
        "3. Select a role and a name for your token\n",
        "\n",
        "## To authenticate your identity using your Access Token in Colab\n",
        "1, Go to the Hugging Face site, Settings page and obtain your User Access Token.\n",
        "7. Run the following command in your Colab: `!huggingface-cli login`.\n",
        "8. Enter your Access Token when prompted.\n",
        "\n",
        "\n",
        "## To set up a Hugging Face secret token in Google Colab\n",
        "1. In your Colab notebook, click on the key icon on the left to add a new secret\n",
        "2. Set the name of the secret to HF_TOKEN and turn off Access from Notebook\n",
        "3. Paste your Hugging Face token value in the Value field\n",
        "4. Click Add to save the secret\n",
        "\n",
        "You can now use the secret token in your Colab notebook to authenticate. Make sure to keep your token private and not share it with anyone else. Happy coding! üòä"
      ],
      "metadata": {
        "id": "VCEKs-Y4wAYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and explore an audio dataset\n",
        "In this section, we dive into loading and exploring an audio dataset using the `datasets` library. Specifically, we're working with the \"PolyAI/minds14\" dataset, a rich collection of audio recordings intended for intent classification tasks‚Äîcrucial for building voice-operated interfaces and understanding natural language commands. Through the code snippets below, we inspect the dataset's structure, listen to audio samples directly within the notebook, and employ visualization techniques to analyze the audio signals. This hands-on exploration is foundational for grasping the nuances of audio data in machine learning applications.\n"
      ],
      "metadata": {
        "id": "RE6ewF9z4r2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install datasets Library with Audio Support\n",
        "To begin working with audio datasets, we first need to install the `datasets` library, ensuring we include support for audio data.\n",
        "This is done by specifying `[audio]` after the library name, which installs additional dependencies necessary for audio processing."
      ],
      "metadata": {
        "id": "j4VqBDxt6vEg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B8bm_3ouldO"
      },
      "outputs": [],
      "source": [
        "!pip install datasets[audio]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Environment Variables for Hugging Face Authentication\n",
        "Next, we set up our environment to authenticate with Hugging Face. This involves storing our Hugging Face API token as an environment variable. This token allows us to access Hugging Face's datasets and models securely. Replace the placeholder token with your own Hugging Face API token.\n"
      ],
      "metadata": {
        "id": "hSY7Cv2A61tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = 'your_huggingface_api_token_here'\n",
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "id": "AszQuG7IS5Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticating Hugging Face CLI\n",
        "We can also authenticate using the Hugging Face Command Line Interface (CLI) with our API token. This step is crucial for using Hugging Face's CLI tools to interact with the platform, enabling us to download datasets, models, and more.\n"
      ],
      "metadata": {
        "id": "DDnoDfNI7Dpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "OHNuVY_RTujq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Audio Dataset\n",
        "Here, we load the \"PolyAI/minds14\" dataset from Hugging Face's datasets library, specifying `en-US` as the language and `train` as the dataset split we wish to load. This dataset is designed for intent classification within spoken language understanding, a critical task in developing conversational AI.\n"
      ],
      "metadata": {
        "id": "9iv2OIvX7RI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
        "minds"
      ],
      "metadata": {
        "id": "j_L1_F8nuohR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying an Example Entry from the Dataset\n",
        "To get a feel for the dataset's structure, we display the first entry. This gives us insight into the data's format, including the audio file's properties and any associated metadata, such as the transcription and intent classification."
      ],
      "metadata": {
        "id": "SEid7sQU7XWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = minds[0]\n",
        "for key, value in example.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "uHbT76n_7amq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mapping Intent Class IDs to Descriptive Labels\n",
        "This dataset includes numerical identifiers for intent classes. To make these identifiers more understandable, we map them to their descriptive labels, making it easier to interpret the data."
      ],
      "metadata": {
        "id": "1Y4DybQT7f_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = minds.features[\"intent_class\"].int2str\n",
        "id2label(example[\"intent_class\"])"
      ],
      "metadata": {
        "id": "RRko_mDI7jVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Unnecessary Columns from the Dataset\n",
        "To streamline our dataset for analysis, we remove columns that are not needed for our current task, such as `lang_id` and `english_transcription`. This simplifies the dataset, focusing on the audio data and intent labels.\n"
      ],
      "metadata": {
        "id": "dx5Nfecu7vah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = [\"lang_id\", \"english_transcription\"]\n",
        "minds = minds.remove_columns(columns_to_remove)"
      ],
      "metadata": {
        "id": "efg2k3oK7zHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listening to an Audio Sample\n",
        "\n",
        "Now that we've loaded and inspected the dataset, let's listen to an audio example. This helps us understand the quality and content of the audio data we'll be working with."
      ],
      "metadata": {
        "id": "f7ZajJclyiMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "example = minds[6]\n",
        "wav_array = example[\"audio\"][\"array\"]\n",
        "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "Audio(data=wav_array, rate=sampling_rate)"
      ],
      "metadata": {
        "id": "kXGqdrFP3a1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing an Audio Signal\n",
        "Visualizing audio signals can provide valuable insights into their characteristics. Here, we plot the waveform of an audio sample, helping us to observe its amplitude variations over time. This visual representation is useful for understanding the audio data's structure and identifying patterns."
      ],
      "metadata": {
        "id": "2GfSJ9wd8KYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "array = example[\"audio\"][\"array\"]\n",
        "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.waveshow(array, sr=sampling_rate)"
      ],
      "metadata": {
        "id": "vWU6HtHH09Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Audio Dataset for Machine Learning\n",
        "\n",
        "Before feeding audio data into machine learning models, it's essential to preprocess it. This section outlines the preprocessing steps necessary to transform the audio data into a format suitable for analysis and modeling. We'll cover casting the audio data to a consistent format, filtering recordings based on their duration to maintain uniformity, and extracting features that models can interpret. These preprocessing steps are vital for ensuring the quality and consistency of the input data, directly impacting the effectiveness of machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "foY6E06WgsdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cast Audio Column to Uniform Format\n",
        "To ensure that all audio data within our dataset adheres to a uniform format, we cast the `audio` column to a specific configuration using Hugging Face's `datasets` library. This step is crucial for maintaining consistency, especially when dealing with datasets that might contain audio files in various formats or sampling rates. Here, we standardize the audio data to a sampling rate of 16,000 Hz, which is a common rate used in speech recognition systems.\n"
      ],
      "metadata": {
        "id": "pOUDSjLX_Hoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "vVM1MlHg8F1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter Out Long Audio Files\n",
        "In speech and audio processing tasks, it's often beneficial to work with audio files of a consistent duration. Longer audio files can be problematic due to increased computational requirements and potential irrelevance of parts of the audio. Here, we define a function to check if the audio's length is within a specified maximum duration. This function will later be used to filter out any audio files exceeding our defined maximum length, ensuring a more uniform dataset conducive to efficient processing."
      ],
      "metadata": {
        "id": "uMhAbmGL_QKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DURATION_IN_SECONDS = 10.0\n",
        "\n",
        "def is_audio_length_in_range(input_length):\n",
        "    return input_length < MAX_DURATION_IN_SECONDS"
      ],
      "metadata": {
        "id": "TZRBIfIP9QI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_column = []\n",
        "\n",
        "for i, example in enumerate(minds):\n",
        "    # print(i)\n",
        "    # print(example[\"audio\"][\"path\"])\n",
        "    audio_array = example[\"audio\"][\"array\"]\n",
        "    sr = example[\"audio\"][\"sampling_rate\"]\n",
        "    duration = librosa.get_duration(y=audio_array, sr=sr)\n",
        "    new_column.append(duration)\n",
        "    # print(f\"{duration:.2f}s\")\n",
        "    # if i == 5:\n",
        "    #     break\n",
        "\n",
        "minds = minds.add_column(\"duration\", new_column)\n",
        "for i, example in enumerate(minds):\n",
        "    # print(i)\n",
        "    # print(example[\"audio\"][\"path\"])\n",
        "    print(example[\"path\"])\n",
        "    print(example[\"duration\"])\n",
        "    if i == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "Pt9I2KqCYroC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Length Filter to Dataset\n",
        "With our length filter function defined, we now apply it to our dataset. Using the `filter` method provided by the `datasets` library, we can efficiently remove entries that don't meet our duration criteria. This step iterates through the dataset, applying our `is_audio_length_in_range` function to each entry and retaining only those that fit within our specified maximum duration. This filtering process is a key step in preparing our dataset for subsequent analysis and modeling.\n"
      ],
      "metadata": {
        "id": "CeCDiNic_xDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use ü§ó Datasets' `filter` method to apply the filtering function\n",
        "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "# remove the temporary helper column\n",
        "minds = minds.remove_columns([\"duration\"])\n",
        "minds"
      ],
      "metadata": {
        "id": "6ToNuFgm9UG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction with WhisperFeatureExtractor\n",
        "Before we can transcribe audio data or use it for training machine learning models, we often need to extract relevant features from the raw audio. This process transforms audio data into a format that models can more easily interpret. Here, we utilize the `WhisperFeatureExtractor` from the `transformers` library to prepare our dataset for the Whisper model. The feature extractor standardizes the audio data, ensuring it's in the correct format and sampling rate expected by Whisper for efficient transcription."
      ],
      "metadata": {
        "id": "x1FoXNdk_ew1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "gjGV9FnB7a0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Dataset for Whisper Model\n",
        "To process our audio data with the Whisper model, we must first prepare it according to the model's requirements. This involves applying the `WhisperFeatureExtractor` to each audio file in our dataset. We define a `prepare_dataset` function that takes an example from our dataset, extracts its audio, and then applies the feature extractor. This function is mapped over the entire dataset, transforming each audio file into a format suitable for transcription with Whisper. This step is crucial for leveraging Whisper's capabilities to transcribe or analyze our audio data.\n"
      ],
      "metadata": {
        "id": "f9fYO2yAAAh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "    features = feature_extractor(\n",
        "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=True\n",
        "    )\n",
        "    return features"
      ],
      "metadata": {
        "id": "SwMbnwit7h3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds = minds.map(prepare_dataset)\n",
        "minds"
      ],
      "metadata": {
        "id": "vA8REAyC7nVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Preprocessed Audio Features\n",
        "After preprocessing our audio data, it's helpful to visualize the resulting features to understand what the Whisper model will \"see\" when analyzing the audio. Here, we select an example from our preprocessed dataset and plot its features. This visualization can give us insight into the nature of the transformations applied by the `WhisperFeatureExtractor` and how they might influence the model's performance on our audio data.\n"
      ],
      "metadata": {
        "id": "hZziQfd0AOKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "example = minds[0]\n",
        "input_features = example[\"input_features\"]\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.specshow(\n",
        "    np.asarray(input_features[0]),\n",
        "    x_axis=\"time\",\n",
        "    y_axis=\"mel\",\n",
        "    sr=feature_extractor.sampling_rate,\n",
        "    hop_length=feature_extractor.hop_length,\n",
        ")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "vLHJ9_zB7F-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = minds[0]\n",
        "example"
      ],
      "metadata": {
        "id": "ZJKWsTMafwvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leveraging Whisper for Audio to Text Transcription\n",
        "\n",
        "OpenAI's Whisper model represents a breakthrough in audio to text transcription technology, offering a powerful tool for converting spoken language into written text. This section guides you through the process of using the Whisper model to transcribe audio data. We'll explore two approaches: one using raw numpy arrays and another utilizing WAV files. By understanding these methods, you'll be equipped to transcribe audio data efficiently, a skill applicable in various domains, including voice recognition and natural language understanding.\n",
        "\n",
        "To transcribe audio to text using OpenAI's Whisper model, you'll first need to ensure you have Whisper installed and then proceed with the Python code to load and transcribe the audio.\n",
        "\n",
        "### Step 1: Install Whisper\n",
        "\n",
        "First, ensure that you have OpenAI's Whisper installed. You can install Whisper using pip if you haven't done so already:\n",
        "\n",
        "```bash\n",
        "pip install openai-whisper\n",
        "```"
      ],
      "metadata": {
        "id": "XxqYaP5WOijy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "id": "WHvsuLsqJcRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 2: Load Your Audio Data\n",
        "\n",
        "Let's use audio data from the `example` dataset, loaded into `wav_array` with a sampling rate defined as `sampling_rate`. This is great because Whisper needs the audio data in a format it can process."
      ],
      "metadata": {
        "id": "tGlhK-3jO8Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "example = minds[200]\n",
        "wav_array = example[\"audio\"][\"array\"]\n",
        "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "Audio(data=wav_array, rate=sampling_rate)"
      ],
      "metadata": {
        "id": "HwjJNp2Sf2F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 3a: Transcribe Audio to Text using numpy\n",
        "\n",
        "In this example, audio data is converted to float32 before passing it to the model.transcribe() function. Here's how you can do it:"
      ],
      "metadata": {
        "id": "xODbkhLMPWn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import numpy as np\n",
        "\n",
        "# Assuming wav_array is your audio data and sampling_rate is defined\n",
        "\n",
        "# Convert wav_array to float32\n",
        "wav_array = wav_array.astype(np.float32)\n",
        "\n",
        "# Initialize the Whisper model\n",
        "model = whisper.load_model(\"small\")  # Adjust model size as needed\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe(audio=wav_array)\n",
        "\n",
        "# Display the transcription\n",
        "print(\"Transcription:\", result[\"text\"])"
      ],
      "metadata": {
        "id": "gdbpEFpVLEjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3b: Transcribe Audio to Text using WAV file\n",
        "\n",
        "In this example, we save the wav_array to a .wav file before transcribing. Here's how you can do it:"
      ],
      "metadata": {
        "id": "WrAEPCJiQ6CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "\n",
        "# Save your wav_array to a file\n",
        "sf.write('temp_audio.wav', wav_array, sampling_rate)\n",
        "\n",
        "# Now transcribe the saved audio file\n",
        "result = model.transcribe('temp_audio.wav')\n",
        "\n",
        "# Print the transcription\n",
        "print(\"Transcription:\", result[\"text\"])\n"
      ],
      "metadata": {
        "id": "PvDSKkjjKvaC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
